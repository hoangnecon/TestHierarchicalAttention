{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyN1iHOX4Wv6A0maA8KJ/++q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoangnecon/TestHierarchicalAttention/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5YXp6Yw9AYI",
        "outputId": "dfead738-06e0-4efe-83ba-d5d2c15c656e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.8/121.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U vnstock -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from vnstock import Vnstock #Ph·∫£i t·∫°o ƒë·ªëi t∆∞·ª£ng\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def fetch_and_combine_data(symbols, start_date, end_date):\n",
        "    all_data = {}\n",
        "    print(f\"B·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu cho c√°c ch·ªâ s·ªë: {symbols} t·ª´ {start_date} ƒë·∫øn {end_date}...\")\n",
        "\n",
        "    # Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng Vnstock\n",
        "    stock_client = Vnstock()\n",
        "\n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            df = stock_client.stock(symbol=symbol).quote.history(\n",
        "                start=start_date,\n",
        "                end=end_date\n",
        "            )\n",
        "\n",
        "            if df.empty:\n",
        "                print(f\"C·∫£nh b√°o: Kh√¥ng c√≥ d·ªØ li·ªáu tr·∫£ v·ªÅ cho ch·ªâ s·ªë {symbol}.\")\n",
        "                continue\n",
        "\n",
        "            df.rename(columns={'time': 'Date', 'open': f'{symbol}_Open', 'high': f'{symbol}_High',\n",
        "                               'low': f'{symbol}_Low', 'close': f'{symbol}_Close', 'volume': f'{symbol}_Volume'},\n",
        "                      inplace=True)\n",
        "\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df.set_index('Date', inplace=True)\n",
        "\n",
        "            all_data[symbol] = df\n",
        "            print(f\"T·∫£i th√†nh c√¥ng d·ªØ li·ªáu cho {symbol}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói khi t·∫£i d·ªØ li·ªáu cho {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"L·ªói: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu cho b·∫•t k·ª≥ ch·ªâ s·ªë n√†o.\")\n",
        "        return None\n",
        "\n",
        "    combined_df = pd.concat(all_data.values(), axis=1, join='outer')\n",
        "    combined_df.sort_index(inplace=True)\n",
        "    combined_df.fillna(method='ffill', inplace=True)\n",
        "    combined_df.dropna(inplace=True)\n",
        "\n",
        "    print(\"\\n‚úÖ Ho√†n t·∫•t vi·ªác t·∫£i v√† h·ª£p nh·∫•t d·ªØ li·ªáu.\")\n",
        "    return combined_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "M02CzyJM9MIp",
        "outputId": "3a235152-23fc-4e3e-aa32-c36ed51db8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html>\n",
              "\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <link href=\"https://fonts.googleapis.com/css2?family=Lexend:wght@400;500;600;700&display=swap\" rel=\"stylesheet\">\n",
              "    <style>\n",
              "        .vnstock-ad-banner {\n",
              "            all: initial;\n",
              "            font-family: 'Lexend', sans-serif;\n",
              "            display: flex;\n",
              "            flex-wrap: wrap;\n",
              "            max-width: 100%;\n",
              "            margin: 12px 0;\n",
              "            border-radius: 10px;\n",
              "            overflow: hidden;\n",
              "            background: #ffffff;\n",
              "            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);\n",
              "            color: #333;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-content {\n",
              "            flex: 1;\n",
              "            padding: 12px 16px;\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            justify-content: center;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-title {\n",
              "            margin: 0 0 10px 0;\n",
              "            line-height: 1.3;\n",
              "            font-size: 18px;\n",
              "            font-weight: 700;\n",
              "            color: #4CAF50;\n",
              "        }\n",
              "\n",
              "        .title-highlight {\n",
              "            color: #8C52FF;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-features {\n",
              "            list-style: none;\n",
              "            padding-left: 0;\n",
              "            margin: 10px 0 12px 0;\n",
              "            font-size: 13px;\n",
              "            line-height: 1.4;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-features li {\n",
              "            margin-bottom: 6px;\n",
              "        }\n",
              "\n",
              "        .feature-highlight {\n",
              "            color: #8C52FF;\n",
              "            font-weight: 600;\n",
              "        }\n",
              "\n",
              "        .button-container {\n",
              "            text-align: center;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-button {\n",
              "            display: inline-block;\n",
              "            background-color: #4CAF50;\n",
              "            color: #fff;\n",
              "            padding: 6px 16px;\n",
              "            text-decoration: none;\n",
              "            font-size: 13px;\n",
              "            border-radius: 20px;\n",
              "            font-weight: 600;\n",
              "            transition: all 0.2s;\n",
              "            box-shadow: 0 2px 6px rgba(76, 175, 80, 0.2);\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-button:hover {\n",
              "            background-color: #8C52FF;\n",
              "            color: white;\n",
              "            transform: translateY(-1px);\n",
              "            box-shadow: 0 4px 8px rgba(140, 82, 255, 0.3);\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-image {\n",
              "            flex: 1;\n",
              "            min-width: 180px;\n",
              "            max-width: 45%;\n",
              "        }\n",
              "\n",
              "        .vnstock-ad-image img {\n",
              "            width: 100%;\n",
              "            height: 100%;\n",
              "            max-height: 250px;\n",
              "            object-fit: cover;\n",
              "            display: block;\n",
              "        }\n",
              "\n",
              "        @media (max-width: 768px) {\n",
              "            .vnstock-ad-banner {\n",
              "                flex-direction: column;\n",
              "            }\n",
              "\n",
              "            .vnstock-ad-image {\n",
              "                min-width: auto;\n",
              "                max-width: 100%;\n",
              "            }\n",
              "        }\n",
              "    </style>\n",
              "</head>\n",
              "\n",
              "<body>\n",
              "    <div class=\"vnstock-ad-banner\">\n",
              "        <div class=\"vnstock-ad-content\">\n",
              "            <h2 class=\"vnstock-ad-title\">\n",
              "                <span class=\"title-highlight\">ü§ñ Python Coding v·ªõi AI:</span> H·ªçc ch·ªß ƒë·ªông qua video ghi h√¨nh\n",
              "            </h2>\n",
              "            <ul class=\"vnstock-ad-features\">\n",
              "                <li><span class=\"feature-highlight\">üé• Kh√≥a h·ªçc live ƒë√£ kh·ªüi ƒë·ªông:</span> Xem l·∫°i video b√†i gi·∫£ng t·∫°i\n",
              "                    website, theo d√µi ti·∫øn ƒë·ªô r√µ r√†ng</li>\n",
              "                <li><span class=\"feature-highlight\">‚ö° ƒêi·ªÅu khi·ªÉn AI vi·∫øt code Python:</span> Kh√¥ng c·∫ßn h·ªçc thu·ªôc l√≤ng,\n",
              "                    h·ªçc ·ª©ng bi·∫øn v·ªõi AI agent</li>\n",
              "                <li><span class=\"feature-highlight\">ü§ñ T·∫≠p trung v√†o AI th·ª±c chi·∫øn:</span> Vi·∫øt bot, ph√¢n t√≠ch d·ªØ li·ªáu\n",
              "                    b·∫±ng Python nh·ªù s·ª©c m·∫°nh AI</li>\n",
              "                <li><span class=\"feature-highlight\">üí° C·ªông ƒë·ªìng ch·∫•t l∆∞·ª£ng cao:</span> 100+ nh√† ƒë·∫ßu t∆∞, chuy√™n gia &\n",
              "                    l·∫≠p tr√¨nh vi√™n c√πng h·ªçc h·ªèi</li>\n",
              "            </ul>\n",
              "            <div class=\"button-container\">\n",
              "                <a href=\"https://vnstocks.com/lp-khoa-hoc-python-chung-khoan\" class=\"vnstock-ad-button\">\n",
              "                    ƒêƒÉng k√Ω h·ªçc ngay ‚Ä∫\n",
              "                </a>\n",
              "            </div>\n",
              "        </div>\n",
              "        <a href=\"https://vnstocks.com/lp-khoa-hoc-python-chung-khoan\" class=\"vnstock-ad-image\">\n",
              "            <img src=\"https://vnstocks.com/img/cta-python-chung-khoan-k11-start-now.jpg\"\n",
              "                alt=\"Kh√≥a h·ªçc Python Coding v·ªõi AI\"\n",
              "                style=\"width: 100%; height: 100%; max-height: 250px; object-fit: cover; display: block;\">\n",
              "        </a>\n",
              "    </div>\n",
              "</body>\n",
              "\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYMBOLS_TO_FETCH = ['VNINDEX', 'VN30', 'VN100']\n",
        "START_DATE = '2019-01-01'\n",
        "END_DATE = '2023-12-31'\n",
        "OUTPUT_CSV_PATH = 'vn_indices_2019_2023.csv'\n",
        "\n",
        "\n",
        "print(\"B·∫Øt ƒë·∫ßu qu√° tr√¨nh...\")\n",
        "final_dataframe = fetch_and_combine_data(\n",
        "    symbols=SYMBOLS_TO_FETCH,\n",
        "    start_date=START_DATE,\n",
        "    end_date=END_DATE\n",
        ")\n",
        "\n",
        "if final_dataframe is not None:\n",
        "\n",
        "    final_dataframe.to_csv(OUTPUT_CSV_PATH)\n",
        "    print(f\"\\n‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu v√†o file: {OUTPUT_CSV_PATH}\")\n",
        "\n",
        "    print(\"\\n5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
        "    print(final_dataframe.head())\n",
        "\n",
        "    print(\"\\nC√°c file trong th∆∞ m·ª•c hi·ªán t·∫°i c·ªßa Colab:\")\n",
        "    !ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDQ_FSoF_XiE",
        "outputId": "13c68fd0-e225-4fdf-f85b-f4e61ec366b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-10 09:37:02 - vnstock.common.data.data_explorer - INFO - Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n",
            "INFO:vnstock.common.data.data_explorer:Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh...\n",
            "B·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu cho c√°c ch·ªâ s·ªë: ['VNINDEX', 'VN30', 'VN100'] t·ª´ 2019-01-01 ƒë·∫øn 2023-12-31...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-10 09:37:04 - vnstock.common.data.data_explorer - INFO - Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n",
            "INFO:vnstock.common.data.data_explorer:Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T·∫£i th√†nh c√¥ng d·ªØ li·ªáu cho VNINDEX.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-07-10 09:37:05 - vnstock.common.data.data_explorer - INFO - Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n",
            "INFO:vnstock.common.data.data_explorer:Kh√¥ng ph·∫£i l√† m√£ ch·ª©ng kho√°n, th√¥ng tin c√¥ng ty v√† t√†i ch√≠nh kh√¥ng kh·∫£ d·ª•ng.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T·∫£i th√†nh c√¥ng d·ªØ li·ªáu cho VN30.\n",
            "T·∫£i th√†nh c√¥ng d·ªØ li·ªáu cho VN100.\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫£i v√† h·ª£p nh·∫•t d·ªØ li·ªáu.\n",
            "\n",
            "‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu v√†o file: vn_indices_2019_2023.csv\n",
            "\n",
            "5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu ti√™n:\n",
            "            VNINDEX_Open  VNINDEX_High  VNINDEX_Low  VNINDEX_Close  \\\n",
            "Date                                                                 \n",
            "2022-06-20       1218.37       1221.60      1180.40        1180.40   \n",
            "2022-06-21       1172.98       1189.97      1162.94        1172.47   \n",
            "2022-06-22       1180.11       1185.86      1162.98        1169.27   \n",
            "2022-06-23       1162.06       1188.88      1162.06        1188.88   \n",
            "2022-06-24       1190.82       1196.85      1185.48        1185.48   \n",
            "\n",
            "            VNINDEX_Volume  VN30_Open  VN30_High  VN30_Low  VN30_Close  \\\n",
            "Date                                                                     \n",
            "2022-06-20       620002900    1258.30    1262.83   1223.03     1225.56   \n",
            "2022-06-21       607711900    1220.76    1240.80   1212.69     1224.54   \n",
            "2022-06-22       529135300    1233.28    1242.06   1222.86     1227.18   \n",
            "2022-06-23       380442900    1219.40    1240.58   1219.08     1240.58   \n",
            "2022-06-24       410876800    1239.54    1246.22   1234.72     1235.47   \n",
            "\n",
            "            VN30_Volume  VN100_Open  VN100_High  VN100_Low  VN100_Close  \\\n",
            "Date                                                                      \n",
            "2022-06-20    168768300     1200.28     1205.64    1162.12      1164.33   \n",
            "2022-06-21    172801800     1164.33     1172.84    1146.33      1155.46   \n",
            "2022-06-22    142695000     1155.46     1171.76    1153.99      1161.32   \n",
            "2022-06-23    105240000     1161.32     1181.06    1155.29      1181.06   \n",
            "2022-06-24     90437000     1181.06     1188.88    1178.27      1178.27   \n",
            "\n",
            "            VN100_Volume  \n",
            "Date                      \n",
            "2022-06-20   440780139.0  \n",
            "2022-06-21    46278396.0  \n",
            "2022-06-22   379972360.0  \n",
            "2022-06-23   287745109.0  \n",
            "2022-06-24    15931986.0  \n",
            "\n",
            "C√°c file trong th∆∞ m·ª•c hi·ªán t·∫°i c·ªßa Colab:\n",
            "total 56K\n",
            "drwxr-xr-x 1 root root 4.0K Jul  8 17:47 sample_data\n",
            "-rw-r--r-- 1 root root  52K Jul 10 09:37 vn_indices_2019_2023.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config.py\n",
        "%%writefile config.py\n",
        "RAW_DATA_PATH = 'vn_indices_2019_2023.csv'\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "INDICATORS = {\n",
        "    'rsi': {'window': 14},\n",
        "    'macd': {'fast': 12, 'slow': 26, 'signal': 9},\n",
        "    'bollinger': {'window': 20, 'std': 2}\n",
        "}\n",
        "TARGET_SYMBOLS = ['VNINDEX', 'VN30', 'VN100']\n",
        "PRICE_COLUMN_SUFFIX = '_Close'\n",
        "VOLUME_COLUMN_SUFFIX = '_Volume'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StCT7HFJBK2w",
        "outputId": "4703fe07-b6ab-4608-f0b4-3eaf917998aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feature_engineering.py\n",
        "# feature_engineering.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Import c√°c c·∫•u h√¨nh t·ª´ file config.py (ƒê√É S·ª¨A: B·ªè PANDAS_FILL_METHOD)\n",
        "from config import RAW_DATA_PATH, PROCESSED_DATA_PATH, INDICATORS, TARGET_SYMBOLS, PRICE_COLUMN_SUFFIX\n",
        "\n",
        "def calculate_rsi(series, window):\n",
        "    \"\"\"T√≠nh to√°n Ch·ªâ s·ªë S·ª©c m·∫°nh T∆∞∆°ng ƒë·ªëi (RSI)\"\"\"\n",
        "    delta = series.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "    rs = gain / loss\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "def calculate_macd(series, fast, slow, signal):\n",
        "    \"\"\"T√≠nh to√°n Trung b√¨nh ƒê·ªông H·ªôi t·ª• Ph√¢n k·ª≥ (MACD)\"\"\"\n",
        "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
        "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
        "    macd = exp1 - exp2\n",
        "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
        "    return macd, signal_line\n",
        "\n",
        "def calculate_bollinger_bands(series, window, std):\n",
        "    \"\"\"T√≠nh to√°n D·∫£i Bollinger\"\"\"\n",
        "    rolling_mean = series.rolling(window=window).mean()\n",
        "    rolling_std = series.rolling(window=window).std()\n",
        "    upper_band = rolling_mean + (rolling_std * std)\n",
        "    lower_band = rolling_mean - (rolling_std * std)\n",
        "    return upper_band, lower_band\n",
        "\n",
        "def add_features(df):\n",
        "    \"\"\"\n",
        "    ƒê·ªçc d·ªØ li·ªáu th√¥, t√≠nh to√°n c√°c ch·ªâ b√°o k·ªπ thu·∫≠t cho m·ªói m√£,\n",
        "    v√† tr·∫£ v·ªÅ m·ªôt dataframe m·ªõi v·ªõi c√°c ƒë·∫∑c tr∆∞ng n√†y.\n",
        "    \"\"\"\n",
        "    print(\"B·∫Øt ƒë·∫ßu qu√° tr√¨nh Feature Engineering...\")\n",
        "\n",
        "    for symbol in TARGET_SYMBOLS:\n",
        "        print(f\"-> ƒêang t√≠nh to√°n ch·ªâ b√°o cho {symbol}...\")\n",
        "        price_col = f\"{symbol}{PRICE_COLUMN_SUFFIX}\"\n",
        "\n",
        "        # --- RSI ---\n",
        "        rsi_params = INDICATORS['rsi']\n",
        "        df[f'{symbol}_RSI'] = calculate_rsi(df[price_col], window=rsi_params['window'])\n",
        "\n",
        "        # --- MACD ---\n",
        "        macd_params = INDICATORS['macd']\n",
        "        macd, signal_line = calculate_macd(df[price_col], fast=macd_params['fast'], slow=macd_params['slow'], signal=macd_params['signal'])\n",
        "        df[f'{symbol}_MACD'] = macd\n",
        "        df[f'{symbol}_MACD_Signal'] = signal_line\n",
        "\n",
        "        # --- Bollinger Bands ---\n",
        "        bb_params = INDICATORS['bollinger']\n",
        "        upper_band, lower_band = calculate_bollinger_bands(df[price_col], window=bb_params['window'], std=bb_params['std'])\n",
        "        df[f'{symbol}_BB_Upper'] = upper_band\n",
        "        df[f'{symbol}_BB_Lower'] = lower_band\n",
        "\n",
        "    original_rows = len(df)\n",
        "    df.dropna(inplace=True)\n",
        "    new_rows = len(df)\n",
        "    print(f\"\\nƒê√£ b·ªè {original_rows - new_rows} h√†ng c√≥ gi√° tr·ªã NaN sau khi t√≠nh to√°n ƒë·∫∑c tr∆∞ng.\")\n",
        "\n",
        "    print(\"\\nHo√†n t·∫•t Feature Engineering.\")\n",
        "    print(\"D·ªØ li·ªáu m·∫´u sau khi x·ª≠ l√Ω:\")\n",
        "    print(df.head())\n",
        "\n",
        "    return df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        raw_df = pd.read_csv(RAW_DATA_PATH, index_col='Date', parse_dates=True)\n",
        "        processed_df = add_features(raw_df)\n",
        "        processed_df.to_csv(PROCESSED_DATA_PATH)\n",
        "        print(f\"\\n‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file {PROCESSED_DATA_PATH}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu th√¥ t·∫°i '{RAW_DATA_PATH}'.\")\n",
        "        print(\"Vui l√≤ng ch·∫°y c√°c √¥ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ƒê√£ x·∫£y ra l·ªói: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EyHYi7dCwf0",
        "outputId": "308efa1f-5fd2-4546-e570-26ad31f0231f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing feature_engineering.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_engineering.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UZmbNwYC-JE",
        "outputId": "aef83db7-5f1d-48e1-b2f4-e26ba680e5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh Feature Engineering...\n",
            "-> ƒêang t√≠nh to√°n ch·ªâ b√°o cho VNINDEX...\n",
            "-> ƒêang t√≠nh to√°n ch·ªâ b√°o cho VN30...\n",
            "-> ƒêang t√≠nh to√°n ch·ªâ b√°o cho VN100...\n",
            "\n",
            "ƒê√£ b·ªè 19 h√†ng c√≥ gi√° tr·ªã NaN sau khi t√≠nh to√°n ƒë·∫∑c tr∆∞ng.\n",
            "\n",
            "Ho√†n t·∫•t Feature Engineering.\n",
            "D·ªØ li·ªáu m·∫´u sau khi x·ª≠ l√Ω:\n",
            "            VNINDEX_Open  VNINDEX_High  ...  VN100_BB_Upper  VN100_BB_Lower\n",
            "Date                                    ...                                \n",
            "2022-07-15       1183.15       1189.66  ...     1214.049565     1142.499435\n",
            "2022-07-18       1184.74       1184.93  ...     1213.959151     1143.285849\n",
            "2022-07-19       1176.10       1180.46  ...     1213.245691     1145.498309\n",
            "2022-07-20       1188.12       1198.63  ...     1213.481968     1147.715032\n",
            "2022-07-21       1195.03       1201.91  ...     1214.512315     1147.852685\n",
            "\n",
            "[5 rows x 30 columns]\n",
            "\n",
            "‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file vn_indices_processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "INDICATORS = {\n",
        "    'rsi': {'window': 14},\n",
        "    'macd': {'fast': 12, 'slow': 26, 'signal': 9},\n",
        "    'bollinger': {'window': 20, 'std': 2}\n",
        "}\n",
        "TARGET_SYMBOLS = ['VNINDEX', 'VN30', 'VN100']\n",
        "PRICE_COLUMN_SUFFIX = '_Close'\n",
        "VOLUME_COLUMN_SUFFIX = '_Volume'\n",
        "\n",
        "LOOKBACK_WINDOW = 30\n",
        "\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIU843s7FAx5",
        "outputId": "7dcf70d9-dc75-4c3d-eedc-3e4616e497ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from config import (\n",
        "    PROCESSED_DATA_PATH, LOOKBACK_WINDOW, TARGET_COLUMN,\n",
        "    WAVELET_FAMILY, WAVELET_LEVEL, TEST_SET_SIZE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "def create_multiscale_features(data, wavelet_family, level):\n",
        "    \"\"\"\n",
        "    Ph√¢n r√£ m·ªói c·ªôt trong dataframe th√†nh c√°c th√†nh ph·∫ßn ƒëa quy m√¥ b·∫±ng Wavelet.\n",
        "    \"\"\"\n",
        "    coeffs_df_list = []\n",
        "    for column in data.columns:\n",
        "        series = data[column].values\n",
        "        coeffs = pywt.wavedec(series, wavelet_family, level=level)\n",
        "\n",
        "        for i, c in enumerate(coeffs):\n",
        "            c_padded = np.pad(c, (0, len(data) - len(c)), 'constant')\n",
        "            coeff_name = f\"{column}_wavelet_L{i}\"\n",
        "            coeffs_df_list.append(pd.DataFrame({coeff_name: c_padded}, index=data.index))\n",
        "\n",
        "    multiscale_df = pd.concat(coeffs_df_list, axis=1)\n",
        "    return multiscale_df\n",
        "\n",
        "def create_sequences(data, target, lookback_window):\n",
        "    \"\"\"T·∫°o c√°c chu·ªói tu·∫ßn t·ª±- slide windows cho b√†i to√°n h·ªçc c√≥ gi√°m s√°t.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback_window):\n",
        "        X.append(data[i:(i + lookback_window)])\n",
        "        y.append(target[i + lookback_window])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    \"\"\"L·ªõp Dataset t√πy ch·ªânh c·ªßa PyTorch.\"\"\"\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "def get_data_loaders():\n",
        "    print(\"B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\")\n",
        "    try:\n",
        "        df = pd.read_csv(PROCESSED_DATA_PATH, index_col='Date', parse_dates=True)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {PROCESSED_DATA_PATH}. Vui l√≤ng ch·∫°y c√°c b∆∞·ªõc tr∆∞·ªõc.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # 2. T·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ b·∫±ng Wavelet\n",
        "    print(f\"-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: {WAVELET_FAMILY}, level: {WAVELET_LEVEL})...\")\n",
        "    multiscale_df = create_multiscale_features(df, WAVELET_FAMILY, WAVELET_LEVEL)\n",
        "\n",
        "    print(\"-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\")\n",
        "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_features = feature_scaler.fit_transform(multiscale_df)\n",
        "\n",
        "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_target = target_scaler.fit_transform(df[[TARGET_COLUMN]])\n",
        "\n",
        "    print(f\"-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = {LOOKBACK_WINDOW}...\")\n",
        "    X, y = create_sequences(scaled_features, scaled_target.flatten(), LOOKBACK_WINDOW)\n",
        "\n",
        "    split_index = int(len(X) * (1 - TEST_SET_SIZE))\n",
        "    X_train, X_test = X[:split_index], X[split_index:]\n",
        "    y_train, y_test = y[:split_index], y[split_index:]\n",
        "    print(f\"-> K√≠ch th∆∞·ªõc t·∫≠p Train: {len(X_train)} m·∫´u\")\n",
        "    print(f\"-> K√≠ch th∆∞·ªõc t·∫≠p Test: {len(X_test)} m·∫´u\")\n",
        "\n",
        "    # 6. T·∫°o c√°c ƒë·ªëi t∆∞·ª£ng Dataset v√† DataLoader c·ªßa PyTorch\n",
        "    train_dataset = StockDataset(X_train, y_train)\n",
        "    test_dataset = StockDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(\"\\n‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\")\n",
        "    return train_loader, test_loader, feature_scaler, target_scaler\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # C√†i ƒë·∫∑t th∆∞ vi·ªán wavelet n·∫øu ch∆∞a c√≥\n",
        "    try:\n",
        "        import pywt\n",
        "    except ImportError:\n",
        "        print(\"ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán PyWavelets...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"PyWavelets\"])\n",
        "\n",
        "    # Ch·∫°y h√†m ch√≠nh ƒë·ªÉ ki·ªÉm tra\n",
        "    train_loader, test_loader, _, target_scaler = get_data_loaders()\n",
        "\n",
        "    if train_loader:\n",
        "        # Ki·ªÉm tra m·ªôt batch d·ªØ li·ªáu\n",
        "        features, labels = next(iter(train_loader))\n",
        "        print(\"\\n--- Ki·ªÉm tra m·ªôt batch t·ª´ DataLoader ---\")\n",
        "        print(f\"K√≠ch th∆∞·ªõc batch features (ƒë·∫ßu v√†o): {features.shape}\")\n",
        "        print(\"==> [S·ªë m·∫´u trong batch, ƒê·ªô d√†i chu·ªói (lookback), S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng]\")\n",
        "        print(f\"K√≠ch th∆∞·ªõc batch labels (ƒë·∫ßu ra): {labels.shape}\")\n",
        "\n",
        "        # In ra v√≠ d·ª• v·ªÅ gi√° tr·ªã ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a\n",
        "        print(f\"\\nV√≠ d·ª• gi√° tr·ªã label (ƒë√£ chu·∫©n h√≥a): {labels[:5].flatten()}\")\n",
        "        # S·ª≠ d·ª•ng target_scaler ƒë·ªÉ bi·∫øn ƒë·ªïi ng∆∞·ª£c l·∫°i v·ªÅ gi√° tr·ªã g·ªëc\n",
        "        original_labels = target_scaler.inverse_transform(labels.numpy().reshape(-1, 1))\n",
        "        print(f\"V√≠ d·ª• gi√° tr·ªã label (ƒë√£ gi·∫£i chu·∫©n h√≥a): \\n{original_labels[:5].flatten()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16wvTFBDFrma",
        "outputId": "5c19aa34-db3b-4e2d-f3d7-6ff1768e72f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIpKTrPzHKtb",
        "outputId": "5bed9dd8-6256-418d-9c2e-fd6b0e176a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "\n",
            "--- Ki·ªÉm tra m·ªôt batch t·ª´ DataLoader ---\n",
            "K√≠ch th∆∞·ªõc batch features (ƒë·∫ßu v√†o): torch.Size([32, 30, 150])\n",
            "==> [S·ªë m·∫´u trong batch, ƒê·ªô d√†i chu·ªói (lookback), S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng]\n",
            "K√≠ch th∆∞·ªõc batch labels (ƒë·∫ßu ra): torch.Size([32])\n",
            "\n",
            "V√≠ d·ª• gi√° tr·ªã label (ƒë√£ chu·∫©n h√≥a): tensor([0.3987, 0.6804, 0.3341, 0.7571, 0.4149])\n",
            "V√≠ d·ª• gi√° tr·ªã label (ƒë√£ gi·∫£i chu·∫©n h√≥a): \n",
            "[1062.19 1168.4  1037.84 1197.33 1068.31]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "TRAINED_MODEL_PATH = 'mslstm_model.pth' #l∆∞u model\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- Tham s·ªë Ki·∫øn tr√∫c Model ---\n",
        "# S·ªë ƒë·∫∑c tr∆∞ng g·ªëc\n",
        "# (OHLCV + 5 ch·ªâ b√°o) * 3 ch·ªâ s·ªë = 10 * 3 = 30\n",
        "NUM_BASE_FEATURES = 30\n",
        "# S·ªë quy m√¥ wavelet = level + 1 (s·ªë nh√°nh lstm)\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "\n",
        "# C√°c tham s·ªë cho l·ªõp LSTM\n",
        "LSTM_HIDDEN_UNITS = 64 # S·ªë unit trong m·ªói cell LSTM\n",
        "LSTM_NUM_LAYERS = 2    # S·ªë l·ªõp LSTM ch·ªìng l√™n nhau trong m·ªói nh√°nh\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ACQ4DNXIsiW",
        "outputId": "e2ed30eb-f4ca-48c6-e593-5d94d142304f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B√¢y gi·ªù s·∫Ω buld MS_LSTM baseline c∆° b·∫£n"
      ],
      "metadata": {
        "id": "djnH3-w2Kz8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    BATCH_SIZE, LOOKBACK_WINDOW\n",
        ")\n",
        "\n",
        "class MSLSTM(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers):\n",
        "        \"\"\"  Args:\n",
        "            input_feature_size (int): S·ªë ƒë·∫∑c tr∆∞ng cho m·ªói quy m√¥ (nh√°nh).\n",
        "            num_scales (int): S·ªë l∆∞·ª£ng quy m√¥ song song (s·ªë nh√°nh LSTM).\n",
        "            lstm_hidden_units (int): S·ªë unit ·∫©n trong m·ªói LSTM.\n",
        "            lstm_num_layers (int): S·ªë l·ªõp trong m·ªói LSTM.\n",
        "        \"\"\"\n",
        "        super(MSLSTM, self).__init__()\n",
        "\n",
        "        self.num_scales = num_scales\n",
        "        self.input_feature_size = input_feature_size\n",
        "\n",
        "        # T·∫°o ra m·ªôt danh s√°ch c√°c nh√°nh LSTM song song\n",
        "        # M·ªói nh√°nh l√† m·ªôt m·∫°ng LSTM ƒë·ªôc l·∫≠p\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(\n",
        "                input_size=input_feature_size,\n",
        "                hidden_size=lstm_hidden_units,\n",
        "                num_layers=lstm_num_layers,\n",
        "                batch_first=True, # ƒê·ªãnh d·∫°ng input: (batch, seq_len, features)\n",
        "                dropout=0.2 if lstm_num_layers > 1 else 0 # Th√™m dropout n·∫øu c√≥ nhi·ªÅu l·ªõp\n",
        "            ) for _ in range(num_scales)\n",
        "        ])\n",
        "\n",
        "        # L·ªõp Fully Connected cu·ªëi c√πng ƒë·ªÉ h·ª£p nh·∫•t k·∫øt qu·∫£ t·ª´ c√°c nh√°nh\n",
        "        # v√† ƒë∆∞a ra d·ª± b√°o cu·ªëi c√πng.\n",
        "        # ƒê·∫ßu v√†o c·ªßa n√≥ l√† t·ªïng s·ªë unit ·∫©n t·ª´ t·∫•t c·∫£ c√°c nh√°nh\n",
        "        self.fc = nn.Linear(in_features=lstm_hidden_units * num_scales, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        ƒê·ªãnh nghƒ©a lu·ªìng d·ªØ li·ªáu ƒëi qua model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor ƒë·∫ßu v√†o v·ªõi shape [batch_size, lookback_window, total_features]\n",
        "                               Trong ƒë√≥ total_features = input_feature_size * num_scales\n",
        "        \"\"\"\n",
        "        # Danh s√°ch ƒë·ªÉ l∆∞u tr·ªØ ƒë·∫ßu ra t·ª´ m·ªói nh√°nh LSTM\n",
        "        lstm_outputs = []\n",
        "\n",
        "        # T√°ch d·ªØ li·ªáu ƒë·∫ßu v√†o v√† ƒë∆∞a v√†o t·ª´ng nh√°nh LSTM t∆∞∆°ng ·ª©ng\n",
        "        for i in range(self.num_scales):\n",
        "            # T√°ch ra ph·∫ßn d·ªØ li·ªáu cho nh√°nh th·ª© i\n",
        "            start_idx = i * self.input_feature_size\n",
        "            end_idx = (i + 1) * self.input_feature_size\n",
        "\n",
        "            branch_input = x[:, :, start_idx:end_idx]\n",
        "\n",
        "            # ƒê∆∞a d·ªØ li·ªáu qua nh√°nh LSTM th·ª© i\n",
        "            # Ch√∫ng ta ch·ªâ c·∫ßn ƒë·∫ßu ra cu·ªëi c√πng c·ªßa chu·ªói (hidden_state)\n",
        "            _, (hn, _) = self.lstm_branches[i](branch_input)\n",
        "\n",
        "            # L·∫•y hidden state c·ªßa l·ªõp cu·ªëi c√πng\n",
        "            # Shape c·ªßa hn l√† [num_layers, batch_size, hidden_units]\n",
        "            # L·∫•y output c·ªßa layer cu·ªëi c√πng: hn[-1]\n",
        "            lstm_outputs.append(hn[-1])\n",
        "\n",
        "        # Concatenate c√°c ƒë·∫ßu ra t·ª´ t·∫•t c·∫£ c√°c nh√°nh l·∫°i v·ªõi nhau\n",
        "        # Shape s·∫Ω l√† [batch_size, lstm_hidden_units * num_scales]\n",
        "        concatenated_output = torch.cat(lstm_outputs, dim=1)\n",
        "\n",
        "        # ƒê∆∞a qua l·ªõp fully connected ƒë·ªÉ c√≥ d·ª± b√°o cu·ªëi c√πng\n",
        "        final_prediction = self.fc(concatenated_output)\n",
        "\n",
        "        return final_prediction.squeeze() # Tr·∫£ v·ªÅ tensor 1 chi·ªÅu\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"--- Ki·ªÉm tra ki·∫øn tr√∫c MS-LSTM ---\")\n",
        "\n",
        "    # D·ªØ li·ªáu gi·∫£ l·∫≠p\n",
        "    batch_size = BATCH_SIZE\n",
        "    lookback = LOOKBACK_WINDOW\n",
        "    total_features = NUM_BASE_FEATURES * NUM_SCALES\n",
        "    dummy_input = torch.randn(batch_size, lookback, total_features)\n",
        "\n",
        "    # Kh·ªüi t·∫°o model\n",
        "    model = MSLSTM(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS\n",
        "    )\n",
        "\n",
        "    # In ra ki·∫øn tr√∫c model\n",
        "    print(model)\n",
        "\n",
        "    # ƒê∆∞a d·ªØ li·ªáu gi·∫£ qua model\n",
        "    output = model(dummy_input)\n",
        "\n",
        "    # Ki·ªÉm tra shape c·ªßa ƒë·∫ßu ra\n",
        "    print(f\"\\nShape c·ªßa input gi·∫£: {dummy_input.shape}\")\n",
        "    print(f\"Shape c·ªßa output: {output.shape}\")\n",
        "    print(f\"==> Shape mong mu·ªën l√† [{batch_size}], k·∫øt qu·∫£: {output.shape[0] == batch_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI_4Xmc4K_Ym",
        "outputId": "ad4fffb0-57f5-4952-bb0d-8704fa151d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBZ3u6UXLyhj",
        "outputId": "ac02eba5-81a7-4e4f-83a2-f25eb2117621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ki·ªÉm tra ki·∫øn tr√∫c MS-LSTM ---\n",
            "MSLSTM(\n",
            "  (lstm_branches): ModuleList(\n",
            "    (0-4): 5 x LSTM(30, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (fc): Linear(in_features=320, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Shape c·ªßa input gi·∫£: torch.Size([32, 30, 150])\n",
            "Shape c·ªßa output: torch.Size([32])\n",
            "==> Shape mong mu·ªën l√† [32], k·∫øt qu·∫£: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "9KCGLAMnON9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    LEARNING_RATE, NUM_EPOCHS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from dataset import get_data_loaders\n",
        "from model import MSLSTM\n",
        "\n",
        "def run_training():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ th·ª±c hi·ªán to√†n b·ªô qu√° tr√¨nh hu·∫•n luy·ªán.\"\"\"\n",
        "\n",
        "    # 0. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu (L·∫•y c·∫£ train v√† test loader)\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. Kh·ªüi t·∫°o Model, Loss, Optimizer\n",
        "    model = MSLSTM(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # 3. V√≤ng l·∫∑p hu·∫•n luy·ªán\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán ---\")\n",
        "    best_val_loss = float('inf') # Bi·∫øn ƒë·ªÉ l∆∞u val_loss t·ªët nh·∫•t\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # --- PHA HU·∫§N LUY·ªÜN (TRAINING PHASE) ---\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        # S·ª≠ d·ª•ng tqdm cho ƒë·∫πp\n",
        "        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # --- PHA KI·ªÇM ƒê·ªäNH (VALIDATION PHASE) ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Valid]\"):\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = loss_function(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # --- IN K·∫æT QU·∫¢ V√Ä L∆ØU MODEL T·ªêT NH·∫§T ---\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Ch·ªâ l∆∞u model n·∫øu validation loss c·ªßa epoch n√†y t·ªët h∆°n c√°c epoch tr∆∞·ªõc\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), TRAINED_MODEL_PATH)\n",
        "            print(f\"   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o '{TRAINED_MODEL_PATH}'\")\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "    print(f\"‚úÖ Model t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = {best_val_loss:.6f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPUsi8rIOSit",
        "outputId": "d271c886-3a8e-4df5-ca66-fe1ad8b84133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60TNNbfZPptX",
        "outputId": "8bccf9fc-3ef5-476d-89eb-fc8c54868fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 12, in <module>\n",
            "    from dataset import get_data_loaders\n",
            "ModuleNotFoundError: No module named 'dataset'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B√¢y gi·ªù s·∫Ω ch·∫°y evaluate"
      ],
      "metadata": {
        "id": "Ru9APmOkRa5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "# evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    TRAINED_MODEL_PATH\n",
        ")\n",
        "from dataset import get_data_loaders\n",
        "from model import MSLSTM\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test.\"\"\"\n",
        "\n",
        "    # 0. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu (ch·ªâ c·∫ßn test_loader v√† scaler)\n",
        "    _, test_loader, _, target_scaler = get_data_loaders()\n",
        "    if not test_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. T·∫£i l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "    print(f\"ƒêang t·∫£i m√¥ h√¨nh t·ª´: {TRAINED_MODEL_PATH}\")\n",
        "    model = MSLSTM(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n",
        "        model.to(device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i '{TRAINED_MODEL_PATH}'.\")\n",
        "        print(\"Vui l√≤ng ch·∫°y train.py tr∆∞·ªõc.\")\n",
        "        return\n",
        "\n",
        "    # 3. ƒê√°nh gi√° tr√™n t·∫≠p Test\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 4. Gi·∫£i chu·∫©n h√≥a ƒë·ªÉ so s√°nh\n",
        "    predictions = np.array(predictions).reshape(-1, 1)\n",
        "    actuals = np.array(actuals).reshape(-1, 1)\n",
        "\n",
        "    original_predictions = target_scaler.inverse_transform(predictions)\n",
        "    original_actuals = target_scaler.inverse_transform(actuals)\n",
        "\n",
        "    # 5. T√≠nh to√°n sai s·ªë v√† in k·∫øt qu·∫£\n",
        "    mae = np.mean(np.abs(original_predictions - original_actuals))\n",
        "    print(f\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test ---\")\n",
        "    print(f\"Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # 6. V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_actuals, label='Gi√° tr·ªã Th·ª±c t·∫ø (Actuals)', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_predictions, label='Gi√° tr·ªã D·ª± ƒëo√°n (Predictions)', color='red', linestyle='--')\n",
        "    plt.title('So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n tr√™n T·∫≠p Test')\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    output_image_path = 'prediction_vs_actual.png'\n",
        "    plt.savefig(output_image_path)\n",
        "    print(f\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file '{output_image_path}'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZxU9jyVRi1W",
        "outputId": "16cd51dd-cfec-43c9-aebc-9188ef43fd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkjYoJ6lRkUy",
        "outputId": "4653fdde-5356-4f14-d75a-9e6aa03bed06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "ƒêang t·∫£i m√¥ h√¨nh t·ª´: mslstm_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test ---\n",
            "Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 20.1838 (ƒëi·ªÉm VN-Index)\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file 'prediction_vs_actual.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B√¢y gi·ªù test t√≠ch h·ª£p c∆° ch·∫ø Ch√∫ √Ω ƒêa ƒë·∫ßu (Multi-Head Attention)."
      ],
      "metadata": {
        "id": "ahNvmKT9T1f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "TRAINED_MODEL_PATH = 'mslstm_attention_model.pth' # ƒê·ªïi t√™n file model m·ªõi\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- Tham s·ªë Ki·∫øn tr√∫c Model ---\n",
        "NUM_BASE_FEATURES = 30\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "LSTM_HIDDEN_UNITS = 64\n",
        "LSTM_NUM_LAYERS = 2\n",
        "# --- THAM S·ªê M·ªöI CHO ATTENTION ---\n",
        "ATTENTION_NUM_HEADS = 4 # S·ªë \"ƒë·∫ßu\" ch√∫ √Ω\n",
        "\n",
        "# --- Tham s·ªë Hu·∫•n luy·ªán ---\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSAynKxwUGER",
        "outputId": "253fce6f-2eea-4d87-d2df-dad66be23899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***MS-LSTM + Attention***"
      ],
      "metadata": {
        "id": "WuezT0kMUNQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    BATCH_SIZE, LOOKBACK_WINDOW, ATTENTION_NUM_HEADS\n",
        ")\n",
        "\n",
        "class MSLSTMAttention(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(MSLSTMAttention, self).__init__()\n",
        "\n",
        "        self.num_scales = num_scales\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.lstm_hidden_units = lstm_hidden_units\n",
        "\n",
        "        # 1. C√°c nh√°nh LSTM song song (Kh√¥ng ƒë·ªïi)\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(\n",
        "                input_size=input_feature_size,\n",
        "                hidden_size=lstm_hidden_units,\n",
        "                num_layers=lstm_num_layers,\n",
        "                batch_first=True,\n",
        "                dropout=0.2 if lstm_num_layers > 1 else 0\n",
        "            ) for _ in range(num_scales)\n",
        "        ])\n",
        "\n",
        "        # 2. L·ªõp Ch√∫ √Ω ƒêa ƒë·∫ßu (Multi-Head Attention)\n",
        "        # N√≥ s·∫Ω nh·∫≠n ƒë·∫ßu v√†o t·ª´ c√°c nh√°nh LSTM ƒë√£ ƒë∆∞·ª£c gh√©p l·∫°i\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden_units * num_scales, # T·ªïng k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # 3. L·ªõp Fully Connected cu·ªëi c√πng\n",
        "        # ƒê·∫ßu v√†o c·ªßa n√≥ v·∫´n l√† output t·ª´ l·ªõp Attention\n",
        "        self.fc = nn.Linear(in_features=lstm_hidden_units * num_scales, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Danh s√°ch ƒë·ªÉ l∆∞u tr·ªØ ƒë·∫ßu ra t·ª´ m·ªói nh√°nh LSTM\n",
        "        # L·∫ßn n√†y ch√∫ng ta c·∫ßn to√†n b·ªô chu·ªói ƒë·∫ßu ra (output), kh√¥ng ch·ªâ hidden state cu·ªëi\n",
        "        branch_outputs = []\n",
        "\n",
        "        for i in range(self.num_scales):\n",
        "            branch_input = x[:, :, i*self.input_feature_size : (i+1)*self.input_feature_size]\n",
        "\n",
        "            # output shape: [batch_size, seq_len, hidden_units]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        # Gh√©p (concatenate) c√°c chu·ªói ƒë·∫ßu ra t·ª´ t·∫•t c·∫£ c√°c nh√°nh\n",
        "        # Shape: [batch_size, seq_len, lstm_hidden_units * num_scales]\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "\n",
        "        # ƒê∆∞a qua l·ªõp Multi-Head Attention\n",
        "        # Query, Key, Value ƒë·ªÅu l√† concatenated_output (self-attention)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "\n",
        "        # Ta ch·ªâ l·∫•y vector cu·ªëi c√πng c·ªßa chu·ªói sau khi qua attention ƒë·ªÉ d·ª± b√°o\n",
        "        # Shape: [batch_size, lstm_hidden_units * num_scales]\n",
        "        last_time_step_output = attention_output[:, -1, :]\n",
        "\n",
        "        # ƒê∆∞a qua l·ªõp fully connected ƒë·ªÉ c√≥ d·ª± b√°o cu·ªëi c√πng\n",
        "        final_prediction = self.fc(last_time_step_output)\n",
        "\n",
        "        return final_prediction.squeeze()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ƒêo·∫°n code ki·ªÉm tra ki·∫øn tr√∫c model m·ªõi\n",
        "    print(\"--- Ki·ªÉm tra ki·∫øn tr√∫c MS-LSTM + Attention ---\")\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    print(model)\n",
        "    dummy_input = torch.randn(BATCH_SIZE, LOOKBACK_WINDOW, NUM_BASE_FEATURES * NUM_SCALES)\n",
        "    output = model(dummy_input)\n",
        "    print(f\"\\nShape c·ªßa input gi·∫£: {dummy_input.shape}\")\n",
        "    print(f\"Shape c·ªßa output: {output.shape}\")\n",
        "    print(f\"==> Shape mong mu·ªën l√† [{BATCH_SIZE}], k·∫øt qu·∫£: {output.shape[0] == BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DgnF98gUUEV",
        "outputId": "7ef5d01d-2e21-4ffc-b78e-dc177c773454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMESaAZeUiYg",
        "outputId": "65b23cf0-aacf-4118-c684-15a2f59e28af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ki·ªÉm tra ki·∫øn tr√∫c MS-LSTM + Attention ---\n",
            "MSLSTMAttention(\n",
            "  (lstm_branches): ModuleList(\n",
            "    (0-4): 5 x LSTM(30, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=320, out_features=320, bias=True)\n",
            "  )\n",
            "  (fc): Linear(in_features=320, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Shape c·ªßa input gi·∫£: torch.Size([32, 30, 150])\n",
            "Shape c·ªßa output: torch.Size([32])\n",
            "==> Shape mong mu·ªën l√† [32], k·∫øt qu·∫£: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, # Th√™m tham s·ªë attention\n",
        "    LEARNING_RATE, NUM_EPOCHS, TRAINED_MODEL_PATH\n",
        ")\n",
        "# S·ª≠a ƒë·ªïi: import model m·ªõi\n",
        "from model import MSLSTMAttention\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ th·ª±c hi·ªán to√†n b·ªô qu√° tr√¨nh hu·∫•n luy·ªán.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    #Kh·ªüi t·∫°o model MSLSTMAttention m·ªõi\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    ).to(device)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model MS-LSTM + Attention ---\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Valid]\"):\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = loss_function(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), TRAINED_MODEL_PATH)\n",
        "            print(f\"   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o '{TRAINED_MODEL_PATH}'\")\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "    print(f\"‚úÖ Model (Attention) t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = {best_val_loss:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Z6cu1eVHqa",
        "outputId": "0784e369-0a1a-43d9-d222-5f8d487e7595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvOUn90uVNT7",
        "outputId": "1f965882-f201-438c-cb4a-1f157bd4c57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "\n",
            "--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model MS-LSTM + Attention ---\n",
            "Epoch 1/50 [Train]: 100% 9/9 [00:00<00:00, 19.58it/s]\n",
            "Epoch 1/50 [Valid]: 100% 3/3 [00:00<00:00, 56.81it/s]\n",
            "Epoch [01/50] | Train Loss: 0.121739 | Validation Loss: 0.069954\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 2/50 [Train]: 100% 9/9 [00:00<00:00, 71.69it/s]\n",
            "Epoch 2/50 [Valid]: 100% 3/3 [00:00<00:00, 308.01it/s]\n",
            "Epoch [02/50] | Train Loss: 0.060295 | Validation Loss: 0.007198\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 3/50 [Train]: 100% 9/9 [00:00<00:00, 72.17it/s]\n",
            "Epoch 3/50 [Valid]: 100% 3/3 [00:00<00:00, 314.75it/s]\n",
            "Epoch [03/50] | Train Loss: 0.044625 | Validation Loss: 0.005162\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 4/50 [Train]: 100% 9/9 [00:00<00:00, 59.06it/s]\n",
            "Epoch 4/50 [Valid]: 100% 3/3 [00:00<00:00, 327.27it/s]\n",
            "Epoch [04/50] | Train Loss: 0.038087 | Validation Loss: 0.009235\n",
            "Epoch 5/50 [Train]: 100% 9/9 [00:00<00:00, 76.79it/s]\n",
            "Epoch 5/50 [Valid]: 100% 3/3 [00:00<00:00, 330.72it/s]\n",
            "Epoch [05/50] | Train Loss: 0.037213 | Validation Loss: 0.009940\n",
            "Epoch 6/50 [Train]: 100% 9/9 [00:00<00:00, 72.89it/s]\n",
            "Epoch 6/50 [Valid]: 100% 3/3 [00:00<00:00, 296.20it/s]\n",
            "Epoch [06/50] | Train Loss: 0.036943 | Validation Loss: 0.005199\n",
            "Epoch 7/50 [Train]: 100% 9/9 [00:00<00:00, 72.72it/s]\n",
            "Epoch 7/50 [Valid]: 100% 3/3 [00:00<00:00, 285.12it/s]\n",
            "Epoch [07/50] | Train Loss: 0.038063 | Validation Loss: 0.010852\n",
            "Epoch 8/50 [Train]: 100% 9/9 [00:00<00:00, 63.16it/s]\n",
            "Epoch 8/50 [Valid]: 100% 3/3 [00:00<00:00, 238.72it/s]\n",
            "Epoch [08/50] | Train Loss: 0.035196 | Validation Loss: 0.015178\n",
            "Epoch 9/50 [Train]: 100% 9/9 [00:00<00:00, 64.52it/s]\n",
            "Epoch 9/50 [Valid]: 100% 3/3 [00:00<00:00, 304.32it/s]\n",
            "Epoch [09/50] | Train Loss: 0.039936 | Validation Loss: 0.005516\n",
            "Epoch 10/50 [Train]: 100% 9/9 [00:00<00:00, 65.89it/s]\n",
            "Epoch 10/50 [Valid]: 100% 3/3 [00:00<00:00, 258.15it/s]\n",
            "Epoch [10/50] | Train Loss: 0.038311 | Validation Loss: 0.012636\n",
            "Epoch 11/50 [Train]: 100% 9/9 [00:00<00:00, 55.23it/s]\n",
            "Epoch 11/50 [Valid]: 100% 3/3 [00:00<00:00, 266.96it/s]\n",
            "Epoch [11/50] | Train Loss: 0.038709 | Validation Loss: 0.004639\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 12/50 [Train]: 100% 9/9 [00:00<00:00, 62.44it/s]\n",
            "Epoch 12/50 [Valid]: 100% 3/3 [00:00<00:00, 254.37it/s]\n",
            "Epoch [12/50] | Train Loss: 0.040524 | Validation Loss: 0.004308\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 13/50 [Train]: 100% 9/9 [00:00<00:00, 93.57it/s]\n",
            "Epoch 13/50 [Valid]: 100% 3/3 [00:00<00:00, 370.64it/s]\n",
            "Epoch [13/50] | Train Loss: 0.043210 | Validation Loss: 0.022547\n",
            "Epoch 14/50 [Train]: 100% 9/9 [00:00<00:00, 90.52it/s]\n",
            "Epoch 14/50 [Valid]: 100% 3/3 [00:00<00:00, 405.03it/s]\n",
            "Epoch [14/50] | Train Loss: 0.041569 | Validation Loss: 0.018454\n",
            "Epoch 15/50 [Train]: 100% 9/9 [00:00<00:00, 110.44it/s]\n",
            "Epoch 15/50 [Valid]: 100% 3/3 [00:00<00:00, 310.03it/s]\n",
            "Epoch [15/50] | Train Loss: 0.043210 | Validation Loss: 0.004447\n",
            "Epoch 16/50 [Train]: 100% 9/9 [00:00<00:00, 97.56it/s]\n",
            "Epoch 16/50 [Valid]: 100% 3/3 [00:00<00:00, 412.14it/s]\n",
            "Epoch [16/50] | Train Loss: 0.041268 | Validation Loss: 0.004071\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "Epoch 17/50 [Train]: 100% 9/9 [00:00<00:00, 108.87it/s]\n",
            "Epoch 17/50 [Valid]: 100% 3/3 [00:00<00:00, 266.23it/s]\n",
            "Epoch [17/50] | Train Loss: 0.037797 | Validation Loss: 0.019599\n",
            "Epoch 18/50 [Train]: 100% 9/9 [00:00<00:00, 107.26it/s]\n",
            "Epoch 18/50 [Valid]: 100% 3/3 [00:00<00:00, 412.46it/s]\n",
            "Epoch [18/50] | Train Loss: 0.038907 | Validation Loss: 0.012615\n",
            "Epoch 19/50 [Train]: 100% 9/9 [00:00<00:00, 107.39it/s]\n",
            "Epoch 19/50 [Valid]: 100% 3/3 [00:00<00:00, 425.27it/s]\n",
            "Epoch [19/50] | Train Loss: 0.035819 | Validation Loss: 0.006886\n",
            "Epoch 20/50 [Train]: 100% 9/9 [00:00<00:00, 107.11it/s]\n",
            "Epoch 20/50 [Valid]: 100% 3/3 [00:00<00:00, 395.12it/s]\n",
            "Epoch [20/50] | Train Loss: 0.038219 | Validation Loss: 0.017881\n",
            "Epoch 21/50 [Train]: 100% 9/9 [00:00<00:00, 99.12it/s]\n",
            "Epoch 21/50 [Valid]: 100% 3/3 [00:00<00:00, 388.90it/s]\n",
            "Epoch [21/50] | Train Loss: 0.037392 | Validation Loss: 0.008938\n",
            "Epoch 22/50 [Train]: 100% 9/9 [00:00<00:00, 107.29it/s]\n",
            "Epoch 22/50 [Valid]: 100% 3/3 [00:00<00:00, 392.75it/s]\n",
            "Epoch [22/50] | Train Loss: 0.037610 | Validation Loss: 0.004370\n",
            "Epoch 23/50 [Train]: 100% 9/9 [00:00<00:00, 106.43it/s]\n",
            "Epoch 23/50 [Valid]: 100% 3/3 [00:00<00:00, 413.65it/s]\n",
            "Epoch [23/50] | Train Loss: 0.036415 | Validation Loss: 0.015123\n",
            "Epoch 24/50 [Train]: 100% 9/9 [00:00<00:00, 104.43it/s]\n",
            "Epoch 24/50 [Valid]: 100% 3/3 [00:00<00:00, 407.72it/s]\n",
            "Epoch [24/50] | Train Loss: 0.037440 | Validation Loss: 0.012695\n",
            "Epoch 25/50 [Train]: 100% 9/9 [00:00<00:00, 100.17it/s]\n",
            "Epoch 25/50 [Valid]: 100% 3/3 [00:00<00:00, 357.68it/s]\n",
            "Epoch [25/50] | Train Loss: 0.036915 | Validation Loss: 0.010367\n",
            "Epoch 26/50 [Train]: 100% 9/9 [00:00<00:00, 83.62it/s]\n",
            "Epoch 26/50 [Valid]: 100% 3/3 [00:00<00:00, 259.62it/s]\n",
            "Epoch [26/50] | Train Loss: 0.038865 | Validation Loss: 0.006366\n",
            "Epoch 27/50 [Train]: 100% 9/9 [00:00<00:00, 99.41it/s]\n",
            "Epoch 27/50 [Valid]: 100% 3/3 [00:00<00:00, 427.70it/s]\n",
            "Epoch [27/50] | Train Loss: 0.035979 | Validation Loss: 0.006176\n",
            "Epoch 28/50 [Train]: 100% 9/9 [00:00<00:00, 107.11it/s]\n",
            "Epoch 28/50 [Valid]: 100% 3/3 [00:00<00:00, 427.29it/s]\n",
            "Epoch [28/50] | Train Loss: 0.035361 | Validation Loss: 0.016217\n",
            "Epoch 29/50 [Train]: 100% 9/9 [00:00<00:00, 103.96it/s]\n",
            "Epoch 29/50 [Valid]: 100% 3/3 [00:00<00:00, 423.05it/s]\n",
            "Epoch [29/50] | Train Loss: 0.036157 | Validation Loss: 0.004897\n",
            "Epoch 30/50 [Train]: 100% 9/9 [00:00<00:00, 72.16it/s]\n",
            "Epoch 30/50 [Valid]: 100% 3/3 [00:00<00:00, 416.20it/s]\n",
            "Epoch [30/50] | Train Loss: 0.036940 | Validation Loss: 0.011907\n",
            "Epoch 31/50 [Train]: 100% 9/9 [00:00<00:00, 103.76it/s]\n",
            "Epoch 31/50 [Valid]: 100% 3/3 [00:00<00:00, 419.71it/s]\n",
            "Epoch [31/50] | Train Loss: 0.036750 | Validation Loss: 0.007819\n",
            "Epoch 32/50 [Train]: 100% 9/9 [00:00<00:00, 107.63it/s]\n",
            "Epoch 32/50 [Valid]: 100% 3/3 [00:00<00:00, 393.06it/s]\n",
            "Epoch [32/50] | Train Loss: 0.037632 | Validation Loss: 0.017556\n",
            "Epoch 33/50 [Train]: 100% 9/9 [00:00<00:00, 106.21it/s]\n",
            "Epoch 33/50 [Valid]: 100% 3/3 [00:00<00:00, 402.01it/s]\n",
            "Epoch [33/50] | Train Loss: 0.038912 | Validation Loss: 0.015610\n",
            "Epoch 34/50 [Train]: 100% 9/9 [00:00<00:00, 105.94it/s]\n",
            "Epoch 34/50 [Valid]: 100% 3/3 [00:00<00:00, 410.83it/s]\n",
            "Epoch [34/50] | Train Loss: 0.037242 | Validation Loss: 0.008161\n",
            "Epoch 35/50 [Train]: 100% 9/9 [00:00<00:00, 101.47it/s]\n",
            "Epoch 35/50 [Valid]: 100% 3/3 [00:00<00:00, 393.27it/s]\n",
            "Epoch [35/50] | Train Loss: 0.035496 | Validation Loss: 0.018227\n",
            "Epoch 36/50 [Train]: 100% 9/9 [00:00<00:00, 106.57it/s]\n",
            "Epoch 36/50 [Valid]: 100% 3/3 [00:00<00:00, 436.00it/s]\n",
            "Epoch [36/50] | Train Loss: 0.038077 | Validation Loss: 0.007279\n",
            "Epoch 37/50 [Train]: 100% 9/9 [00:00<00:00, 87.47it/s]\n",
            "Epoch 37/50 [Valid]: 100% 3/3 [00:00<00:00, 361.30it/s]\n",
            "Epoch [37/50] | Train Loss: 0.037043 | Validation Loss: 0.006419\n",
            "Epoch 38/50 [Train]: 100% 9/9 [00:00<00:00, 109.53it/s]\n",
            "Epoch 38/50 [Valid]: 100% 3/3 [00:00<00:00, 383.93it/s]\n",
            "Epoch [38/50] | Train Loss: 0.035614 | Validation Loss: 0.013309\n",
            "Epoch 39/50 [Train]: 100% 9/9 [00:00<00:00, 82.25it/s]\n",
            "Epoch 39/50 [Valid]: 100% 3/3 [00:00<00:00, 293.53it/s]\n",
            "Epoch [39/50] | Train Loss: 0.037760 | Validation Loss: 0.014261\n",
            "Epoch 40/50 [Train]: 100% 9/9 [00:00<00:00, 107.46it/s]\n",
            "Epoch 40/50 [Valid]: 100% 3/3 [00:00<00:00, 389.70it/s]\n",
            "Epoch [40/50] | Train Loss: 0.037344 | Validation Loss: 0.014474\n",
            "Epoch 41/50 [Train]: 100% 9/9 [00:00<00:00, 106.46it/s]\n",
            "Epoch 41/50 [Valid]: 100% 3/3 [00:00<00:00, 412.11it/s]\n",
            "Epoch [41/50] | Train Loss: 0.038288 | Validation Loss: 0.008317\n",
            "Epoch 42/50 [Train]: 100% 9/9 [00:00<00:00, 106.21it/s]\n",
            "Epoch 42/50 [Valid]: 100% 3/3 [00:00<00:00, 423.77it/s]\n",
            "Epoch [42/50] | Train Loss: 0.035819 | Validation Loss: 0.009478\n",
            "Epoch 43/50 [Train]: 100% 9/9 [00:00<00:00, 105.29it/s]\n",
            "Epoch 43/50 [Valid]: 100% 3/3 [00:00<00:00, 387.08it/s]\n",
            "Epoch [43/50] | Train Loss: 0.037534 | Validation Loss: 0.006483\n",
            "Epoch 44/50 [Train]: 100% 9/9 [00:00<00:00, 105.60it/s]\n",
            "Epoch 44/50 [Valid]: 100% 3/3 [00:00<00:00, 407.27it/s]\n",
            "Epoch [44/50] | Train Loss: 0.036859 | Validation Loss: 0.008407\n",
            "Epoch 45/50 [Train]: 100% 9/9 [00:00<00:00, 103.92it/s]\n",
            "Epoch 45/50 [Valid]: 100% 3/3 [00:00<00:00, 425.17it/s]\n",
            "Epoch [45/50] | Train Loss: 0.035221 | Validation Loss: 0.012612\n",
            "Epoch 46/50 [Train]: 100% 9/9 [00:00<00:00, 92.72it/s]\n",
            "Epoch 46/50 [Valid]: 100% 3/3 [00:00<00:00, 364.65it/s]\n",
            "Epoch [46/50] | Train Loss: 0.036828 | Validation Loss: 0.009421\n",
            "Epoch 47/50 [Train]: 100% 9/9 [00:00<00:00, 82.13it/s]\n",
            "Epoch 47/50 [Valid]: 100% 3/3 [00:00<00:00, 324.88it/s]\n",
            "Epoch [47/50] | Train Loss: 0.034655 | Validation Loss: 0.009992\n",
            "Epoch 48/50 [Train]: 100% 9/9 [00:00<00:00, 101.12it/s]\n",
            "Epoch 48/50 [Valid]: 100% 3/3 [00:00<00:00, 365.27it/s]\n",
            "Epoch [48/50] | Train Loss: 0.034862 | Validation Loss: 0.012069\n",
            "Epoch 49/50 [Train]: 100% 9/9 [00:00<00:00, 79.61it/s]\n",
            "Epoch 49/50 [Valid]: 100% 3/3 [00:00<00:00, 404.34it/s]\n",
            "Epoch [49/50] | Train Loss: 0.036830 | Validation Loss: 0.006667\n",
            "Epoch 50/50 [Train]: 100% 9/9 [00:00<00:00, 107.31it/s]\n",
            "Epoch 50/50 [Valid]: 100% 3/3 [00:00<00:00, 441.86it/s]\n",
            "Epoch [50/50] | Train Loss: 0.044091 | Validation Loss: 0.003992\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_model.pth'\n",
            "\n",
            "--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\n",
            "‚úÖ Model (Attention) t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = 0.003992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluate sau train***"
      ],
      "metadata": {
        "id": "2RJ77CzmVdV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "# evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS,\n",
        "    TRAINED_MODEL_PATH\n",
        ")\n",
        "#import model m·ªõi\n",
        "from model import MSLSTMAttention\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu (ch·ªâ c·∫ßn test_loader v√† scaler)\n",
        "    _, test_loader, _, target_scaler = get_data_loaders()\n",
        "    if not test_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. T·∫£i l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "    print(f\"ƒêang t·∫£i m√¥ h√¨nh t·ª´: {TRAINED_MODEL_PATH}\")\n",
        "    # S·ª≠a ƒë·ªïi: Kh·ªüi t·∫°o ƒë√∫ng class model m·ªõi\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n",
        "        model.to(device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i '{TRAINED_MODEL_PATH}'.\")\n",
        "        print(\"Vui l√≤ng ch·∫°y train.py tr∆∞·ªõc.\")\n",
        "        return\n",
        "\n",
        "    # 3. ƒê√°nh gi√° tr√™n t·∫≠p Test\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 4. Gi·∫£i chu·∫©n h√≥a ƒë·ªÉ so s√°nh\n",
        "    predictions = np.array(predictions).reshape(-1, 1)\n",
        "    actuals = np.array(actuals).reshape(-1, 1)\n",
        "\n",
        "    original_predictions = target_scaler.inverse_transform(predictions)\n",
        "    original_actuals = target_scaler.inverse_transform(actuals)\n",
        "\n",
        "    # 5. T√≠nh to√°n sai s·ªë v√† in k·∫øt qu·∫£\n",
        "    mae = np.mean(np.abs(original_predictions - original_actuals))\n",
        "    print(f\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model + Attention) ---\")\n",
        "    print(f\"Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # 6. V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_actuals, label='Gi√° tr·ªã Th·ª±c t·∫ø (Actuals)', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_predictions, label='Gi√° tr·ªã D·ª± ƒëo√°n (Predictions)', color='red', linestyle='--')\n",
        "    plt.title('So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n (Model + Attention)')\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    output_image_path = 'prediction_vs_actual_attention.png'\n",
        "    plt.savefig(output_image_path)\n",
        "    print(f\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file '{output_image_path}'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w--U4Lv8Vcwo",
        "outputId": "5b0267df-3ea0-494b-b5be-144f947902de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu0DKsUSVrtF",
        "outputId": "58e9145f-9bd4-45fc-81a2-f4e87d31c025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "ƒêang t·∫£i m√¥ h√¨nh t·ª´: mslstm_attention_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model + Attention) ---\n",
            "Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 20.4384 (ƒëi·ªÉm VN-Index)\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file 'prediction_vs_actual_attention.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***MAE th·∫≠m ch√≠ c√≤n th·∫•p h∆°n, b√¢y gi·ªù s·∫Ω ch·ªânh s·ª≠a si√™u tham s·ªë***"
      ],
      "metadata": {
        "id": "SyP91Lo8WQZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "# ƒê·ªïi t√™n file model ƒë·ªÉ l∆∞u k·∫øt qu·∫£ c·ªßa l·∫ßn ch·∫°y m·ªõi\n",
        "TRAINED_MODEL_PATH = 'mslstm_attention_tuned.pth'\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- Tham s·ªë Ki·∫øn tr√∫c Model ---\n",
        "NUM_BASE_FEATURES = 30\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "# --- THAY ƒê·ªîI ---\n",
        "LSTM_HIDDEN_UNITS = 128 #TƒÉng t·ª´ 64 l√™n 148 units\n",
        "LSTM_NUM_LAYERS = 2\n",
        "ATTENTION_NUM_HEADS = 4\n",
        "\n",
        "# --- Tham s·ªë Hu·∫•n luy·ªán ---\n",
        "# --- THAY ƒê·ªîI ---\n",
        "LEARNING_RATE = 0.0005 # Gi·∫£m l·∫°i learning rate\n",
        "NUM_EPOCHS = 100       # TƒÉng epoch ƒë·ªÉ xem c√≥ ph√°t huy ƒë∆∞·ª£c kh√¥ng"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_RpMcO4Wabj",
        "outputId": "f00a9299-76f1-4d3c-dae7-9c4d25c000f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Train v√† evaluate l·∫°i config m·ªõi***"
      ],
      "metadata": {
        "id": "3M5o5N88XCoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, # Th√™m tham s·ªë attention\n",
        "    LEARNING_RATE, NUM_EPOCHS, TRAINED_MODEL_PATH\n",
        ")\n",
        "# S·ª≠a ƒë·ªïi: import model m·ªõi\n",
        "from model import MSLSTMAttention\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ th·ª±c hi·ªán to√†n b·ªô qu√° tr√¨nh hu·∫•n luy·ªán.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    #Kh·ªüi t·∫°o model MSLSTMAttention m·ªõi\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    ).to(device)\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model MS-LSTM + Attention ---\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Valid]\"):\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = loss_function(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), TRAINED_MODEL_PATH)\n",
        "            print(f\"   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o '{TRAINED_MODEL_PATH}'\")\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "    print(f\"‚úÖ Model (Attention) t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = {best_val_loss:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxcM4qdPXAf_",
        "outputId": "8af63eaf-ae1f-444e-ce56-10f65612b485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw3zTl-uW9DB",
        "outputId": "f09e107b-71fa-4328-bc36-3e5acac7097b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "\n",
            "--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model MS-LSTM + Attention ---\n",
            "Epoch 1/100 [Train]: 100% 9/9 [00:00<00:00, 26.19it/s]\n",
            "Epoch 1/100 [Valid]: 100% 3/3 [00:00<00:00, 130.50it/s]\n",
            "Epoch [01/100] | Train Loss: 0.105180 | Validation Loss: 0.050690\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 2/100 [Train]: 100% 9/9 [00:00<00:00, 85.90it/s]\n",
            "Epoch 2/100 [Valid]: 100% 3/3 [00:00<00:00, 225.87it/s]\n",
            "Epoch [02/100] | Train Loss: 0.056489 | Validation Loss: 0.012146\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 3/100 [Train]: 100% 9/9 [00:00<00:00, 86.77it/s]\n",
            "Epoch 3/100 [Valid]: 100% 3/3 [00:00<00:00, 312.46it/s]\n",
            "Epoch [03/100] | Train Loss: 0.047284 | Validation Loss: 0.004356\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 4/100 [Train]: 100% 9/9 [00:00<00:00, 101.10it/s]\n",
            "Epoch 4/100 [Valid]: 100% 3/3 [00:00<00:00, 299.09it/s]\n",
            "Epoch [04/100] | Train Loss: 0.042457 | Validation Loss: 0.021470\n",
            "Epoch 5/100 [Train]: 100% 9/9 [00:00<00:00, 104.45it/s]\n",
            "Epoch 5/100 [Valid]: 100% 3/3 [00:00<00:00, 311.11it/s]\n",
            "Epoch [05/100] | Train Loss: 0.038717 | Validation Loss: 0.010976\n",
            "Epoch 6/100 [Train]: 100% 9/9 [00:00<00:00, 87.43it/s]\n",
            "Epoch 6/100 [Valid]: 100% 3/3 [00:00<00:00, 304.52it/s]\n",
            "Epoch [06/100] | Train Loss: 0.038793 | Validation Loss: 0.004093\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 7/100 [Train]: 100% 9/9 [00:00<00:00, 100.92it/s]\n",
            "Epoch 7/100 [Valid]: 100% 3/3 [00:00<00:00, 306.65it/s]\n",
            "Epoch [07/100] | Train Loss: 0.040365 | Validation Loss: 0.017738\n",
            "Epoch 8/100 [Train]: 100% 9/9 [00:00<00:00, 102.02it/s]\n",
            "Epoch 8/100 [Valid]: 100% 3/3 [00:00<00:00, 302.70it/s]\n",
            "Epoch [08/100] | Train Loss: 0.037876 | Validation Loss: 0.009999\n",
            "Epoch 9/100 [Train]: 100% 9/9 [00:00<00:00, 105.97it/s]\n",
            "Epoch 9/100 [Valid]: 100% 3/3 [00:00<00:00, 310.04it/s]\n",
            "Epoch [09/100] | Train Loss: 0.036197 | Validation Loss: 0.009152\n",
            "Epoch 10/100 [Train]: 100% 9/9 [00:00<00:00, 102.11it/s]\n",
            "Epoch 10/100 [Valid]: 100% 3/3 [00:00<00:00, 300.14it/s]\n",
            "Epoch [10/100] | Train Loss: 0.035976 | Validation Loss: 0.018388\n",
            "Epoch 11/100 [Train]: 100% 9/9 [00:00<00:00, 91.37it/s]\n",
            "Epoch 11/100 [Valid]: 100% 3/3 [00:00<00:00, 261.08it/s]\n",
            "Epoch [11/100] | Train Loss: 0.036898 | Validation Loss: 0.004447\n",
            "Epoch 12/100 [Train]: 100% 9/9 [00:00<00:00, 99.05it/s]\n",
            "Epoch 12/100 [Valid]: 100% 3/3 [00:00<00:00, 302.65it/s]\n",
            "Epoch [12/100] | Train Loss: 0.040271 | Validation Loss: 0.006085\n",
            "Epoch 13/100 [Train]: 100% 9/9 [00:00<00:00, 100.81it/s]\n",
            "Epoch 13/100 [Valid]: 100% 3/3 [00:00<00:00, 307.93it/s]\n",
            "Epoch [13/100] | Train Loss: 0.038323 | Validation Loss: 0.021899\n",
            "Epoch 14/100 [Train]: 100% 9/9 [00:00<00:00, 107.77it/s]\n",
            "Epoch 14/100 [Valid]: 100% 3/3 [00:00<00:00, 299.95it/s]\n",
            "Epoch [14/100] | Train Loss: 0.038540 | Validation Loss: 0.010391\n",
            "Epoch 15/100 [Train]: 100% 9/9 [00:00<00:00, 107.76it/s]\n",
            "Epoch 15/100 [Valid]: 100% 3/3 [00:00<00:00, 297.98it/s]\n",
            "Epoch [15/100] | Train Loss: 0.037624 | Validation Loss: 0.010887\n",
            "Epoch 16/100 [Train]: 100% 9/9 [00:00<00:00, 91.84it/s]\n",
            "Epoch 16/100 [Valid]: 100% 3/3 [00:00<00:00, 302.93it/s]\n",
            "Epoch [16/100] | Train Loss: 0.036242 | Validation Loss: 0.009331\n",
            "Epoch 17/100 [Train]: 100% 9/9 [00:00<00:00, 106.45it/s]\n",
            "Epoch 17/100 [Valid]: 100% 3/3 [00:00<00:00, 297.95it/s]\n",
            "Epoch [17/100] | Train Loss: 0.036536 | Validation Loss: 0.006941\n",
            "Epoch 18/100 [Train]: 100% 9/9 [00:00<00:00, 106.60it/s]\n",
            "Epoch 18/100 [Valid]: 100% 3/3 [00:00<00:00, 300.32it/s]\n",
            "Epoch [18/100] | Train Loss: 0.036384 | Validation Loss: 0.010585\n",
            "Epoch 19/100 [Train]: 100% 9/9 [00:00<00:00, 106.04it/s]\n",
            "Epoch 19/100 [Valid]: 100% 3/3 [00:00<00:00, 307.49it/s]\n",
            "Epoch [19/100] | Train Loss: 0.038332 | Validation Loss: 0.006190\n",
            "Epoch 20/100 [Train]: 100% 9/9 [00:00<00:00, 107.00it/s]\n",
            "Epoch 20/100 [Valid]: 100% 3/3 [00:00<00:00, 294.78it/s]\n",
            "Epoch [20/100] | Train Loss: 0.037032 | Validation Loss: 0.016967\n",
            "Epoch 21/100 [Train]: 100% 9/9 [00:00<00:00, 99.20it/s]\n",
            "Epoch 21/100 [Valid]: 100% 3/3 [00:00<00:00, 302.94it/s]\n",
            "Epoch [21/100] | Train Loss: 0.036357 | Validation Loss: 0.004238\n",
            "Epoch 22/100 [Train]: 100% 9/9 [00:00<00:00, 104.42it/s]\n",
            "Epoch 22/100 [Valid]: 100% 3/3 [00:00<00:00, 308.59it/s]\n",
            "Epoch [22/100] | Train Loss: 0.037633 | Validation Loss: 0.006475\n",
            "Epoch 23/100 [Train]: 100% 9/9 [00:00<00:00, 100.74it/s]\n",
            "Epoch 23/100 [Valid]: 100% 3/3 [00:00<00:00, 283.05it/s]\n",
            "Epoch [23/100] | Train Loss: 0.036812 | Validation Loss: 0.017994\n",
            "Epoch 24/100 [Train]: 100% 9/9 [00:00<00:00, 93.49it/s]\n",
            "Epoch 24/100 [Valid]: 100% 3/3 [00:00<00:00, 312.23it/s]\n",
            "Epoch [24/100] | Train Loss: 0.037639 | Validation Loss: 0.007874\n",
            "Epoch 25/100 [Train]: 100% 9/9 [00:00<00:00, 105.68it/s]\n",
            "Epoch 25/100 [Valid]: 100% 3/3 [00:00<00:00, 301.08it/s]\n",
            "Epoch [25/100] | Train Loss: 0.036353 | Validation Loss: 0.009262\n",
            "Epoch 26/100 [Train]: 100% 9/9 [00:00<00:00, 88.97it/s]\n",
            "Epoch 26/100 [Valid]: 100% 3/3 [00:00<00:00, 311.30it/s]\n",
            "Epoch [26/100] | Train Loss: 0.035704 | Validation Loss: 0.009398\n",
            "Epoch 27/100 [Train]: 100% 9/9 [00:00<00:00, 104.75it/s]\n",
            "Epoch 27/100 [Valid]: 100% 3/3 [00:00<00:00, 287.98it/s]\n",
            "Epoch [27/100] | Train Loss: 0.036997 | Validation Loss: 0.008637\n",
            "Epoch 28/100 [Train]: 100% 9/9 [00:00<00:00, 102.66it/s]\n",
            "Epoch 28/100 [Valid]: 100% 3/3 [00:00<00:00, 309.78it/s]\n",
            "Epoch [28/100] | Train Loss: 0.035923 | Validation Loss: 0.004253\n",
            "Epoch 29/100 [Train]: 100% 9/9 [00:00<00:00, 102.16it/s]\n",
            "Epoch 29/100 [Valid]: 100% 3/3 [00:00<00:00, 302.57it/s]\n",
            "Epoch [29/100] | Train Loss: 0.039266 | Validation Loss: 0.011954\n",
            "Epoch 30/100 [Train]: 100% 9/9 [00:00<00:00, 104.27it/s]\n",
            "Epoch 30/100 [Valid]: 100% 3/3 [00:00<00:00, 299.47it/s]\n",
            "Epoch [30/100] | Train Loss: 0.038249 | Validation Loss: 0.010416\n",
            "Epoch 31/100 [Train]: 100% 9/9 [00:00<00:00, 99.46it/s]\n",
            "Epoch 31/100 [Valid]: 100% 3/3 [00:00<00:00, 308.19it/s]\n",
            "Epoch [31/100] | Train Loss: 0.035714 | Validation Loss: 0.021737\n",
            "Epoch 32/100 [Train]: 100% 9/9 [00:00<00:00, 95.78it/s]\n",
            "Epoch 32/100 [Valid]: 100% 3/3 [00:00<00:00, 315.72it/s]\n",
            "Epoch [32/100] | Train Loss: 0.038077 | Validation Loss: 0.008348\n",
            "Epoch 33/100 [Train]: 100% 9/9 [00:00<00:00, 103.60it/s]\n",
            "Epoch 33/100 [Valid]: 100% 3/3 [00:00<00:00, 301.11it/s]\n",
            "Epoch [33/100] | Train Loss: 0.034611 | Validation Loss: 0.013552\n",
            "Epoch 34/100 [Train]: 100% 9/9 [00:00<00:00, 108.55it/s]\n",
            "Epoch 34/100 [Valid]: 100% 3/3 [00:00<00:00, 299.29it/s]\n",
            "Epoch [34/100] | Train Loss: 0.035903 | Validation Loss: 0.004482\n",
            "Epoch 35/100 [Train]: 100% 9/9 [00:00<00:00, 109.64it/s]\n",
            "Epoch 35/100 [Valid]: 100% 3/3 [00:00<00:00, 304.80it/s]\n",
            "Epoch [35/100] | Train Loss: 0.035523 | Validation Loss: 0.010670\n",
            "Epoch 36/100 [Train]: 100% 9/9 [00:00<00:00, 104.87it/s]\n",
            "Epoch 36/100 [Valid]: 100% 3/3 [00:00<00:00, 224.03it/s]\n",
            "Epoch [36/100] | Train Loss: 0.036699 | Validation Loss: 0.010517\n",
            "Epoch 37/100 [Train]: 100% 9/9 [00:00<00:00, 97.07it/s]\n",
            "Epoch 37/100 [Valid]: 100% 3/3 [00:00<00:00, 310.54it/s]\n",
            "Epoch [37/100] | Train Loss: 0.037097 | Validation Loss: 0.014961\n",
            "Epoch 38/100 [Train]: 100% 9/9 [00:00<00:00, 107.55it/s]\n",
            "Epoch 38/100 [Valid]: 100% 3/3 [00:00<00:00, 302.74it/s]\n",
            "Epoch [38/100] | Train Loss: 0.035496 | Validation Loss: 0.018927\n",
            "Epoch 39/100 [Train]: 100% 9/9 [00:00<00:00, 101.12it/s]\n",
            "Epoch 39/100 [Valid]: 100% 3/3 [00:00<00:00, 292.59it/s]\n",
            "Epoch [39/100] | Train Loss: 0.038181 | Validation Loss: 0.005474\n",
            "Epoch 40/100 [Train]: 100% 9/9 [00:00<00:00, 105.35it/s]\n",
            "Epoch 40/100 [Valid]: 100% 3/3 [00:00<00:00, 293.66it/s]\n",
            "Epoch [40/100] | Train Loss: 0.038578 | Validation Loss: 0.004318\n",
            "Epoch 41/100 [Train]: 100% 9/9 [00:00<00:00, 104.39it/s]\n",
            "Epoch 41/100 [Valid]: 100% 3/3 [00:00<00:00, 306.81it/s]\n",
            "Epoch [41/100] | Train Loss: 0.037841 | Validation Loss: 0.013638\n",
            "Epoch 42/100 [Train]: 100% 9/9 [00:00<00:00, 90.30it/s]\n",
            "Epoch 42/100 [Valid]: 100% 3/3 [00:00<00:00, 314.08it/s]\n",
            "Epoch [42/100] | Train Loss: 0.037802 | Validation Loss: 0.029586\n",
            "Epoch 43/100 [Train]: 100% 9/9 [00:00<00:00, 104.45it/s]\n",
            "Epoch 43/100 [Valid]: 100% 3/3 [00:00<00:00, 292.85it/s]\n",
            "Epoch [43/100] | Train Loss: 0.038591 | Validation Loss: 0.004719\n",
            "Epoch 44/100 [Train]: 100% 9/9 [00:00<00:00, 103.86it/s]\n",
            "Epoch 44/100 [Valid]: 100% 3/3 [00:00<00:00, 303.44it/s]\n",
            "Epoch [44/100] | Train Loss: 0.040626 | Validation Loss: 0.004221\n",
            "Epoch 45/100 [Train]: 100% 9/9 [00:00<00:00, 108.25it/s]\n",
            "Epoch 45/100 [Valid]: 100% 3/3 [00:00<00:00, 298.61it/s]\n",
            "Epoch [45/100] | Train Loss: 0.043420 | Validation Loss: 0.023871\n",
            "Epoch 46/100 [Train]: 100% 9/9 [00:00<00:00, 105.97it/s]\n",
            "Epoch 46/100 [Valid]: 100% 3/3 [00:00<00:00, 293.92it/s]\n",
            "Epoch [46/100] | Train Loss: 0.035905 | Validation Loss: 0.006964\n",
            "Epoch 47/100 [Train]: 100% 9/9 [00:00<00:00, 91.60it/s]\n",
            "Epoch 47/100 [Valid]: 100% 3/3 [00:00<00:00, 314.61it/s]\n",
            "Epoch [47/100] | Train Loss: 0.038527 | Validation Loss: 0.004469\n",
            "Epoch 48/100 [Train]: 100% 9/9 [00:00<00:00, 108.00it/s]\n",
            "Epoch 48/100 [Valid]: 100% 3/3 [00:00<00:00, 304.20it/s]\n",
            "Epoch [48/100] | Train Loss: 0.037774 | Validation Loss: 0.009162\n",
            "Epoch 49/100 [Train]: 100% 9/9 [00:00<00:00, 104.32it/s]\n",
            "Epoch 49/100 [Valid]: 100% 3/3 [00:00<00:00, 303.07it/s]\n",
            "Epoch [49/100] | Train Loss: 0.035174 | Validation Loss: 0.016073\n",
            "Epoch 50/100 [Train]: 100% 9/9 [00:00<00:00, 102.28it/s]\n",
            "Epoch 50/100 [Valid]: 100% 3/3 [00:00<00:00, 298.43it/s]\n",
            "Epoch [50/100] | Train Loss: 0.036575 | Validation Loss: 0.009047\n",
            "Epoch 51/100 [Train]: 100% 9/9 [00:00<00:00, 89.55it/s]\n",
            "Epoch 51/100 [Valid]: 100% 3/3 [00:00<00:00, 300.77it/s]\n",
            "Epoch [51/100] | Train Loss: 0.033764 | Validation Loss: 0.013746\n",
            "Epoch 52/100 [Train]: 100% 9/9 [00:00<00:00, 106.12it/s]\n",
            "Epoch 52/100 [Valid]: 100% 3/3 [00:00<00:00, 295.74it/s]\n",
            "Epoch [52/100] | Train Loss: 0.035865 | Validation Loss: 0.015624\n",
            "Epoch 53/100 [Train]: 100% 9/9 [00:00<00:00, 101.64it/s]\n",
            "Epoch 53/100 [Valid]: 100% 3/3 [00:00<00:00, 309.18it/s]\n",
            "Epoch [53/100] | Train Loss: 0.036312 | Validation Loss: 0.004005\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 54/100 [Train]: 100% 9/9 [00:00<00:00, 104.74it/s]\n",
            "Epoch 54/100 [Valid]: 100% 3/3 [00:00<00:00, 295.68it/s]\n",
            "Epoch [54/100] | Train Loss: 0.036713 | Validation Loss: 0.014421\n",
            "Epoch 55/100 [Train]: 100% 9/9 [00:00<00:00, 103.36it/s]\n",
            "Epoch 55/100 [Valid]: 100% 3/3 [00:00<00:00, 293.77it/s]\n",
            "Epoch [55/100] | Train Loss: 0.035564 | Validation Loss: 0.005621\n",
            "Epoch 56/100 [Train]: 100% 9/9 [00:00<00:00, 105.32it/s]\n",
            "Epoch 56/100 [Valid]: 100% 3/3 [00:00<00:00, 303.61it/s]\n",
            "Epoch [56/100] | Train Loss: 0.035967 | Validation Loss: 0.006278\n",
            "Epoch 57/100 [Train]: 100% 9/9 [00:00<00:00, 91.47it/s]\n",
            "Epoch 57/100 [Valid]: 100% 3/3 [00:00<00:00, 306.40it/s]\n",
            "Epoch [57/100] | Train Loss: 0.036649 | Validation Loss: 0.006559\n",
            "Epoch 58/100 [Train]: 100% 9/9 [00:00<00:00, 98.80it/s]\n",
            "Epoch 58/100 [Valid]: 100% 3/3 [00:00<00:00, 304.17it/s]\n",
            "Epoch [58/100] | Train Loss: 0.033944 | Validation Loss: 0.008897\n",
            "Epoch 59/100 [Train]: 100% 9/9 [00:00<00:00, 101.32it/s]\n",
            "Epoch 59/100 [Valid]: 100% 3/3 [00:00<00:00, 304.47it/s]\n",
            "Epoch [59/100] | Train Loss: 0.035398 | Validation Loss: 0.004466\n",
            "Epoch 60/100 [Train]: 100% 9/9 [00:00<00:00, 92.99it/s]\n",
            "Epoch 60/100 [Valid]: 100% 3/3 [00:00<00:00, 304.60it/s]\n",
            "Epoch [60/100] | Train Loss: 0.035271 | Validation Loss: 0.013877\n",
            "Epoch 61/100 [Train]: 100% 9/9 [00:00<00:00, 102.57it/s]\n",
            "Epoch 61/100 [Valid]: 100% 3/3 [00:00<00:00, 309.92it/s]\n",
            "Epoch [61/100] | Train Loss: 0.034064 | Validation Loss: 0.008279\n",
            "Epoch 62/100 [Train]: 100% 9/9 [00:00<00:00, 106.54it/s]\n",
            "Epoch 62/100 [Valid]: 100% 3/3 [00:00<00:00, 303.08it/s]\n",
            "Epoch [62/100] | Train Loss: 0.035407 | Validation Loss: 0.003982\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 63/100 [Train]: 100% 9/9 [00:00<00:00, 101.47it/s]\n",
            "Epoch 63/100 [Valid]: 100% 3/3 [00:00<00:00, 298.36it/s]\n",
            "Epoch [63/100] | Train Loss: 0.037145 | Validation Loss: 0.004001\n",
            "Epoch 64/100 [Train]: 100% 9/9 [00:00<00:00, 103.51it/s]\n",
            "Epoch 64/100 [Valid]: 100% 3/3 [00:00<00:00, 300.85it/s]\n",
            "Epoch [64/100] | Train Loss: 0.039688 | Validation Loss: 0.004580\n",
            "Epoch 65/100 [Train]: 100% 9/9 [00:00<00:00, 107.37it/s]\n",
            "Epoch 65/100 [Valid]: 100% 3/3 [00:00<00:00, 301.06it/s]\n",
            "Epoch [65/100] | Train Loss: 0.039874 | Validation Loss: 0.030205\n",
            "Epoch 66/100 [Train]: 100% 9/9 [00:00<00:00, 103.74it/s]\n",
            "Epoch 66/100 [Valid]: 100% 3/3 [00:00<00:00, 291.78it/s]\n",
            "Epoch [66/100] | Train Loss: 0.041015 | Validation Loss: 0.009076\n",
            "Epoch 67/100 [Train]: 100% 9/9 [00:00<00:00, 90.91it/s]\n",
            "Epoch 67/100 [Valid]: 100% 3/3 [00:00<00:00, 311.52it/s]\n",
            "Epoch [67/100] | Train Loss: 0.035187 | Validation Loss: 0.005809\n",
            "Epoch 68/100 [Train]: 100% 9/9 [00:00<00:00, 74.01it/s]\n",
            "Epoch 68/100 [Valid]: 100% 3/3 [00:00<00:00, 269.55it/s]\n",
            "Epoch [68/100] | Train Loss: 0.036590 | Validation Loss: 0.006435\n",
            "Epoch 69/100 [Train]: 100% 9/9 [00:00<00:00, 70.77it/s]\n",
            "Epoch 69/100 [Valid]: 100% 3/3 [00:00<00:00, 263.38it/s]\n",
            "Epoch [69/100] | Train Loss: 0.037227 | Validation Loss: 0.007624\n",
            "Epoch 70/100 [Train]: 100% 9/9 [00:00<00:00, 68.28it/s]\n",
            "Epoch 70/100 [Valid]: 100% 3/3 [00:00<00:00, 282.41it/s]\n",
            "Epoch [70/100] | Train Loss: 0.036521 | Validation Loss: 0.019659\n",
            "Epoch 71/100 [Train]: 100% 9/9 [00:00<00:00, 69.28it/s]\n",
            "Epoch 71/100 [Valid]: 100% 3/3 [00:00<00:00, 280.45it/s]\n",
            "Epoch [71/100] | Train Loss: 0.035372 | Validation Loss: 0.008957\n",
            "Epoch 72/100 [Train]: 100% 9/9 [00:00<00:00, 74.68it/s]\n",
            "Epoch 72/100 [Valid]: 100% 3/3 [00:00<00:00, 291.10it/s]\n",
            "Epoch [72/100] | Train Loss: 0.035259 | Validation Loss: 0.006244\n",
            "Epoch 73/100 [Train]: 100% 9/9 [00:00<00:00, 75.93it/s]\n",
            "Epoch 73/100 [Valid]: 100% 3/3 [00:00<00:00, 289.96it/s]\n",
            "Epoch [73/100] | Train Loss: 0.032883 | Validation Loss: 0.013667\n",
            "Epoch 74/100 [Train]: 100% 9/9 [00:00<00:00, 73.54it/s]\n",
            "Epoch 74/100 [Valid]: 100% 3/3 [00:00<00:00, 275.98it/s]\n",
            "Epoch [74/100] | Train Loss: 0.034478 | Validation Loss: 0.004057\n",
            "Epoch 75/100 [Train]: 100% 9/9 [00:00<00:00, 64.37it/s]\n",
            "Epoch 75/100 [Valid]: 100% 3/3 [00:00<00:00, 287.66it/s]\n",
            "Epoch [75/100] | Train Loss: 0.037744 | Validation Loss: 0.003984\n",
            "Epoch 76/100 [Train]: 100% 9/9 [00:00<00:00, 73.29it/s]\n",
            "Epoch 76/100 [Valid]: 100% 3/3 [00:00<00:00, 229.64it/s]\n",
            "Epoch [76/100] | Train Loss: 0.037787 | Validation Loss: 0.013522\n",
            "Epoch 77/100 [Train]: 100% 9/9 [00:00<00:00, 63.60it/s]\n",
            "Epoch 77/100 [Valid]: 100% 3/3 [00:00<00:00, 293.06it/s]\n",
            "Epoch [77/100] | Train Loss: 0.035268 | Validation Loss: 0.010300\n",
            "Epoch 78/100 [Train]: 100% 9/9 [00:00<00:00, 70.79it/s]\n",
            "Epoch 78/100 [Valid]: 100% 3/3 [00:00<00:00, 270.30it/s]\n",
            "Epoch [78/100] | Train Loss: 0.034616 | Validation Loss: 0.015840\n",
            "Epoch 79/100 [Train]: 100% 9/9 [00:00<00:00, 64.98it/s]\n",
            "Epoch 79/100 [Valid]: 100% 3/3 [00:00<00:00, 288.83it/s]\n",
            "Epoch [79/100] | Train Loss: 0.036463 | Validation Loss: 0.005215\n",
            "Epoch 80/100 [Train]: 100% 9/9 [00:00<00:00, 72.64it/s]\n",
            "Epoch 80/100 [Valid]: 100% 3/3 [00:00<00:00, 255.36it/s]\n",
            "Epoch [80/100] | Train Loss: 0.032222 | Validation Loss: 0.005682\n",
            "Epoch 81/100 [Train]: 100% 9/9 [00:00<00:00, 62.55it/s]\n",
            "Epoch 81/100 [Valid]: 100% 3/3 [00:00<00:00, 259.63it/s]\n",
            "Epoch [81/100] | Train Loss: 0.038293 | Validation Loss: 0.004905\n",
            "Epoch 82/100 [Train]: 100% 9/9 [00:00<00:00, 63.83it/s]\n",
            "Epoch 82/100 [Valid]: 100% 3/3 [00:00<00:00, 248.59it/s]\n",
            "Epoch [82/100] | Train Loss: 0.036515 | Validation Loss: 0.026112\n",
            "Epoch 83/100 [Train]: 100% 9/9 [00:00<00:00, 62.27it/s]\n",
            "Epoch 83/100 [Valid]: 100% 3/3 [00:00<00:00, 299.08it/s]\n",
            "Epoch [83/100] | Train Loss: 0.040439 | Validation Loss: 0.014536\n",
            "Epoch 84/100 [Train]: 100% 9/9 [00:00<00:00, 103.45it/s]\n",
            "Epoch 84/100 [Valid]: 100% 3/3 [00:00<00:00, 306.27it/s]\n",
            "Epoch [84/100] | Train Loss: 0.039214 | Validation Loss: 0.004008\n",
            "Epoch 85/100 [Train]: 100% 9/9 [00:00<00:00, 104.71it/s]\n",
            "Epoch 85/100 [Valid]: 100% 3/3 [00:00<00:00, 302.92it/s]\n",
            "Epoch [85/100] | Train Loss: 0.035196 | Validation Loss: 0.013026\n",
            "Epoch 86/100 [Train]: 100% 9/9 [00:00<00:00, 101.62it/s]\n",
            "Epoch 86/100 [Valid]: 100% 3/3 [00:00<00:00, 302.66it/s]\n",
            "Epoch [86/100] | Train Loss: 0.034934 | Validation Loss: 0.008802\n",
            "Epoch 87/100 [Train]: 100% 9/9 [00:00<00:00, 106.59it/s]\n",
            "Epoch 87/100 [Valid]: 100% 3/3 [00:00<00:00, 269.96it/s]\n",
            "Epoch [87/100] | Train Loss: 0.032881 | Validation Loss: 0.019331\n",
            "Epoch 88/100 [Train]: 100% 9/9 [00:00<00:00, 106.63it/s]\n",
            "Epoch 88/100 [Valid]: 100% 3/3 [00:00<00:00, 295.43it/s]\n",
            "Epoch [88/100] | Train Loss: 0.035638 | Validation Loss: 0.004371\n",
            "Epoch 89/100 [Train]: 100% 9/9 [00:00<00:00, 107.30it/s]\n",
            "Epoch 89/100 [Valid]: 100% 3/3 [00:00<00:00, 298.70it/s]\n",
            "Epoch [89/100] | Train Loss: 0.036562 | Validation Loss: 0.004136\n",
            "Epoch 90/100 [Train]: 100% 9/9 [00:00<00:00, 105.70it/s]\n",
            "Epoch 90/100 [Valid]: 100% 3/3 [00:00<00:00, 300.51it/s]\n",
            "Epoch [90/100] | Train Loss: 0.034119 | Validation Loss: 0.017567\n",
            "Epoch 91/100 [Train]: 100% 9/9 [00:00<00:00, 98.96it/s]\n",
            "Epoch 91/100 [Valid]: 100% 3/3 [00:00<00:00, 300.87it/s]\n",
            "Epoch [91/100] | Train Loss: 0.031416 | Validation Loss: 0.022742\n",
            "Epoch 92/100 [Train]: 100% 9/9 [00:00<00:00, 108.22it/s]\n",
            "Epoch 92/100 [Valid]: 100% 3/3 [00:00<00:00, 256.58it/s]\n",
            "Epoch [92/100] | Train Loss: 0.036087 | Validation Loss: 0.021768\n",
            "Epoch 93/100 [Train]: 100% 9/9 [00:00<00:00, 92.11it/s]\n",
            "Epoch 93/100 [Valid]: 100% 3/3 [00:00<00:00, 297.79it/s]\n",
            "Epoch [93/100] | Train Loss: 0.035740 | Validation Loss: 0.004892\n",
            "Epoch 94/100 [Train]: 100% 9/9 [00:00<00:00, 102.11it/s]\n",
            "Epoch 94/100 [Valid]: 100% 3/3 [00:00<00:00, 289.61it/s]\n",
            "Epoch [94/100] | Train Loss: 0.031765 | Validation Loss: 0.003981\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'mslstm_attention_tuned.pth'\n",
            "Epoch 95/100 [Train]: 100% 9/9 [00:00<00:00, 101.99it/s]\n",
            "Epoch 95/100 [Valid]: 100% 3/3 [00:00<00:00, 294.35it/s]\n",
            "Epoch [95/100] | Train Loss: 0.034797 | Validation Loss: 0.004749\n",
            "Epoch 96/100 [Train]: 100% 9/9 [00:00<00:00, 100.21it/s]\n",
            "Epoch 96/100 [Valid]: 100% 3/3 [00:00<00:00, 300.32it/s]\n",
            "Epoch [96/100] | Train Loss: 0.033439 | Validation Loss: 0.005520\n",
            "Epoch 97/100 [Train]: 100% 9/9 [00:00<00:00, 107.36it/s]\n",
            "Epoch 97/100 [Valid]: 100% 3/3 [00:00<00:00, 296.09it/s]\n",
            "Epoch [97/100] | Train Loss: 0.032726 | Validation Loss: 0.008154\n",
            "Epoch 98/100 [Train]: 100% 9/9 [00:00<00:00, 100.10it/s]\n",
            "Epoch 98/100 [Valid]: 100% 3/3 [00:00<00:00, 303.54it/s]\n",
            "Epoch [98/100] | Train Loss: 0.032995 | Validation Loss: 0.013737\n",
            "Epoch 99/100 [Train]: 100% 9/9 [00:00<00:00, 106.65it/s]\n",
            "Epoch 99/100 [Valid]: 100% 3/3 [00:00<00:00, 295.07it/s]\n",
            "Epoch [99/100] | Train Loss: 0.031896 | Validation Loss: 0.005957\n",
            "Epoch 100/100 [Train]: 100% 9/9 [00:00<00:00, 106.25it/s]\n",
            "Epoch 100/100 [Valid]: 100% 3/3 [00:00<00:00, 293.52it/s]\n",
            "Epoch [100/100] | Train Loss: 0.030009 | Validation Loss: 0.003997\n",
            "\n",
            "--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\n",
            "‚úÖ Model (Attention) t·ªët nh·∫•t ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = 0.003981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evalute l·∫°i***"
      ],
      "metadata": {
        "id": "GWVZ55rOXT42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "# evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS,\n",
        "    TRAINED_MODEL_PATH\n",
        ")\n",
        "#import model m·ªõi\n",
        "from model import MSLSTMAttention\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu (ch·ªâ c·∫ßn test_loader v√† scaler)\n",
        "    _, test_loader, _, target_scaler = get_data_loaders()\n",
        "    if not test_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. T·∫£i l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "    print(f\"ƒêang t·∫£i m√¥ h√¨nh t·ª´: {TRAINED_MODEL_PATH}\")\n",
        "    # S·ª≠a ƒë·ªïi: Kh·ªüi t·∫°o ƒë√∫ng class model m·ªõi\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS,\n",
        "        lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n",
        "        model.to(device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i '{TRAINED_MODEL_PATH}'.\")\n",
        "        print(\"Vui l√≤ng ch·∫°y train.py tr∆∞·ªõc.\")\n",
        "        return\n",
        "\n",
        "    # 3. ƒê√°nh gi√° tr√™n t·∫≠p Test\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            outputs = model(features)\n",
        "\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 4. Gi·∫£i chu·∫©n h√≥a ƒë·ªÉ so s√°nh\n",
        "    predictions = np.array(predictions).reshape(-1, 1)\n",
        "    actuals = np.array(actuals).reshape(-1, 1)\n",
        "\n",
        "    original_predictions = target_scaler.inverse_transform(predictions)\n",
        "    original_actuals = target_scaler.inverse_transform(actuals)\n",
        "\n",
        "    # 5. T√≠nh to√°n sai s·ªë v√† in k·∫øt qu·∫£\n",
        "    mae = np.mean(np.abs(original_predictions - original_actuals))\n",
        "    print(f\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model + Attention) ---\")\n",
        "    print(f\"Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # 6. V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_actuals, label='Gi√° tr·ªã Th·ª±c t·∫ø (Actuals)', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_predictions, label='Gi√° tr·ªã D·ª± ƒëo√°n (Predictions)', color='red', linestyle='--')\n",
        "    plt.title('So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n (Model + Attention)')\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    output_image_path = 'prediction_vs_actual_attention.png'\n",
        "    plt.savefig(output_image_path)\n",
        "    print(f\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file '{output_image_path}'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j580yRKFXW2s",
        "outputId": "96a6aedc-99ed-4c39-817e-6dc224acb180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rlk983tdXb7w",
        "outputId": "305dc1d1-73e2-4fbe-afe3-6c202fff094e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o DataLoader...\n",
            "-> ƒêang t·∫°o ƒë·∫∑c tr∆∞ng ƒëa quy m√¥ v·ªõi Wavelet (family: db4, level: 4)...\n",
            "-> ƒêang chu·∫©n h√≥a d·ªØ li·ªáu (scaling)...\n",
            "-> ƒêang t·∫°o c√°c chu·ªói tu·∫ßn t·ª± v·ªõi lookback window = 30...\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Train: 270 m·∫´u\n",
            "-> K√≠ch th∆∞·ªõc t·∫≠p Test: 68 m·∫´u\n",
            "\n",
            "‚úÖ Ho√†n t·∫•t vi·ªác t·∫°o DataLoader.\n",
            "ƒêang t·∫£i m√¥ h√¨nh t·ª´: mslstm_attention_tuned.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model + Attention) ---\n",
            "Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 20.2117 (ƒëi·ªÉm VN-Index)\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh v√†o file 'prediction_vs_actual_attention.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***K·∫øt qu·∫£ ra kh√¥ng ·ªïn l·∫Øm, th·ª≠ qua Hierachical ƒë·ªÉ xem ph√¢n c·∫•p ph·ª• thu·ªôc c√≥ ·ªïn kh√¥ng?***"
      ],
      "metadata": {
        "id": "r3aD-qOaXuYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dataset.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pywt\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from config import (\n",
        "    PROCESSED_DATA_PATH, LOOKBACK_WINDOW, TARGET_COLUMN,\n",
        "    WAVELET_FAMILY, WAVELET_LEVEL, TEST_SET_SIZE, BATCH_SIZE\n",
        ")\n",
        "\n",
        "def create_multiscale_features(data, wavelet_family, level):\n",
        "    \"\"\"Ph√¢n r√£ m·ªói c·ªôt th√†nh c√°c th√†nh ph·∫ßn ƒëa quy m√¥ b·∫±ng Wavelet.\"\"\"\n",
        "    coeffs_df_list = []\n",
        "    # D·ªØ li·ªáu ƒë·∫ßu v√†o `data` b√¢y gi·ªù s·∫Ω l√† 30 c·ªôt g·ªëc\n",
        "    for column in data.columns:\n",
        "        series = data[column].values\n",
        "        coeffs = pywt.wavedec(series, wavelet_family, level=level)\n",
        "        for i, c in enumerate(coeffs):\n",
        "            c_padded = np.pad(c, (0, len(data) - len(c)), 'constant')\n",
        "            coeffs_df_list.append(pd.DataFrame({f\"{column}_wavelet_L{i}\": c_padded}, index=data.index))\n",
        "    return pd.concat(coeffs_df_list, axis=1)\n",
        "\n",
        "def create_multitask_labels(df, target_col, lookback_window):\n",
        "    \"\"\"T·∫°o ra 3 lo·∫°i nh√£n cho b√†i to√°n ƒëa nhi·ªám.\"\"\"\n",
        "    price_labels = df[target_col].values\n",
        "    trend_labels = (df[target_col].diff() > 0).astype(int).values\n",
        "    daily_returns = df[target_col].pct_change()\n",
        "    volatility_labels = daily_returns.rolling(window=lookback_window).std().values\n",
        "    return price_labels, trend_labels, volatility_labels\n",
        "\n",
        "def create_sequences(features, price_labels, trend_labels, vol_labels, lookback):\n",
        "    \"\"\"T·∫°o c√°c chu·ªói tu·∫ßn t·ª± cho c·∫£ features v√† 3 lo·∫°i labels.\"\"\"\n",
        "    X, y_price, y_trend, y_vol = [], [], [], []\n",
        "    for i in range(len(features) - lookback):\n",
        "        X.append(features[i:(i + lookback)])\n",
        "        y_price.append(price_labels[i + lookback])\n",
        "        y_trend.append(trend_labels[i + lookback])\n",
        "        y_vol.append(vol_labels[i + lookback])\n",
        "    return np.array(X), np.array(y_price), np.array(y_trend), np.array(y_vol)\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    \"\"\"Dataset t√πy ch·ªânh cho b√†i to√°n ƒëa nhi·ªám.\"\"\"\n",
        "    def __init__(self, features, price_lbl, trend_lbl, vol_lbl):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.price_lbl = torch.tensor(price_lbl, dtype=torch.float32)\n",
        "        self.trend_lbl = torch.tensor(trend_lbl, dtype=torch.float32)\n",
        "        self.vol_lbl = torch.tensor(vol_lbl, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.features[idx],\n",
        "                (self.price_lbl[idx], self.trend_lbl[idx], self.vol_lbl[idx]))\n",
        "\n",
        "def get_data_loaders():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ t·∫£i, x·ª≠ l√Ω v√† t·∫°o ra c√°c DataLoader ƒëa nhi·ªám.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(PROCESSED_DATA_PATH, index_col='Date', parse_dates=True)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file {PROCESSED_DATA_PATH}. Vui l√≤ng ch·∫°y c√°c b∆∞·ªõc tr∆∞·ªõc.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # === LOGIC ƒê√É ƒê∆Ø·ª¢C S·ª¨A L·∫†I ===\n",
        "    # 1. T·∫°o ƒë·∫∑c tr∆∞ng Wavelet t·ª´ TO√ÄN B·ªò 30 c·ªôt g·ªëc\n",
        "    # df ·ªü ƒë√¢y c√≥ 30 c·ªôt ƒë·∫∑c tr∆∞ng ƒë√£ t√≠nh to√°n (OHLCV + indicators cho 3 symbols)\n",
        "    multiscale_df = create_multiscale_features(df, WAVELET_FAMILY, WAVELET_LEVEL)\n",
        "    # => multiscale_df b√¢y gi·ªù s·∫Ω c√≥ ƒë√∫ng 30 * 5 = 150 c·ªôt\n",
        "\n",
        "    # 2. T·∫°o c√°c nh√£n m·ªôt c√°ch ri√™ng bi·ªát\n",
        "    price_lbl, trend_lbl, vol_lbl = create_multitask_labels(df, TARGET_COLUMN, LOOKBACK_WINDOW)\n",
        "\n",
        "    # 3. K·∫øt h·ª£p features v√† labels v√†o m·ªôt DataFrame ƒë·ªÉ dropna ƒë·ªìng b·ªô\n",
        "    labels_df = pd.DataFrame({\n",
        "        'price_label': price_lbl,\n",
        "        'trend_label': trend_lbl,\n",
        "        'volatility_label': vol_lbl\n",
        "    }, index=df.index)\n",
        "\n",
        "    full_df = pd.concat([multiscale_df, labels_df], axis=1)\n",
        "    full_df.dropna(inplace=True)\n",
        "\n",
        "    # 4. T√°ch l·∫°i features v√† labels sau khi ƒë√£ l√†m s·∫°ch\n",
        "    final_features = full_df.drop(columns=['price_label', 'trend_label', 'volatility_label']).values\n",
        "    final_price_lbl = full_df['price_label'].values\n",
        "    final_trend_lbl = full_df['trend_label'].values\n",
        "    final_vol_lbl = full_df['volatility_label'].values\n",
        "\n",
        "    # 5. Chu·∫©n h√≥a d·ªØ li·ªáu\n",
        "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_features = feature_scaler.fit_transform(final_features)\n",
        "\n",
        "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_price_lbl = target_scaler.fit_transform(final_price_lbl.reshape(-1, 1)).flatten()\n",
        "\n",
        "    volatility_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_vol_lbl = volatility_scaler.fit_transform(final_vol_lbl.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 6. T·∫°o chu·ªói v√† DataLoader nh∆∞ c≈©\n",
        "    X, y_p, y_t, y_v = create_sequences(scaled_features, scaled_price_lbl, final_trend_lbl, scaled_vol_lbl, LOOKBACK_WINDOW)\n",
        "    split = int(len(X) * (1 - TEST_SET_SIZE))\n",
        "    train_dataset = StockDataset(X[:split], y_p[:split], y_t[:split], y_v[:split])\n",
        "    test_dataset = StockDataset(X[split:], y_p[split:], y_t[split:], y_v[split:])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    print(\"‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\")\n",
        "    return train_loader, test_loader, target_scaler, volatility_scaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnRt_KHjYMxa",
        "outputId": "f62c8b5d-6490-49d3-b620-a61ee088b589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dataset.py"
      ],
      "metadata": {
        "id": "mF5M4Q1tYfk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "# ƒê√¢y l√† file model cu·ªëi c√πng\n",
        "TRAINED_MODEL_PATH = 'final_hierarchical_model.pth'\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "# C·ªôt m·ª•c ti√™u ch√≠nh v·∫´n l√† VNINDEX Close\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- Tham s·ªë Ki·∫øn tr√∫c Model ---\n",
        "NUM_BASE_FEATURES = 30\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "LSTM_HIDDEN_UNITS = 128\n",
        "LSTM_NUM_LAYERS = 2\n",
        "ATTENTION_NUM_HEADS = 4\n",
        "\n",
        "# --- THAM S·ªê M·ªöI CHO MULTI-TASK LEARNING ---\n",
        "LOSS_WEIGHTS = {\n",
        "    'price': 0.6,\n",
        "    'trend': 0.3,\n",
        "    'volatility': 0.1\n",
        "}\n",
        "\n",
        "# --- Tham s·ªë Hu·∫•n luy·ªán ---\n",
        "LEARNING_RATE = 0.0005\n",
        "NUM_EPOCHS = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCcg8fOPX5k-",
        "outputId": "d2147536-932f-4f82-a992-47e81059d3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import BATCH_SIZE, LOOKBACK_WINDOW # Ch·ªâ import nh·ªØng g√¨ c·∫ßn cho vi·ªác test\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(FinalModel, self).__init__()\n",
        "\n",
        "        # S·ª¨A L·ªñI: L∆∞u l·∫°i c√°c tham s·ªë ki·∫øn tr√∫c l√†m thu·ªôc t√≠nh c·ªßa class\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.num_scales = num_scales\n",
        "\n",
        "        # --- Ph·∫ßn th√¢n chung (Shared Body) ---\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(input_size=self.input_feature_size, hidden_size=lstm_hidden_units,\n",
        "                    num_layers=lstm_num_layers, batch_first=True, dropout=0.2 if lstm_num_layers > 1 else 0)\n",
        "            for _ in range(self.num_scales)\n",
        "        ])\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_units * self.num_scales,\n",
        "                                             num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        shared_feature_dim = lstm_hidden_units * self.num_scales\n",
        "        self.intermediate_layer = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim, shared_feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # --- C√°c ƒê·∫ßu ra ƒêa nhi·ªám (Multi-Task Heads) ---\n",
        "        self.price_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "        self.trend_head = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim // 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        self.volatility_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_scales):\n",
        "            # S·ª¨A L·ªñI: D√πng thu·ªôc t√≠nh c·ªßa class (self.input_feature_size) thay v√¨ bi·∫øn to√†n c·ª•c\n",
        "            start_idx = i * self.input_feature_size\n",
        "            end_idx = (i + 1) * self.input_feature_size\n",
        "\n",
        "            branch_input = x[:, :, start_idx:end_idx]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "        shared_features = self.intermediate_layer(attention_output[:, -1, :])\n",
        "\n",
        "        price_prediction = self.price_head(shared_features)\n",
        "        trend_prediction = self.trend_head(shared_features)\n",
        "        volatility_prediction = self.volatility_head(shared_features)\n",
        "\n",
        "        return price_prediction.squeeze(), trend_prediction.squeeze(), volatility_prediction.squeeze()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ƒêo·∫°n test n√†y c·∫ßn import t·ª´ config\n",
        "    from config import NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS, ATTENTION_NUM_HEADS\n",
        "\n",
        "    print(\"--- Ki·ªÉm tra ki·∫øn tr√∫c Final Model (ƒë√£ s·ª≠a l·ªói) ---\")\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    print(model)\n",
        "    dummy_input = torch.randn(BATCH_SIZE, LOOKBACK_WINDOW, NUM_BASE_FEATURES * NUM_SCALES)\n",
        "    price, trend, volatility = model(dummy_input)\n",
        "\n",
        "    print(f\"\\nShape c·ªßa input: {dummy_input.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Price: {price.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Trend: {trend.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Volatility: {volatility.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONA_-rfVYEEk",
        "outputId": "e7ff095c-67ee-479d-c638-d4084f3e6a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQrbJw3DYG_a",
        "outputId": "8f9209f0-e829-4aa6-8eec-fd4077be2567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ki·ªÉm tra ki·∫øn tr√∫c Final Model (ƒë√£ s·ª≠a l·ªói) ---\n",
            "FinalModel(\n",
            "  (lstm_branches): ModuleList(\n",
            "    (0-4): 5 x LSTM(30, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=640, out_features=640, bias=True)\n",
            "  )\n",
            "  (intermediate_layer): Sequential(\n",
            "    (0): Linear(in_features=640, out_features=320, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (price_head): Linear(in_features=320, out_features=1, bias=True)\n",
            "  (trend_head): Sequential(\n",
            "    (0): Linear(in_features=320, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (volatility_head): Linear(in_features=320, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Shape c·ªßa input: torch.Size([32, 30, 150])\n",
            "Shape ƒë·∫ßu ra Price: torch.Size([32])\n",
            "Shape ƒë·∫ßu ra Trend: torch.Size([32])\n",
            "Shape ƒë·∫ßu ra Volatility: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, LOSS_WEIGHTS,\n",
        "    LEARNING_RATE, NUM_EPOCHS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from model import FinalModel\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "def run_training():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader: return\n",
        "\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    ).to(device)\n",
        "\n",
        "    # ƒê·ªãnh nghƒ©a c√°c h√†m loss cho t·ª´ng nhi·ªám v·ª•\n",
        "    price_loss_fn = nn.MSELoss() # Cho gi√° (h·ªìi quy)\n",
        "    # D√πng BCEWithLogitsLoss cho ƒë·ªô ·ªïn ƒë·ªãnh s·ªë h·ªçc, n√≥ ƒë√£ t√≠ch h·ª£p s·∫µn Sigmoid\n",
        "    trend_loss_fn = nn.BCEWithLogitsLoss() # Cho xu h∆∞·ªõng (ph√¢n lo·∫°i)\n",
        "    vol_loss_fn = nn.MSELoss() # Cho ƒë·ªô bi·∫øn ƒë·ªông (h·ªìi quy)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model Cu·ªëi c√πng (Hierarchical + Multi-Task) ---\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # --- PHA HU·∫§N LUY·ªÜN ---\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for features, (price_lbl, trend_lbl, vol_lbl) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features = features.to(device)\n",
        "            price_lbl, trend_lbl, vol_lbl = price_lbl.to(device), trend_lbl.to(device), vol_lbl.to(device)\n",
        "\n",
        "            # L·∫•y 3 ƒë·∫ßu ra t·ª´ model\n",
        "            price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "            # T√≠nh to√°n loss cho t·ª´ng nhi·ªám v·ª•\n",
        "            loss_p = price_loss_fn(price_pred, price_lbl)\n",
        "            loss_t = trend_loss_fn(trend_pred, trend_lbl)\n",
        "            loss_v = vol_loss_fn(vol_pred, vol_lbl)\n",
        "\n",
        "            # T√≠nh loss t·ªïng h·ª£p d·ª±a tr√™n tr·ªçng s·ªë\n",
        "            total_loss = (LOSS_WEIGHTS['price'] * loss_p +\n",
        "                          LOSS_WEIGHTS['trend'] * loss_t +\n",
        "                          LOSS_WEIGHTS['volatility'] * loss_v)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += total_loss.item()\n",
        "\n",
        "        # --- PHA KI·ªÇM ƒê·ªäNH ---\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, (price_lbl, trend_lbl, vol_lbl) in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Valid]\"):\n",
        "                features = features.to(device)\n",
        "                price_lbl, trend_lbl, vol_lbl = price_lbl.to(device), trend_lbl.to(device), vol_lbl.to(device)\n",
        "\n",
        "                price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "                loss_p = price_loss_fn(price_pred, price_lbl)\n",
        "                loss_t = trend_loss_fn(trend_pred, trend_lbl)\n",
        "                loss_v = vol_loss_fn(vol_pred, vol_lbl)\n",
        "\n",
        "                total_loss = (LOSS_WEIGHTS['price'] * loss_p +\n",
        "                              LOSS_WEIGHTS['trend'] * loss_t +\n",
        "                              LOSS_WEIGHTS['volatility'] * loss_v)\n",
        "                total_val_loss += total_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), TRAINED_MODEL_PATH)\n",
        "            print(f\"   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o '{TRAINED_MODEL_PATH}'\")\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "    print(f\"‚úÖ Model cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = {best_val_loss:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4uQY0WgYmQP",
        "outputId": "0a5c458d-0efe-4b6d-c412-da6b08bbaa72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ibCuSWYqOu",
        "outputId": "ce8d0447-1cc6-4013-ea15-76bc37626fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\n",
            "--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model Cu·ªëi c√πng (Hierarchical + Multi-Task) ---\n",
            "Epoch 1/100 [Train]: 100% 8/8 [00:00<00:00, 13.22it/s]\n",
            "Epoch 1/100 [Valid]: 100% 2/2 [00:00<00:00, 142.25it/s]\n",
            "Epoch [01/100] | Train Loss: 0.287770 | Validation Loss: 0.220904\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_hierarchical_model.pth'\n",
            "Epoch 2/100 [Train]: 100% 8/8 [00:00<00:00, 66.65it/s]\n",
            "Epoch 2/100 [Valid]: 100% 2/2 [00:00<00:00, 148.36it/s]\n",
            "Epoch [02/100] | Train Loss: 0.250243 | Validation Loss: 0.207669\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_hierarchical_model.pth'\n",
            "Epoch 3/100 [Train]: 100% 8/8 [00:00<00:00, 86.52it/s]\n",
            "Epoch 3/100 [Valid]: 100% 2/2 [00:00<00:00, 240.69it/s]\n",
            "Epoch [03/100] | Train Loss: 0.243089 | Validation Loss: 0.214556\n",
            "Epoch 4/100 [Train]: 100% 8/8 [00:00<00:00, 87.85it/s]\n",
            "Epoch 4/100 [Valid]: 100% 2/2 [00:00<00:00, 246.72it/s]\n",
            "Epoch [04/100] | Train Loss: 0.244731 | Validation Loss: 0.208783\n",
            "Epoch 5/100 [Train]: 100% 8/8 [00:00<00:00, 88.75it/s]\n",
            "Epoch 5/100 [Valid]: 100% 2/2 [00:00<00:00, 243.62it/s]\n",
            "Epoch [05/100] | Train Loss: 0.246188 | Validation Loss: 0.207600\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_hierarchical_model.pth'\n",
            "Epoch 6/100 [Train]: 100% 8/8 [00:00<00:00, 87.90it/s]\n",
            "Epoch 6/100 [Valid]: 100% 2/2 [00:00<00:00, 238.46it/s]\n",
            "Epoch [06/100] | Train Loss: 0.244371 | Validation Loss: 0.214120\n",
            "Epoch 7/100 [Train]: 100% 8/8 [00:00<00:00, 92.30it/s]\n",
            "Epoch 7/100 [Valid]: 100% 2/2 [00:00<00:00, 251.56it/s]\n",
            "Epoch [07/100] | Train Loss: 0.241459 | Validation Loss: 0.207056\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_hierarchical_model.pth'\n",
            "Epoch 8/100 [Train]: 100% 8/8 [00:00<00:00, 74.22it/s]\n",
            "Epoch 8/100 [Valid]: 100% 2/2 [00:00<00:00, 249.97it/s]\n",
            "Epoch [08/100] | Train Loss: 0.240903 | Validation Loss: 0.212709\n",
            "Epoch 9/100 [Train]: 100% 8/8 [00:00<00:00, 81.76it/s]\n",
            "Epoch 9/100 [Valid]: 100% 2/2 [00:00<00:00, 252.06it/s]\n",
            "Epoch [09/100] | Train Loss: 0.243484 | Validation Loss: 0.208768\n",
            "Epoch 10/100 [Train]: 100% 8/8 [00:00<00:00, 92.04it/s]\n",
            "Epoch 10/100 [Valid]: 100% 2/2 [00:00<00:00, 252.05it/s]\n",
            "Epoch [10/100] | Train Loss: 0.243340 | Validation Loss: 0.213215\n",
            "Epoch 11/100 [Train]: 100% 8/8 [00:00<00:00, 92.45it/s]\n",
            "Epoch 11/100 [Valid]: 100% 2/2 [00:00<00:00, 247.04it/s]\n",
            "Epoch [11/100] | Train Loss: 0.243338 | Validation Loss: 0.206705\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_hierarchical_model.pth'\n",
            "Epoch 12/100 [Train]: 100% 8/8 [00:00<00:00, 90.44it/s]\n",
            "Epoch 12/100 [Valid]: 100% 2/2 [00:00<00:00, 256.04it/s]\n",
            "Epoch [12/100] | Train Loss: 0.244715 | Validation Loss: 0.210774\n",
            "Epoch 13/100 [Train]: 100% 8/8 [00:00<00:00, 90.59it/s]\n",
            "Epoch 13/100 [Valid]: 100% 2/2 [00:00<00:00, 241.91it/s]\n",
            "Epoch [13/100] | Train Loss: 0.241956 | Validation Loss: 0.210429\n",
            "Epoch 14/100 [Train]: 100% 8/8 [00:00<00:00, 89.11it/s]\n",
            "Epoch 14/100 [Valid]: 100% 2/2 [00:00<00:00, 248.53it/s]\n",
            "Epoch [14/100] | Train Loss: 0.242866 | Validation Loss: 0.207854\n",
            "Epoch 15/100 [Train]: 100% 8/8 [00:00<00:00, 90.86it/s]\n",
            "Epoch 15/100 [Valid]: 100% 2/2 [00:00<00:00, 251.93it/s]\n",
            "Epoch [15/100] | Train Loss: 0.242383 | Validation Loss: 0.207116\n",
            "Epoch 16/100 [Train]: 100% 8/8 [00:00<00:00, 91.27it/s]\n",
            "Epoch 16/100 [Valid]: 100% 2/2 [00:00<00:00, 243.59it/s]\n",
            "Epoch [16/100] | Train Loss: 0.243426 | Validation Loss: 0.209690\n",
            "Epoch 17/100 [Train]: 100% 8/8 [00:00<00:00, 88.55it/s]\n",
            "Epoch 17/100 [Valid]: 100% 2/2 [00:00<00:00, 249.53it/s]\n",
            "Epoch [17/100] | Train Loss: 0.240933 | Validation Loss: 0.208431\n",
            "Epoch 18/100 [Train]: 100% 8/8 [00:00<00:00, 72.29it/s]\n",
            "Epoch 18/100 [Valid]: 100% 2/2 [00:00<00:00, 251.49it/s]\n",
            "Epoch [18/100] | Train Loss: 0.242966 | Validation Loss: 0.212962\n",
            "Epoch 19/100 [Train]: 100% 8/8 [00:00<00:00, 90.08it/s]\n",
            "Epoch 19/100 [Valid]: 100% 2/2 [00:00<00:00, 253.21it/s]\n",
            "Epoch [19/100] | Train Loss: 0.242629 | Validation Loss: 0.210273\n",
            "Epoch 20/100 [Train]: 100% 8/8 [00:00<00:00, 85.27it/s]\n",
            "Epoch 20/100 [Valid]: 100% 2/2 [00:00<00:00, 254.90it/s]\n",
            "Epoch [20/100] | Train Loss: 0.244659 | Validation Loss: 0.207468\n",
            "Epoch 21/100 [Train]: 100% 8/8 [00:00<00:00, 94.75it/s]\n",
            "Epoch 21/100 [Valid]: 100% 2/2 [00:00<00:00, 253.23it/s]\n",
            "Epoch [21/100] | Train Loss: 0.242879 | Validation Loss: 0.209203\n",
            "Epoch 22/100 [Train]: 100% 8/8 [00:00<00:00, 90.34it/s]\n",
            "Epoch 22/100 [Valid]: 100% 2/2 [00:00<00:00, 245.95it/s]\n",
            "Epoch [22/100] | Train Loss: 0.242157 | Validation Loss: 0.213583\n",
            "Epoch 23/100 [Train]: 100% 8/8 [00:00<00:00, 84.99it/s]\n",
            "Epoch 23/100 [Valid]: 100% 2/2 [00:00<00:00, 244.79it/s]\n",
            "Epoch [23/100] | Train Loss: 0.242506 | Validation Loss: 0.209736\n",
            "Epoch 24/100 [Train]: 100% 8/8 [00:00<00:00, 92.43it/s]\n",
            "Epoch 24/100 [Valid]: 100% 2/2 [00:00<00:00, 248.37it/s]\n",
            "Epoch [24/100] | Train Loss: 0.243234 | Validation Loss: 0.207213\n",
            "Epoch 25/100 [Train]: 100% 8/8 [00:00<00:00, 89.39it/s]\n",
            "Epoch 25/100 [Valid]: 100% 2/2 [00:00<00:00, 236.35it/s]\n",
            "Epoch [25/100] | Train Loss: 0.242065 | Validation Loss: 0.212099\n",
            "Epoch 26/100 [Train]: 100% 8/8 [00:00<00:00, 90.38it/s]\n",
            "Epoch 26/100 [Valid]: 100% 2/2 [00:00<00:00, 246.30it/s]\n",
            "Epoch [26/100] | Train Loss: 0.242432 | Validation Loss: 0.209915\n",
            "Epoch 27/100 [Train]: 100% 8/8 [00:00<00:00, 91.78it/s]\n",
            "Epoch 27/100 [Valid]: 100% 2/2 [00:00<00:00, 235.32it/s]\n",
            "Epoch [27/100] | Train Loss: 0.242737 | Validation Loss: 0.206815\n",
            "Epoch 28/100 [Train]: 100% 8/8 [00:00<00:00, 87.96it/s]\n",
            "Epoch 28/100 [Valid]: 100% 2/2 [00:00<00:00, 250.77it/s]\n",
            "Epoch [28/100] | Train Loss: 0.243840 | Validation Loss: 0.212737\n",
            "Epoch 29/100 [Train]: 100% 8/8 [00:00<00:00, 71.16it/s]\n",
            "Epoch 29/100 [Valid]: 100% 2/2 [00:00<00:00, 236.20it/s]\n",
            "Epoch [29/100] | Train Loss: 0.242839 | Validation Loss: 0.211004\n",
            "Epoch 30/100 [Train]: 100% 8/8 [00:00<00:00, 93.06it/s]\n",
            "Epoch 30/100 [Valid]: 100% 2/2 [00:00<00:00, 246.89it/s]\n",
            "Epoch [30/100] | Train Loss: 0.241963 | Validation Loss: 0.209935\n",
            "Epoch 31/100 [Train]: 100% 8/8 [00:00<00:00, 91.61it/s]\n",
            "Epoch 31/100 [Valid]: 100% 2/2 [00:00<00:00, 251.00it/s]\n",
            "Epoch [31/100] | Train Loss: 0.242057 | Validation Loss: 0.207840\n",
            "Epoch 32/100 [Train]: 100% 8/8 [00:00<00:00, 92.96it/s]\n",
            "Epoch 32/100 [Valid]: 100% 2/2 [00:00<00:00, 250.71it/s]\n",
            "Epoch [32/100] | Train Loss: 0.242504 | Validation Loss: 0.212019\n",
            "Epoch 33/100 [Train]: 100% 8/8 [00:00<00:00, 90.52it/s]\n",
            "Epoch 33/100 [Valid]: 100% 2/2 [00:00<00:00, 249.88it/s]\n",
            "Epoch [33/100] | Train Loss: 0.241690 | Validation Loss: 0.210865\n",
            "Epoch 34/100 [Train]: 100% 8/8 [00:00<00:00, 88.70it/s]\n",
            "Epoch 34/100 [Valid]: 100% 2/2 [00:00<00:00, 249.29it/s]\n",
            "Epoch [34/100] | Train Loss: 0.242144 | Validation Loss: 0.207158\n",
            "Epoch 35/100 [Train]: 100% 8/8 [00:00<00:00, 91.65it/s]\n",
            "Epoch 35/100 [Valid]: 100% 2/2 [00:00<00:00, 247.95it/s]\n",
            "Epoch [35/100] | Train Loss: 0.244936 | Validation Loss: 0.207796\n",
            "Epoch 36/100 [Train]: 100% 8/8 [00:00<00:00, 89.58it/s]\n",
            "Epoch 36/100 [Valid]: 100% 2/2 [00:00<00:00, 245.60it/s]\n",
            "Epoch [36/100] | Train Loss: 0.240874 | Validation Loss: 0.211869\n",
            "Epoch 37/100 [Train]: 100% 8/8 [00:00<00:00, 88.28it/s]\n",
            "Epoch 37/100 [Valid]: 100% 2/2 [00:00<00:00, 228.48it/s]\n",
            "Epoch [37/100] | Train Loss: 0.241931 | Validation Loss: 0.207195\n",
            "Epoch 38/100 [Train]: 100% 8/8 [00:00<00:00, 89.83it/s]\n",
            "Epoch 38/100 [Valid]: 100% 2/2 [00:00<00:00, 241.70it/s]\n",
            "Epoch [38/100] | Train Loss: 0.242514 | Validation Loss: 0.210089\n",
            "Epoch 39/100 [Train]: 100% 8/8 [00:00<00:00, 69.52it/s]\n",
            "Epoch 39/100 [Valid]: 100% 2/2 [00:00<00:00, 251.85it/s]\n",
            "Epoch [39/100] | Train Loss: 0.241992 | Validation Loss: 0.216148\n",
            "Epoch 40/100 [Train]: 100% 8/8 [00:00<00:00, 91.08it/s]\n",
            "Epoch 40/100 [Valid]: 100% 2/2 [00:00<00:00, 248.68it/s]\n",
            "Epoch [40/100] | Train Loss: 0.243269 | Validation Loss: 0.206813\n",
            "Epoch 41/100 [Train]: 100% 8/8 [00:00<00:00, 94.79it/s]\n",
            "Epoch 41/100 [Valid]: 100% 2/2 [00:00<00:00, 243.30it/s]\n",
            "Epoch [41/100] | Train Loss: 0.244851 | Validation Loss: 0.209013\n",
            "Epoch 42/100 [Train]: 100% 8/8 [00:00<00:00, 88.67it/s]\n",
            "Epoch 42/100 [Valid]: 100% 2/2 [00:00<00:00, 244.97it/s]\n",
            "Epoch [42/100] | Train Loss: 0.242888 | Validation Loss: 0.212184\n",
            "Epoch 43/100 [Train]: 100% 8/8 [00:00<00:00, 82.87it/s]\n",
            "Epoch 43/100 [Valid]: 100% 2/2 [00:00<00:00, 247.88it/s]\n",
            "Epoch [43/100] | Train Loss: 0.241690 | Validation Loss: 0.207360\n",
            "Epoch 44/100 [Train]: 100% 8/8 [00:00<00:00, 92.97it/s]\n",
            "Epoch 44/100 [Valid]: 100% 2/2 [00:00<00:00, 252.82it/s]\n",
            "Epoch [44/100] | Train Loss: 0.242172 | Validation Loss: 0.212587\n",
            "Epoch 45/100 [Train]: 100% 8/8 [00:00<00:00, 93.35it/s]\n",
            "Epoch 45/100 [Valid]: 100% 2/2 [00:00<00:00, 241.40it/s]\n",
            "Epoch [45/100] | Train Loss: 0.242665 | Validation Loss: 0.208254\n",
            "Epoch 46/100 [Train]: 100% 8/8 [00:00<00:00, 87.56it/s]\n",
            "Epoch 46/100 [Valid]: 100% 2/2 [00:00<00:00, 224.88it/s]\n",
            "Epoch [46/100] | Train Loss: 0.242106 | Validation Loss: 0.207316\n",
            "Epoch 47/100 [Train]: 100% 8/8 [00:00<00:00, 88.86it/s]\n",
            "Epoch 47/100 [Valid]: 100% 2/2 [00:00<00:00, 245.09it/s]\n",
            "Epoch [47/100] | Train Loss: 0.241532 | Validation Loss: 0.207641\n",
            "Epoch 48/100 [Train]: 100% 8/8 [00:00<00:00, 89.96it/s]\n",
            "Epoch 48/100 [Valid]: 100% 2/2 [00:00<00:00, 234.78it/s]\n",
            "Epoch [48/100] | Train Loss: 0.241384 | Validation Loss: 0.206961\n",
            "Epoch 49/100 [Train]: 100% 8/8 [00:00<00:00, 70.56it/s]\n",
            "Epoch 49/100 [Valid]: 100% 2/2 [00:00<00:00, 255.63it/s]\n",
            "Epoch [49/100] | Train Loss: 0.242992 | Validation Loss: 0.207145\n",
            "Epoch 50/100 [Train]: 100% 8/8 [00:00<00:00, 93.75it/s]\n",
            "Epoch 50/100 [Valid]: 100% 2/2 [00:00<00:00, 243.04it/s]\n",
            "Epoch [50/100] | Train Loss: 0.243866 | Validation Loss: 0.214831\n",
            "Epoch 51/100 [Train]: 100% 8/8 [00:00<00:00, 92.24it/s]\n",
            "Epoch 51/100 [Valid]: 100% 2/2 [00:00<00:00, 249.32it/s]\n",
            "Epoch [51/100] | Train Loss: 0.242061 | Validation Loss: 0.207110\n",
            "Epoch 52/100 [Train]: 100% 8/8 [00:00<00:00, 93.49it/s]\n",
            "Epoch 52/100 [Valid]: 100% 2/2 [00:00<00:00, 248.82it/s]\n",
            "Epoch [52/100] | Train Loss: 0.242102 | Validation Loss: 0.209796\n",
            "Epoch 53/100 [Train]: 100% 8/8 [00:00<00:00, 93.44it/s]\n",
            "Epoch 53/100 [Valid]: 100% 2/2 [00:00<00:00, 230.93it/s]\n",
            "Epoch [53/100] | Train Loss: 0.241739 | Validation Loss: 0.206739\n",
            "Epoch 54/100 [Train]: 100% 8/8 [00:00<00:00, 89.17it/s]\n",
            "Epoch 54/100 [Valid]: 100% 2/2 [00:00<00:00, 241.77it/s]\n",
            "Epoch [54/100] | Train Loss: 0.239786 | Validation Loss: 0.208379\n",
            "Epoch 55/100 [Train]: 100% 8/8 [00:00<00:00, 93.66it/s]\n",
            "Epoch 55/100 [Valid]: 100% 2/2 [00:00<00:00, 253.77it/s]\n",
            "Epoch [55/100] | Train Loss: 0.242535 | Validation Loss: 0.219919\n",
            "Epoch 56/100 [Train]: 100% 8/8 [00:00<00:00, 91.31it/s]\n",
            "Epoch 56/100 [Valid]: 100% 2/2 [00:00<00:00, 246.85it/s]\n",
            "Epoch [56/100] | Train Loss: 0.241474 | Validation Loss: 0.212738\n",
            "Epoch 57/100 [Train]: 100% 8/8 [00:00<00:00, 90.44it/s]\n",
            "Epoch 57/100 [Valid]: 100% 2/2 [00:00<00:00, 240.89it/s]\n",
            "Epoch [57/100] | Train Loss: 0.238007 | Validation Loss: 0.218315\n",
            "Epoch 58/100 [Train]: 100% 8/8 [00:00<00:00, 91.07it/s]\n",
            "Epoch 58/100 [Valid]: 100% 2/2 [00:00<00:00, 239.43it/s]\n",
            "Epoch [58/100] | Train Loss: 0.237313 | Validation Loss: 0.211628\n",
            "Epoch 59/100 [Train]: 100% 8/8 [00:00<00:00, 87.57it/s]\n",
            "Epoch 59/100 [Valid]: 100% 2/2 [00:00<00:00, 249.93it/s]\n",
            "Epoch [59/100] | Train Loss: 0.246645 | Validation Loss: 0.222563\n",
            "Epoch 60/100 [Train]: 100% 8/8 [00:00<00:00, 75.80it/s]\n",
            "Epoch 60/100 [Valid]: 100% 2/2 [00:00<00:00, 227.20it/s]\n",
            "Epoch [60/100] | Train Loss: 0.244610 | Validation Loss: 0.209833\n",
            "Epoch 61/100 [Train]: 100% 8/8 [00:00<00:00, 91.51it/s]\n",
            "Epoch 61/100 [Valid]: 100% 2/2 [00:00<00:00, 239.26it/s]\n",
            "Epoch [61/100] | Train Loss: 0.241957 | Validation Loss: 0.207806\n",
            "Epoch 62/100 [Train]: 100% 8/8 [00:00<00:00, 92.13it/s]\n",
            "Epoch 62/100 [Valid]: 100% 2/2 [00:00<00:00, 246.99it/s]\n",
            "Epoch [62/100] | Train Loss: 0.241688 | Validation Loss: 0.211908\n",
            "Epoch 63/100 [Train]: 100% 8/8 [00:00<00:00, 84.20it/s]\n",
            "Epoch 63/100 [Valid]: 100% 2/2 [00:00<00:00, 247.27it/s]\n",
            "Epoch [63/100] | Train Loss: 0.242445 | Validation Loss: 0.212168\n",
            "Epoch 64/100 [Train]: 100% 8/8 [00:00<00:00, 90.06it/s]\n",
            "Epoch 64/100 [Valid]: 100% 2/2 [00:00<00:00, 246.35it/s]\n",
            "Epoch [64/100] | Train Loss: 0.242466 | Validation Loss: 0.207167\n",
            "Epoch 65/100 [Train]: 100% 8/8 [00:00<00:00, 79.23it/s]\n",
            "Epoch 65/100 [Valid]: 100% 2/2 [00:00<00:00, 206.95it/s]\n",
            "Epoch [65/100] | Train Loss: 0.242096 | Validation Loss: 0.207574\n",
            "Epoch 66/100 [Train]: 100% 8/8 [00:00<00:00, 85.72it/s]\n",
            "Epoch 66/100 [Valid]: 100% 2/2 [00:00<00:00, 244.03it/s]\n",
            "Epoch [66/100] | Train Loss: 0.241925 | Validation Loss: 0.212066\n",
            "Epoch 67/100 [Train]: 100% 8/8 [00:00<00:00, 89.51it/s]\n",
            "Epoch 67/100 [Valid]: 100% 2/2 [00:00<00:00, 213.52it/s]\n",
            "Epoch [67/100] | Train Loss: 0.241323 | Validation Loss: 0.207450\n",
            "Epoch 68/100 [Train]: 100% 8/8 [00:00<00:00, 80.80it/s]\n",
            "Epoch 68/100 [Valid]: 100% 2/2 [00:00<00:00, 243.22it/s]\n",
            "Epoch [68/100] | Train Loss: 0.241250 | Validation Loss: 0.207282\n",
            "Epoch 69/100 [Train]: 100% 8/8 [00:00<00:00, 86.87it/s]\n",
            "Epoch 69/100 [Valid]: 100% 2/2 [00:00<00:00, 249.47it/s]\n",
            "Epoch [69/100] | Train Loss: 0.240878 | Validation Loss: 0.207090\n",
            "Epoch 70/100 [Train]: 100% 8/8 [00:00<00:00, 79.53it/s]\n",
            "Epoch 70/100 [Valid]: 100% 2/2 [00:00<00:00, 256.02it/s]\n",
            "Epoch [70/100] | Train Loss: 0.243024 | Validation Loss: 0.207296\n",
            "Epoch 71/100 [Train]: 100% 8/8 [00:00<00:00, 93.80it/s]\n",
            "Epoch 71/100 [Valid]: 100% 2/2 [00:00<00:00, 243.18it/s]\n",
            "Epoch [71/100] | Train Loss: 0.243736 | Validation Loss: 0.208182\n",
            "Epoch 72/100 [Train]: 100% 8/8 [00:00<00:00, 91.29it/s]\n",
            "Epoch 72/100 [Valid]: 100% 2/2 [00:00<00:00, 243.54it/s]\n",
            "Epoch [72/100] | Train Loss: 0.242401 | Validation Loss: 0.208034\n",
            "Epoch 73/100 [Train]: 100% 8/8 [00:00<00:00, 92.72it/s]\n",
            "Epoch 73/100 [Valid]: 100% 2/2 [00:00<00:00, 246.80it/s]\n",
            "Epoch [73/100] | Train Loss: 0.241451 | Validation Loss: 0.208731\n",
            "Epoch 74/100 [Train]: 100% 8/8 [00:00<00:00, 94.90it/s]\n",
            "Epoch 74/100 [Valid]: 100% 2/2 [00:00<00:00, 240.16it/s]\n",
            "Epoch [74/100] | Train Loss: 0.241216 | Validation Loss: 0.209736\n",
            "Epoch 75/100 [Train]: 100% 8/8 [00:00<00:00, 89.98it/s]\n",
            "Epoch 75/100 [Valid]: 100% 2/2 [00:00<00:00, 251.28it/s]\n",
            "Epoch [75/100] | Train Loss: 0.241635 | Validation Loss: 0.208838\n",
            "Epoch 76/100 [Train]: 100% 8/8 [00:00<00:00, 82.76it/s]\n",
            "Epoch 76/100 [Valid]: 100% 2/2 [00:00<00:00, 213.53it/s]\n",
            "Epoch [76/100] | Train Loss: 0.241392 | Validation Loss: 0.207521\n",
            "Epoch 77/100 [Train]: 100% 8/8 [00:00<00:00, 87.23it/s]\n",
            "Epoch 77/100 [Valid]: 100% 2/2 [00:00<00:00, 225.09it/s]\n",
            "Epoch [77/100] | Train Loss: 0.240849 | Validation Loss: 0.218892\n",
            "Epoch 78/100 [Train]: 100% 8/8 [00:00<00:00, 94.20it/s]\n",
            "Epoch 78/100 [Valid]: 100% 2/2 [00:00<00:00, 246.48it/s]\n",
            "Epoch [78/100] | Train Loss: 0.245068 | Validation Loss: 0.209630\n",
            "Epoch 79/100 [Train]: 100% 8/8 [00:00<00:00, 92.47it/s]\n",
            "Epoch 79/100 [Valid]: 100% 2/2 [00:00<00:00, 235.82it/s]\n",
            "Epoch [79/100] | Train Loss: 0.244308 | Validation Loss: 0.206951\n",
            "Epoch 80/100 [Train]: 100% 8/8 [00:00<00:00, 79.75it/s]\n",
            "Epoch 80/100 [Valid]: 100% 2/2 [00:00<00:00, 248.54it/s]\n",
            "Epoch [80/100] | Train Loss: 0.241130 | Validation Loss: 0.211463\n",
            "Epoch 81/100 [Train]: 100% 8/8 [00:00<00:00, 92.76it/s]\n",
            "Epoch 81/100 [Valid]: 100% 2/2 [00:00<00:00, 253.63it/s]\n",
            "Epoch [81/100] | Train Loss: 0.239249 | Validation Loss: 0.214535\n",
            "Epoch 82/100 [Train]: 100% 8/8 [00:00<00:00, 91.11it/s]\n",
            "Epoch 82/100 [Valid]: 100% 2/2 [00:00<00:00, 249.94it/s]\n",
            "Epoch [82/100] | Train Loss: 0.242016 | Validation Loss: 0.211358\n",
            "Epoch 83/100 [Train]: 100% 8/8 [00:00<00:00, 87.49it/s]\n",
            "Epoch 83/100 [Valid]: 100% 2/2 [00:00<00:00, 244.36it/s]\n",
            "Epoch [83/100] | Train Loss: 0.242576 | Validation Loss: 0.207373\n",
            "Epoch 84/100 [Train]: 100% 8/8 [00:00<00:00, 91.52it/s]\n",
            "Epoch 84/100 [Valid]: 100% 2/2 [00:00<00:00, 250.43it/s]\n",
            "Epoch [84/100] | Train Loss: 0.244205 | Validation Loss: 0.209236\n",
            "Epoch 85/100 [Train]: 100% 8/8 [00:00<00:00, 86.04it/s]\n",
            "Epoch 85/100 [Valid]: 100% 2/2 [00:00<00:00, 241.28it/s]\n",
            "Epoch [85/100] | Train Loss: 0.243732 | Validation Loss: 0.217229\n",
            "Epoch 86/100 [Train]: 100% 8/8 [00:00<00:00, 77.99it/s]\n",
            "Epoch 86/100 [Valid]: 100% 2/2 [00:00<00:00, 249.73it/s]\n",
            "Epoch [86/100] | Train Loss: 0.243316 | Validation Loss: 0.207946\n",
            "Epoch 87/100 [Train]: 100% 8/8 [00:00<00:00, 85.53it/s]\n",
            "Epoch 87/100 [Valid]: 100% 2/2 [00:00<00:00, 240.62it/s]\n",
            "Epoch [87/100] | Train Loss: 0.241975 | Validation Loss: 0.208688\n",
            "Epoch 88/100 [Train]: 100% 8/8 [00:00<00:00, 91.88it/s]\n",
            "Epoch 88/100 [Valid]: 100% 2/2 [00:00<00:00, 246.71it/s]\n",
            "Epoch [88/100] | Train Loss: 0.242925 | Validation Loss: 0.212133\n",
            "Epoch 89/100 [Train]: 100% 8/8 [00:00<00:00, 89.94it/s]\n",
            "Epoch 89/100 [Valid]: 100% 2/2 [00:00<00:00, 249.42it/s]\n",
            "Epoch [89/100] | Train Loss: 0.241876 | Validation Loss: 0.208002\n",
            "Epoch 90/100 [Train]: 100% 8/8 [00:00<00:00, 89.62it/s]\n",
            "Epoch 90/100 [Valid]: 100% 2/2 [00:00<00:00, 183.37it/s]\n",
            "Epoch [90/100] | Train Loss: 0.240915 | Validation Loss: 0.209495\n",
            "Epoch 91/100 [Train]: 100% 8/8 [00:00<00:00, 80.42it/s]\n",
            "Epoch 91/100 [Valid]: 100% 2/2 [00:00<00:00, 250.29it/s]\n",
            "Epoch [91/100] | Train Loss: 0.241713 | Validation Loss: 0.210447\n",
            "Epoch 92/100 [Train]: 100% 8/8 [00:00<00:00, 91.45it/s]\n",
            "Epoch 92/100 [Valid]: 100% 2/2 [00:00<00:00, 242.54it/s]\n",
            "Epoch [92/100] | Train Loss: 0.239538 | Validation Loss: 0.207288\n",
            "Epoch 93/100 [Train]: 100% 8/8 [00:00<00:00, 92.46it/s]\n",
            "Epoch 93/100 [Valid]: 100% 2/2 [00:00<00:00, 252.29it/s]\n",
            "Epoch [93/100] | Train Loss: 0.239820 | Validation Loss: 0.210353\n",
            "Epoch 94/100 [Train]: 100% 8/8 [00:00<00:00, 86.37it/s]\n",
            "Epoch 94/100 [Valid]: 100% 2/2 [00:00<00:00, 254.39it/s]\n",
            "Epoch [94/100] | Train Loss: 0.238581 | Validation Loss: 0.207769\n",
            "Epoch 95/100 [Train]: 100% 8/8 [00:00<00:00, 89.47it/s]\n",
            "Epoch 95/100 [Valid]: 100% 2/2 [00:00<00:00, 243.69it/s]\n",
            "Epoch [95/100] | Train Loss: 0.238985 | Validation Loss: 0.254745\n",
            "Epoch 96/100 [Train]: 100% 8/8 [00:00<00:00, 78.04it/s]\n",
            "Epoch 96/100 [Valid]: 100% 2/2 [00:00<00:00, 229.86it/s]\n",
            "Epoch [96/100] | Train Loss: 0.247846 | Validation Loss: 0.215098\n",
            "Epoch 97/100 [Train]: 100% 8/8 [00:00<00:00, 90.58it/s]\n",
            "Epoch 97/100 [Valid]: 100% 2/2 [00:00<00:00, 250.53it/s]\n",
            "Epoch [97/100] | Train Loss: 0.242486 | Validation Loss: 0.207760\n",
            "Epoch 98/100 [Train]: 100% 8/8 [00:00<00:00, 93.81it/s]\n",
            "Epoch 98/100 [Valid]: 100% 2/2 [00:00<00:00, 243.88it/s]\n",
            "Epoch [98/100] | Train Loss: 0.240618 | Validation Loss: 0.208304\n",
            "Epoch 99/100 [Train]: 100% 8/8 [00:00<00:00, 62.41it/s]\n",
            "Epoch 99/100 [Valid]: 100% 2/2 [00:00<00:00, 205.44it/s]\n",
            "Epoch [99/100] | Train Loss: 0.241437 | Validation Loss: 0.209335\n",
            "Epoch 100/100 [Train]: 100% 8/8 [00:00<00:00, 59.47it/s]\n",
            "Epoch 100/100 [Valid]: 100% 2/2 [00:00<00:00, 195.51it/s]\n",
            "Epoch [100/100] | Train Loss: 0.240706 | Validation Loss: 0.209151\n",
            "\n",
            "--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\n",
            "‚úÖ Model cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = 0.206705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "# evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from model import FinalModel\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ƒëa nhi·ªám tr√™n t·∫≠p test.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu\n",
        "    _, test_loader, target_scaler, volatility_scaler = get_data_loaders()\n",
        "    if not test_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. T·∫£i l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "    print(f\"ƒêang t·∫£i m√¥ h√¨nh t·ª´: {TRAINED_MODEL_PATH}\")\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n",
        "        model.to(device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i '{TRAINED_MODEL_PATH}'. Vui l√≤ng ch·∫°y train.py tr∆∞·ªõc.\")\n",
        "        return\n",
        "\n",
        "    # 3. ƒê√°nh gi√° tr√™n t·∫≠p Test\n",
        "    model.eval()\n",
        "    all_price_preds, all_trend_preds, all_vol_preds = [], [], []\n",
        "    all_price_lbls, all_trend_lbls, all_vol_lbls = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, (price_lbl, trend_lbl, vol_lbl) in test_loader:\n",
        "            features = features.to(device)\n",
        "\n",
        "            price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "            # L∆∞u l·∫°i k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
        "            all_price_preds.extend(price_pred.cpu().numpy())\n",
        "            all_trend_preds.extend(trend_pred.cpu().numpy())\n",
        "\n",
        "            all_price_lbls.extend(price_lbl.numpy())\n",
        "            all_trend_lbls.extend(trend_lbl.numpy())\n",
        "\n",
        "    # 4. X·ª≠ l√Ω v√† Gi·∫£i chu·∫©n h√≥a\n",
        "    # --- X·ª≠ l√Ω cho D·ª± ƒëo√°n Gi√° ---\n",
        "    price_preds = np.array(all_price_preds).reshape(-1, 1)\n",
        "    price_actuals = np.array(all_price_lbls).reshape(-1, 1)\n",
        "    original_price_preds = target_scaler.inverse_transform(price_preds)\n",
        "    original_price_actuals = target_scaler.inverse_transform(price_actuals)\n",
        "\n",
        "    # --- X·ª≠ l√Ω cho D·ª± ƒëo√°n Xu h∆∞·ªõng ---\n",
        "    # Chuy·ªÉn ƒë·ªïi output c·ªßa model (logits) th√†nh x√°c su·∫•t r·ªìi th√†nh nh√£n (0 ho·∫∑c 1)\n",
        "    trend_probs = torch.sigmoid(torch.tensor(all_trend_preds)).numpy()\n",
        "    trend_preds_labels = (trend_probs > 0.5).astype(int)\n",
        "    trend_actuals = np.array(all_trend_lbls)\n",
        "\n",
        "    # 5. T√≠nh to√°n c√°c ch·ªâ s·ªë v√† in k·∫øt qu·∫£\n",
        "    print(\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model Cu·ªëi c√πng) ---\")\n",
        "\n",
        "    # --- Nhi·ªám v·ª• 1: D·ª± ƒëo√°n Gi√° ---\n",
        "    mae = np.mean(np.abs(original_price_preds - original_price_actuals))\n",
        "    print(f\"üéØ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # --- Nhi·ªám v·ª• 2: D·ª± ƒëo√°n Xu h∆∞·ªõng ---\n",
        "    accuracy = accuracy_score(trend_actuals, trend_preds_labels)\n",
        "    print(f\"üéØ [Xu h∆∞·ªõng] ƒê·ªô ch√≠nh x√°c (Accuracy): {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # 6. V·∫Ω bi·ªÉu ƒë·ªì\n",
        "    # --- Bi·ªÉu ƒë·ªì 1: So s√°nh Gi√° ---\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_price_actuals, label='Gi√° tr·ªã Th·ª±c t·∫ø', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_price_preds, label='Gi√° tr·ªã D·ª± ƒëo√°n', color='red', linestyle='--')\n",
        "    plt.title('So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n (Model Cu·ªëi c√πng)')\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('final_prediction_vs_actual.png')\n",
        "    print(\"\\n‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh gi√° v√†o file 'final_prediction_vs_actual.png'\")\n",
        "\n",
        "    # --- Bi·ªÉu ƒë·ªì 2: Ma tr·∫≠n nh·∫ßm l·∫´n cho Xu h∆∞·ªõng ---\n",
        "    cm = confusion_matrix(trend_actuals, trend_preds_labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Gi·∫£m', 'TƒÉng'])\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "    ax.set_title('Ma tr·∫≠n Nh·∫ßm l·∫´n - D·ª± ƒëo√°n Xu h∆∞·ªõng')\n",
        "    plt.savefig('final_confusion_matrix.png')\n",
        "    print(\"‚úÖ ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n v√†o file 'final_confusion_matrix.png'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zjJkk5rZpZH",
        "outputId": "a9707295-753d-48ac-c6d5-031e905ae05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hphM-ljBZqXv",
        "outputId": "1f41edca-111c-46c4-f957-7dcadf7a8a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "ƒêang t·∫£i m√¥ h√¨nh t·ª´: final_hierarchical_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model Cu·ªëi c√πng) ---\n",
            "üéØ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 18.6659 (ƒëi·ªÉm VN-Index)\n",
            "üéØ [Xu h∆∞·ªõng] ƒê·ªô ch√≠nh x√°c (Accuracy): 61.29%\n",
            "\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh gi√° v√†o file 'final_prediction_vs_actual.png'\n",
            "‚úÖ ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n v√†o file 'final_confusion_matrix.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***B√¢y gi·ªù √°p d·ª•ng t·ª± ƒë·ªông k·ªπ thu·∫≠t t·ªëi ∆∞u tham s·ªë***"
      ],
      "metadata": {
        "id": "WJiVhwjScBL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import BATCH_SIZE, LOOKBACK_WINDOW # Ch·ªâ import nh·ªØng g√¨ c·∫ßn cho vi·ªác test\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(FinalModel, self).__init__()\n",
        "\n",
        "        # S·ª¨A L·ªñI: L∆∞u l·∫°i c√°c tham s·ªë ki·∫øn tr√∫c l√†m thu·ªôc t√≠nh c·ªßa class\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.num_scales = num_scales\n",
        "\n",
        "        # --- Ph·∫ßn th√¢n chung (Shared Body) ---\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(input_size=self.input_feature_size, hidden_size=lstm_hidden_units,\n",
        "                    num_layers=lstm_num_layers, batch_first=True, dropout=0.2 if lstm_num_layers > 1 else 0)\n",
        "            for _ in range(self.num_scales)\n",
        "        ])\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_units * self.num_scales,\n",
        "                                             num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        shared_feature_dim = lstm_hidden_units * self.num_scales\n",
        "        self.intermediate_layer = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim, shared_feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # --- C√°c ƒê·∫ßu ra ƒêa nhi·ªám (Multi-Task Heads) ---\n",
        "        self.price_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "        self.trend_head = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim // 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        self.volatility_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_scales):\n",
        "            # S·ª¨A L·ªñI: D√πng thu·ªôc t√≠nh c·ªßa class (self.input_feature_size) thay v√¨ bi·∫øn to√†n c·ª•c\n",
        "            start_idx = i * self.input_feature_size\n",
        "            end_idx = (i + 1) * self.input_feature_size\n",
        "\n",
        "            branch_input = x[:, :, start_idx:end_idx]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "        shared_features = self.intermediate_layer(attention_output[:, -1, :])\n",
        "\n",
        "        price_prediction = self.price_head(shared_features)\n",
        "        trend_prediction = self.trend_head(shared_features)\n",
        "        volatility_prediction = self.volatility_head(shared_features)\n",
        "\n",
        "        return price_prediction.squeeze(), trend_prediction.squeeze(), volatility_prediction.squeeze()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ƒêo·∫°n test n√†y c·∫ßn import t·ª´ config\n",
        "    from config import NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS, ATTENTION_NUM_HEADS\n",
        "\n",
        "    print(\"--- Ki·ªÉm tra ki·∫øn tr√∫c Final Model (ƒë√£ s·ª≠a l·ªói) ---\")\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    print(model)\n",
        "    dummy_input = torch.randn(BATCH_SIZE, LOOKBACK_WINDOW, NUM_BASE_FEATURES * NUM_SCALES)\n",
        "    price, trend, volatility = model(dummy_input)\n",
        "\n",
        "    print(f\"\\nShape c·ªßa input: {dummy_input.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Price: {price.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Trend: {trend.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Volatility: {volatility.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw7VJVcyd_l2",
        "outputId": "32327270-caa9-46a7-b101-e437b19651e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "# S·ª¨A L·ªñI: Tr·ªè ƒë·∫øn ƒë√∫ng file d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω (c√≥ 30 c·ªôt)\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "TRAINED_MODEL_PATH = 'final_optimized_model.pth'\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- Tham s·ªë Ki·∫øn tr√∫c M·∫∑c ƒë·ªãnh ---\n",
        "NUM_BASE_FEATURES = 30\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "LSTM_HIDDEN_UNITS = 128\n",
        "LSTM_NUM_LAYERS = 2\n",
        "ATTENTION_NUM_HEADS = 4\n",
        "\n",
        "# --- Tham s·ªë cho Multi-Task Learning ---\n",
        "LOSS_WEIGHTS = {\n",
        "    'price': 0.6,\n",
        "    'trend': 0.3,\n",
        "    'volatility': 0.1\n",
        "}\n",
        "\n",
        "# --- C·∫•u h√¨nh cho Optuna ---\n",
        "OPTUNA_N_TRIALS = 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmuDAV-_cp8l",
        "outputId": "7681d90e-6397-432e-bef9-3ae288d74ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna -q"
      ],
      "metadata": {
        "id": "Pt92_k8vcKn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_tuner.py\n",
        "# train_tuner.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LOSS_WEIGHTS,\n",
        "    OPTUNA_N_TRIALS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from model import FinalModel\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "# 1. ƒê·ªãnh nghƒ©a H√†m M·ª•c ti√™u (Objective Function)\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    H√†m n√†y ƒë∆∞·ª£c Optuna g·ªçi trong m·ªói l·∫ßn th·ª≠ nghi·ªám.\n",
        "    N√≥ s·∫Ω hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi b·ªô tham s·ªë do `trial` ƒë·ªÅ xu·∫•t v√† tr·∫£ v·ªÅ val_loss.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # --- Optuna ƒë·ªÅ xu·∫•t c√°c si√™u tham s·ªë ---\n",
        "    # `trial.suggest_` s·∫Ω ch·ªçn c√°c gi√° tr·ªã trong kho·∫£ng cho tr∆∞·ªõc\n",
        "    lstm_hidden = trial.suggest_int(\"lstm_hidden_units\", 64, 256, step=32)\n",
        "    lstm_layers = trial.suggest_int(\"lstm_num_layers\", 1, 3)\n",
        "    attention_heads = trial.suggest_categorical(\"attention_num_heads\", [2, 4, 8])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
        "\n",
        "    # 2. T·∫£i d·ªØ li·ªáu v√† kh·ªüi t·∫°o model v·ªõi c√°c tham s·ªë m·ªõi\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=lstm_hidden,\n",
        "        lstm_num_layers=lstm_layers,\n",
        "        num_heads=attention_heads\n",
        "    ).to(device)\n",
        "\n",
        "    price_loss_fn = nn.MSELoss()\n",
        "    trend_loss_fn = nn.BCEWithLogitsLoss()\n",
        "    vol_loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Gi·∫£m s·ªë epoch trong m·ªói l·∫ßn th·ª≠ ƒë·ªÉ ch·∫°y nhanh h∆°n\n",
        "    num_epochs = 30\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # 3. V√≤ng l·∫∑p hu·∫•n luy·ªán r√∫t g·ªçn\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for features, (p_lbl, t_lbl, v_lbl) in train_loader:\n",
        "            features, p_lbl, t_lbl, v_lbl = features.to(device), p_lbl.to(device), t_lbl.to(device), v_lbl.to(device)\n",
        "            p_pred, t_pred, v_pred = model(features)\n",
        "            loss = (LOSS_WEIGHTS['price'] * price_loss_fn(p_pred, p_lbl) +\n",
        "                    LOSS_WEIGHTS['trend'] * trend_loss_fn(t_pred, t_lbl) +\n",
        "                    LOSS_WEIGHTS['volatility'] * vol_loss_fn(v_pred, v_lbl))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # ƒê√°nh gi√° tr√™n t·∫≠p validation sau m·ªói epoch\n",
        "        model.eval()\n",
        "        current_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, (p_lbl, t_lbl, v_lbl) in test_loader:\n",
        "                features, p_lbl, t_lbl, v_lbl = features.to(device), p_lbl.to(device), t_lbl.to(device), v_lbl.to(device)\n",
        "                p_pred, t_pred, v_pred = model(features)\n",
        "                loss = (LOSS_WEIGHTS['price'] * price_loss_fn(p_pred, p_lbl) +\n",
        "                        LOSS_WEIGHTS['trend'] * trend_loss_fn(t_pred, t_lbl) +\n",
        "                        LOSS_WEIGHTS['volatility'] * vol_loss_fn(v_pred, v_lbl))\n",
        "                current_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = current_val_loss / len(test_loader)\n",
        "\n",
        "        # B√°o c√°o k·∫øt qu·∫£ cho Optuna\n",
        "        trial.report(avg_val_loss, epoch)\n",
        "\n",
        "        # C·∫Øt t·ªâa (Pruning): D·ª´ng s·ªõm nh·ªØng l·∫ßn th·ª≠ kh√¥ng c√≥ tri·ªÉn v·ªçng\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "\n",
        "    # H√†m objective tr·∫£ v·ªÅ validation loss cu·ªëi c√πng c·ªßa l·∫ßn th·ª≠ n√†y\n",
        "    return best_val_loss\n",
        "\n",
        "# 4. Ch·∫°y Cu·ªôc t√¨m ki·∫øm (Study)\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- B·∫Øt ƒë·∫ßu T·ªëi ∆∞u h√≥a Si√™u tham s·ªë v·ªõi Optuna ---\")\n",
        "\n",
        "    # T·∫°o m·ªôt \"study\" v·ªõi m·ª•c ti√™u l√† \"minimize\" (t·ªëi thi·ªÉu h√≥a) h√†m objective\n",
        "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
        "\n",
        "    # B·∫Øt ƒë·∫ßu ch·∫°y N_TRIALS l·∫ßn th·ª≠\n",
        "    study.optimize(objective, n_trials=OPTUNA_N_TRIALS)\n",
        "\n",
        "    # In ra k·∫øt qu·∫£\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[optuna.trial.TrialState.COMPLETE])\n",
        "\n",
        "    print(\"\\n--- T·ªëi ∆∞u h√≥a Ho√†n t·∫•t ---\")\n",
        "    print(\"Study statistics: \")\n",
        "    print(f\"  S·ªë l·∫ßn th·ª≠ ho√†n th√†nh: {len(complete_trials)}\")\n",
        "    print(f\"  S·ªë l·∫ßn th·ª≠ b·ªã c·∫Øt t·ªâa (d·ª´ng s·ªõm): {len(pruned_trials)}\")\n",
        "\n",
        "    print(\"\\nüèÜ B·ªò THAM S·ªê T·ªêT NH·∫§T:\")\n",
        "    best_params = study.best_trial.params\n",
        "    for key, value in best_params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(f\"\\nGi√° tr·ªã Validation Loss t·ªët nh·∫•t: {study.best_value:.6f}\")\n",
        "    print(\"\\n=> G·ª£i √Ω: H√£y c·∫≠p nh·∫≠t c√°c tham s·ªë n√†y v√†o file config.py v√† ch·∫°y l·∫°i file train.py ch√≠nh th·ª©c ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh cu·ªëi c√πng.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L940aSNscO28",
        "outputId": "823e1a8d-3033-4469-9141-b0d78ecf64b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_tuner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_tuner.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3jc4C0vcVh5",
        "outputId": "84021589-620a-4402-c837-7fa7e3d28b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- B·∫Øt ƒë·∫ßu T·ªëi ∆∞u h√≥a Si√™u tham s·ªë v·ªõi Optuna ---\n",
            "\u001b[32m[I 2025-07-09 13:57:29,154]\u001b[0m A new study created in memory with name: no-name-5cd36d74-5a33-4460-ba86-d9f0954f3479\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:57:38,735]\u001b[0m Trial 0 finished with value: 0.20590250194072723 and parameters: {'lstm_hidden_units': 224, 'lstm_num_layers': 3, 'attention_num_heads': 4, 'learning_rate': 0.0009194168469382918}. Best is trial 0 with value: 0.20590250194072723.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:57:48,096]\u001b[0m Trial 1 finished with value: 0.20616554468870163 and parameters: {'lstm_hidden_units': 256, 'lstm_num_layers': 3, 'attention_num_heads': 2, 'learning_rate': 0.0005738733779054687}. Best is trial 0 with value: 0.20590250194072723.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:57:54,001]\u001b[0m Trial 2 finished with value: 0.20579843968153 and parameters: {'lstm_hidden_units': 192, 'lstm_num_layers': 1, 'attention_num_heads': 8, 'learning_rate': 0.0022088373370435133}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:57:57,506]\u001b[0m Trial 3 finished with value: 0.21266303211450577 and parameters: {'lstm_hidden_units': 128, 'lstm_num_layers': 3, 'attention_num_heads': 4, 'learning_rate': 0.009764102472444075}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:07,180]\u001b[0m Trial 4 finished with value: 0.20623130351305008 and parameters: {'lstm_hidden_units': 256, 'lstm_num_layers': 3, 'attention_num_heads': 2, 'learning_rate': 0.00031495121036367884}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:07,346]\u001b[0m Trial 5 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:07,859]\u001b[0m Trial 6 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:08,031]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:11,397]\u001b[0m Trial 8 finished with value: 0.2060781791806221 and parameters: {'lstm_hidden_units': 96, 'lstm_num_layers': 3, 'attention_num_heads': 4, 'learning_rate': 0.0023009554850280628}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:11,586]\u001b[0m Trial 9 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:11,870]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:17,841]\u001b[0m Trial 11 finished with value: 0.20584291219711304 and parameters: {'lstm_hidden_units': 192, 'lstm_num_layers': 1, 'attention_num_heads': 4, 'learning_rate': 0.0014804013260213255}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:18,314]\u001b[0m Trial 12 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:18,609]\u001b[0m Trial 13 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:22,735]\u001b[0m Trial 14 finished with value: 0.20658081769943237 and parameters: {'lstm_hidden_units': 160, 'lstm_num_layers': 1, 'attention_num_heads': 8, 'learning_rate': 0.0011737729977820092}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:26,922]\u001b[0m Trial 15 finished with value: 0.20649652928113937 and parameters: {'lstm_hidden_units': 160, 'lstm_num_layers': 1, 'attention_num_heads': 4, 'learning_rate': 0.0011240116336460214}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:27,240]\u001b[0m Trial 16 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:27,619]\u001b[0m Trial 17 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:27,970]\u001b[0m Trial 18 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:34,331]\u001b[0m Trial 19 finished with value: 0.20660246163606644 and parameters: {'lstm_hidden_units': 224, 'lstm_num_layers': 2, 'attention_num_heads': 2, 'learning_rate': 0.0006705080131930841}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:34,468]\u001b[0m Trial 20 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:34,999]\u001b[0m Trial 21 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:35,376]\u001b[0m Trial 22 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:35,670]\u001b[0m Trial 23 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:36,076]\u001b[0m Trial 24 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:36,294]\u001b[0m Trial 25 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:36,749]\u001b[0m Trial 26 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:36,998]\u001b[0m Trial 27 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:44,742]\u001b[0m Trial 28 finished with value: 0.20630258321762085 and parameters: {'lstm_hidden_units': 256, 'lstm_num_layers': 2, 'attention_num_heads': 2, 'learning_rate': 0.0009718987976502385}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:45,146]\u001b[0m Trial 29 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:45,639]\u001b[0m Trial 30 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:45,811]\u001b[0m Trial 31 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:46,102]\u001b[0m Trial 32 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:46,445]\u001b[0m Trial 33 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:46,646]\u001b[0m Trial 34 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:47,236]\u001b[0m Trial 35 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:47,423]\u001b[0m Trial 36 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:47,765]\u001b[0m Trial 37 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:47,939]\u001b[0m Trial 38 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:48,520]\u001b[0m Trial 39 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:48,671]\u001b[0m Trial 40 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:49,149]\u001b[0m Trial 41 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:49,660]\u001b[0m Trial 42 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:50,129]\u001b[0m Trial 43 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:51,040]\u001b[0m Trial 44 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:51,389]\u001b[0m Trial 45 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:52,025]\u001b[0m Trial 46 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:52,919]\u001b[0m Trial 47 pruned. \u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:59,418]\u001b[0m Trial 48 finished with value: 0.20631743222475052 and parameters: {'lstm_hidden_units': 224, 'lstm_num_layers': 2, 'attention_num_heads': 8, 'learning_rate': 0.0017515664567020995}. Best is trial 2 with value: 0.20579843968153.\u001b[0m\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\u001b[32m[I 2025-07-09 13:58:59,822]\u001b[0m Trial 49 pruned. \u001b[0m\n",
            "\n",
            "--- T·ªëi ∆∞u h√≥a Ho√†n t·∫•t ---\n",
            "Study statistics: \n",
            "  S·ªë l·∫ßn th·ª≠ ho√†n th√†nh: 12\n",
            "  S·ªë l·∫ßn th·ª≠ b·ªã c·∫Øt t·ªâa (d·ª´ng s·ªõm): 38\n",
            "\n",
            "üèÜ B·ªò THAM S·ªê T·ªêT NH·∫§T:\n",
            "  lstm_hidden_units: 192\n",
            "  lstm_num_layers: 1\n",
            "  attention_num_heads: 8\n",
            "  learning_rate: 0.0022088373370435133\n",
            "\n",
            "Gi√° tr·ªã Validation Loss t·ªët nh·∫•t: 0.205798\n",
            "\n",
            "=> G·ª£i √Ω: H√£y c·∫≠p nh·∫≠t c√°c tham s·ªë n√†y v√†o file config.py v√† ch·∫°y l·∫°i file train.py ch√≠nh th·ª©c ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh cu·ªëi c√πng.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Gi·ªù test th·ª≠ si√™u tham s·ªë m·ªõi***"
      ],
      "metadata": {
        "id": "YnVezlzefMLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import BATCH_SIZE, LOOKBACK_WINDOW # Ch·ªâ import nh·ªØng g√¨ c·∫ßn cho vi·ªác test\n",
        "\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(FinalModel, self).__init__()\n",
        "\n",
        "        # S·ª¨A L·ªñI: L∆∞u l·∫°i c√°c tham s·ªë ki·∫øn tr√∫c l√†m thu·ªôc t√≠nh c·ªßa class\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.num_scales = num_scales\n",
        "\n",
        "        # --- Ph·∫ßn th√¢n chung (Shared Body) ---\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(input_size=self.input_feature_size, hidden_size=lstm_hidden_units,\n",
        "                    num_layers=lstm_num_layers, batch_first=True, dropout=0.2 if lstm_num_layers > 1 else 0)\n",
        "            for _ in range(self.num_scales)\n",
        "        ])\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_units * self.num_scales,\n",
        "                                             num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        shared_feature_dim = lstm_hidden_units * self.num_scales\n",
        "        self.intermediate_layer = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim, shared_feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # --- C√°c ƒê·∫ßu ra ƒêa nhi·ªám (Multi-Task Heads) ---\n",
        "        self.price_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "        self.trend_head = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim // 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        self.volatility_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_scales):\n",
        "            # S·ª¨A L·ªñI: D√πng thu·ªôc t√≠nh c·ªßa class (self.input_feature_size) thay v√¨ bi·∫øn to√†n c·ª•c\n",
        "            start_idx = i * self.input_feature_size\n",
        "            end_idx = (i + 1) * self.input_feature_size\n",
        "\n",
        "            branch_input = x[:, :, start_idx:end_idx]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "        shared_features = self.intermediate_layer(attention_output[:, -1, :])\n",
        "\n",
        "        price_prediction = self.price_head(shared_features)\n",
        "        trend_prediction = self.trend_head(shared_features)\n",
        "        volatility_prediction = self.volatility_head(shared_features)\n",
        "\n",
        "        return price_prediction.squeeze(), trend_prediction.squeeze(), volatility_prediction.squeeze()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ƒêo·∫°n test n√†y c·∫ßn import t·ª´ config\n",
        "    from config import NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS, ATTENTION_NUM_HEADS\n",
        "\n",
        "    print(\"--- Ki·ªÉm tra ki·∫øn tr√∫c Final Model (ƒë√£ s·ª≠a l·ªói) ---\")\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    print(model)\n",
        "    dummy_input = torch.randn(BATCH_SIZE, LOOKBACK_WINDOW, NUM_BASE_FEATURES * NUM_SCALES)\n",
        "    price, trend, volatility = model(dummy_input)\n",
        "\n",
        "    print(f\"\\nShape c·ªßa input: {dummy_input.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Price: {price.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Trend: {trend.shape}\")\n",
        "    print(f\"Shape ƒë·∫ßu ra Volatility: {volatility.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDkbEg0IfSwx",
        "outputId": "c4e4df11-4962-425b-d96d-083711423c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwXI7qYDfYwY",
        "outputId": "e7b8c1b1-7ee6-4118-e965-7f39bce776a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ki·ªÉm tra ki·∫øn tr√∫c Final Model (ƒë√£ s·ª≠a l·ªói) ---\n",
            "FinalModel(\n",
            "  (lstm_branches): ModuleList(\n",
            "    (0-4): 5 x LSTM(30, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=640, out_features=640, bias=True)\n",
            "  )\n",
            "  (intermediate_layer): Sequential(\n",
            "    (0): Linear(in_features=640, out_features=320, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "  )\n",
            "  (price_head): Linear(in_features=320, out_features=1, bias=True)\n",
            "  (trend_head): Sequential(\n",
            "    (0): Linear(in_features=320, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            "  (volatility_head): Linear(in_features=320, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Shape c·ªßa input: torch.Size([32, 30, 150])\n",
            "Shape ƒë·∫ßu ra Price: torch.Size([32])\n",
            "Shape ƒë·∫ßu ra Trend: torch.Size([32])\n",
            "Shape ƒë·∫ßu ra Volatility: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.py\n",
        "# config.py\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file ---\n",
        "PROCESSED_DATA_PATH = 'vn_indices_processed.csv'\n",
        "# File model cu·ªëi c√πng sau khi ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u\n",
        "TRAINED_MODEL_PATH = 'final_optimized_model.pth'\n",
        "\n",
        "# --- Tham s·ªë t·∫°o Dataset ---\n",
        "LOOKBACK_WINDOW = 30\n",
        "TARGET_COLUMN = 'VNINDEX_Close'\n",
        "TEST_SET_SIZE = 0.2\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# --- Tham s·ªë Bi·∫øn ƒë·ªïi Wavelet ---\n",
        "WAVELET_FAMILY = 'db4'\n",
        "WAVELET_LEVEL = 4\n",
        "\n",
        "# --- THAM S·ªê KI·∫æN TR√öC T·ªêT NH·∫§T T·ª™ OPTUNA ---\n",
        "NUM_BASE_FEATURES = 30\n",
        "NUM_SCALES = WAVELET_LEVEL + 1\n",
        "LSTM_HIDDEN_UNITS = 192   # <--- C·∫¨P NH·∫¨T\n",
        "LSTM_NUM_LAYERS = 1       # <--- C·∫¨P NH·∫¨T\n",
        "ATTENTION_NUM_HEADS = 8   # <--- C·∫¨P NH·∫¨T\n",
        "\n",
        "# --- Tham s·ªë cho Multi-Task Learning ---\n",
        "LOSS_WEIGHTS = {\n",
        "    'price': 0.6,\n",
        "    'trend': 0.3,\n",
        "    'volatility': 0.1\n",
        "}\n",
        "\n",
        "# --- Tham s·ªë Hu·∫•n luy·ªán T·ªêT NH·∫§T T·ª™ OPTUNA ---\n",
        "LEARNING_RATE = 0.0022088373370435133 # <--- C·∫¨P NH·∫¨T\n",
        "NUM_EPOCHS = 100 # Gi·ªØ nguy√™n 100 epoch ƒë·ªÉ h·ªçc s√¢u h∆°n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "947Jx0Asfchk",
        "outputId": "8d701759-749e-4788-8f7f-b59b59f94ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, LOSS_WEIGHTS,\n",
        "    LEARNING_RATE, NUM_EPOCHS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from model import FinalModel\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "def run_training():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader: return\n",
        "\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    ).to(device)\n",
        "\n",
        "    # ƒê·ªãnh nghƒ©a c√°c h√†m loss cho t·ª´ng nhi·ªám v·ª•\n",
        "    price_loss_fn = nn.MSELoss() # Cho gi√° (h·ªìi quy)\n",
        "    # D√πng BCEWithLogitsLoss cho ƒë·ªô ·ªïn ƒë·ªãnh s·ªë h·ªçc, n√≥ ƒë√£ t√≠ch h·ª£p s·∫µn Sigmoid\n",
        "    trend_loss_fn = nn.BCEWithLogitsLoss() # Cho xu h∆∞·ªõng (ph√¢n lo·∫°i)\n",
        "    vol_loss_fn = nn.MSELoss() # Cho ƒë·ªô bi·∫øn ƒë·ªông (h·ªìi quy)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model Cu·ªëi c√πng (Hierarchical + Multi-Task) ---\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        # --- PHA HU·∫§N LUY·ªÜN ---\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for features, (price_lbl, trend_lbl, vol_lbl) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features = features.to(device)\n",
        "            price_lbl, trend_lbl, vol_lbl = price_lbl.to(device), trend_lbl.to(device), vol_lbl.to(device)\n",
        "\n",
        "            # L·∫•y 3 ƒë·∫ßu ra t·ª´ model\n",
        "            price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "            # T√≠nh to√°n loss cho t·ª´ng nhi·ªám v·ª•\n",
        "            loss_p = price_loss_fn(price_pred, price_lbl)\n",
        "            loss_t = trend_loss_fn(trend_pred, trend_lbl)\n",
        "            loss_v = vol_loss_fn(vol_pred, vol_lbl)\n",
        "\n",
        "            # T√≠nh loss t·ªïng h·ª£p d·ª±a tr√™n tr·ªçng s·ªë\n",
        "            total_loss = (LOSS_WEIGHTS['price'] * loss_p +\n",
        "                          LOSS_WEIGHTS['trend'] * loss_t +\n",
        "                          LOSS_WEIGHTS['volatility'] * loss_v)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += total_loss.item()\n",
        "\n",
        "        # --- PHA KI·ªÇM ƒê·ªäNH ---\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, (price_lbl, trend_lbl, vol_lbl) in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Valid]\"):\n",
        "                features = features.to(device)\n",
        "                price_lbl, trend_lbl, vol_lbl = price_lbl.to(device), trend_lbl.to(device), vol_lbl.to(device)\n",
        "\n",
        "                price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "                loss_p = price_loss_fn(price_pred, price_lbl)\n",
        "                loss_t = trend_loss_fn(trend_pred, trend_lbl)\n",
        "                loss_v = vol_loss_fn(vol_pred, vol_lbl)\n",
        "\n",
        "                total_loss = (LOSS_WEIGHTS['price'] * loss_p +\n",
        "                              LOSS_WEIGHTS['trend'] * loss_t +\n",
        "                              LOSS_WEIGHTS['volatility'] * loss_v)\n",
        "                total_val_loss += total_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), TRAINED_MODEL_PATH)\n",
        "            print(f\"   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o '{TRAINED_MODEL_PATH}'\")\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "    print(f\"‚úÖ Model cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = {best_val_loss:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbk01oqUfqwr",
        "outputId": "ff2ba4e6-1d51-46ec-9b27-809b6da347b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swMdOaG_fv7o",
        "outputId": "b51e0a92-923a-4ed9-9347-e4cdc4f12cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\n",
            "--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán Model Cu·ªëi c√πng (Hierarchical + Multi-Task) ---\n",
            "Epoch 1/100 [Train]: 100% 8/8 [00:00<00:00, 17.59it/s]\n",
            "Epoch 1/100 [Valid]: 100% 2/2 [00:00<00:00, 73.48it/s]\n",
            "Epoch [01/100] | Train Loss: 0.445531 | Validation Loss: 0.222070\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_optimized_model.pth'\n",
            "Epoch 2/100 [Train]: 100% 8/8 [00:00<00:00, 43.60it/s]\n",
            "Epoch 2/100 [Valid]: 100% 2/2 [00:00<00:00, 77.30it/s]\n",
            "Epoch [02/100] | Train Loss: 0.273318 | Validation Loss: 0.207194\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_optimized_model.pth'\n",
            "Epoch 3/100 [Train]: 100% 8/8 [00:00<00:00, 45.33it/s]\n",
            "Epoch 3/100 [Valid]: 100% 2/2 [00:00<00:00, 77.78it/s]\n",
            "Epoch [03/100] | Train Loss: 0.245420 | Validation Loss: 0.220386\n",
            "Epoch 4/100 [Train]: 100% 8/8 [00:00<00:00, 45.12it/s]\n",
            "Epoch 4/100 [Valid]: 100% 2/2 [00:00<00:00, 75.73it/s]\n",
            "Epoch [04/100] | Train Loss: 0.253762 | Validation Loss: 0.210849\n",
            "Epoch 5/100 [Train]: 100% 8/8 [00:00<00:00, 45.32it/s]\n",
            "Epoch 5/100 [Valid]: 100% 2/2 [00:00<00:00, 78.07it/s]\n",
            "Epoch [05/100] | Train Loss: 0.245228 | Validation Loss: 0.219912\n",
            "Epoch 6/100 [Train]: 100% 8/8 [00:00<00:00, 45.40it/s]\n",
            "Epoch 6/100 [Valid]: 100% 2/2 [00:00<00:00, 77.68it/s]\n",
            "Epoch [06/100] | Train Loss: 0.243556 | Validation Loss: 0.213404\n",
            "Epoch 7/100 [Train]: 100% 8/8 [00:00<00:00, 44.30it/s]\n",
            "Epoch 7/100 [Valid]: 100% 2/2 [00:00<00:00, 77.94it/s]\n",
            "Epoch [07/100] | Train Loss: 0.244947 | Validation Loss: 0.207900\n",
            "Epoch 8/100 [Train]: 100% 8/8 [00:00<00:00, 45.22it/s]\n",
            "Epoch 8/100 [Valid]: 100% 2/2 [00:00<00:00, 77.91it/s]\n",
            "Epoch [08/100] | Train Loss: 0.244304 | Validation Loss: 0.211158\n",
            "Epoch 9/100 [Train]: 100% 8/8 [00:00<00:00, 45.25it/s]\n",
            "Epoch 9/100 [Valid]: 100% 2/2 [00:00<00:00, 77.08it/s]\n",
            "Epoch [09/100] | Train Loss: 0.248317 | Validation Loss: 0.210626\n",
            "Epoch 10/100 [Train]: 100% 8/8 [00:00<00:00, 45.50it/s]\n",
            "Epoch 10/100 [Valid]: 100% 2/2 [00:00<00:00, 74.68it/s]\n",
            "Epoch [10/100] | Train Loss: 0.247588 | Validation Loss: 0.206120\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_optimized_model.pth'\n",
            "Epoch 11/100 [Train]: 100% 8/8 [00:00<00:00, 44.78it/s]\n",
            "Epoch 11/100 [Valid]: 100% 2/2 [00:00<00:00, 74.67it/s]\n",
            "Epoch [11/100] | Train Loss: 0.245616 | Validation Loss: 0.205983\n",
            "   -> Validation loss c·∫£i thi·ªán. ƒê√£ l∆∞u model t·ªët nh·∫•t v√†o 'final_optimized_model.pth'\n",
            "Epoch 12/100 [Train]: 100% 8/8 [00:00<00:00, 44.67it/s]\n",
            "Epoch 12/100 [Valid]: 100% 2/2 [00:00<00:00, 75.47it/s]\n",
            "Epoch [12/100] | Train Loss: 0.244543 | Validation Loss: 0.216944\n",
            "Epoch 13/100 [Train]: 100% 8/8 [00:00<00:00, 44.16it/s]\n",
            "Epoch 13/100 [Valid]: 100% 2/2 [00:00<00:00, 70.20it/s]\n",
            "Epoch [13/100] | Train Loss: 0.245771 | Validation Loss: 0.207393\n",
            "Epoch 14/100 [Train]: 100% 8/8 [00:00<00:00, 45.24it/s]\n",
            "Epoch 14/100 [Valid]: 100% 2/2 [00:00<00:00, 73.42it/s]\n",
            "Epoch [14/100] | Train Loss: 0.244729 | Validation Loss: 0.206506\n",
            "Epoch 15/100 [Train]: 100% 8/8 [00:00<00:00, 44.89it/s]\n",
            "Epoch 15/100 [Valid]: 100% 2/2 [00:00<00:00, 74.72it/s]\n",
            "Epoch [15/100] | Train Loss: 0.242426 | Validation Loss: 0.208122\n",
            "Epoch 16/100 [Train]: 100% 8/8 [00:00<00:00, 43.66it/s]\n",
            "Epoch 16/100 [Valid]: 100% 2/2 [00:00<00:00, 76.38it/s]\n",
            "Epoch [16/100] | Train Loss: 0.243383 | Validation Loss: 0.210802\n",
            "Epoch 17/100 [Train]: 100% 8/8 [00:00<00:00, 44.32it/s]\n",
            "Epoch 17/100 [Valid]: 100% 2/2 [00:00<00:00, 74.81it/s]\n",
            "Epoch [17/100] | Train Loss: 0.242694 | Validation Loss: 0.208918\n",
            "Epoch 18/100 [Train]: 100% 8/8 [00:00<00:00, 44.74it/s]\n",
            "Epoch 18/100 [Valid]: 100% 2/2 [00:00<00:00, 73.12it/s]\n",
            "Epoch [18/100] | Train Loss: 0.243548 | Validation Loss: 0.215109\n",
            "Epoch 19/100 [Train]: 100% 8/8 [00:00<00:00, 38.99it/s]\n",
            "Epoch 19/100 [Valid]: 100% 2/2 [00:00<00:00, 78.02it/s]\n",
            "Epoch [19/100] | Train Loss: 0.242515 | Validation Loss: 0.223670\n",
            "Epoch 20/100 [Train]: 100% 8/8 [00:00<00:00, 45.08it/s]\n",
            "Epoch 20/100 [Valid]: 100% 2/2 [00:00<00:00, 78.07it/s]\n",
            "Epoch [20/100] | Train Loss: 0.244431 | Validation Loss: 0.222460\n",
            "Epoch 21/100 [Train]: 100% 8/8 [00:00<00:00, 45.14it/s]\n",
            "Epoch 21/100 [Valid]: 100% 2/2 [00:00<00:00, 77.66it/s]\n",
            "Epoch [21/100] | Train Loss: 0.245102 | Validation Loss: 0.206558\n",
            "Epoch 22/100 [Train]: 100% 8/8 [00:00<00:00, 45.09it/s]\n",
            "Epoch 22/100 [Valid]: 100% 2/2 [00:00<00:00, 77.99it/s]\n",
            "Epoch [22/100] | Train Loss: 0.243092 | Validation Loss: 0.206584\n",
            "Epoch 23/100 [Train]: 100% 8/8 [00:00<00:00, 44.71it/s]\n",
            "Epoch 23/100 [Valid]: 100% 2/2 [00:00<00:00, 72.86it/s]\n",
            "Epoch [23/100] | Train Loss: 0.243170 | Validation Loss: 0.211526\n",
            "Epoch 24/100 [Train]: 100% 8/8 [00:00<00:00, 44.89it/s]\n",
            "Epoch 24/100 [Valid]: 100% 2/2 [00:00<00:00, 75.55it/s]\n",
            "Epoch [24/100] | Train Loss: 0.242578 | Validation Loss: 0.210839\n",
            "Epoch 25/100 [Train]: 100% 8/8 [00:00<00:00, 44.97it/s]\n",
            "Epoch 25/100 [Valid]: 100% 2/2 [00:00<00:00, 76.89it/s]\n",
            "Epoch [25/100] | Train Loss: 0.237878 | Validation Loss: 0.210074\n",
            "Epoch 26/100 [Train]: 100% 8/8 [00:00<00:00, 44.97it/s]\n",
            "Epoch 26/100 [Valid]: 100% 2/2 [00:00<00:00, 77.71it/s]\n",
            "Epoch [26/100] | Train Loss: 0.232718 | Validation Loss: 0.218818\n",
            "Epoch 27/100 [Train]: 100% 8/8 [00:00<00:00, 45.24it/s]\n",
            "Epoch 27/100 [Valid]: 100% 2/2 [00:00<00:00, 76.01it/s]\n",
            "Epoch [27/100] | Train Loss: 0.227765 | Validation Loss: 0.215888\n",
            "Epoch 28/100 [Train]: 100% 8/8 [00:00<00:00, 44.49it/s]\n",
            "Epoch 28/100 [Valid]: 100% 2/2 [00:00<00:00, 73.49it/s]\n",
            "Epoch [28/100] | Train Loss: 0.223231 | Validation Loss: 0.246875\n",
            "Epoch 29/100 [Train]: 100% 8/8 [00:00<00:00, 45.08it/s]\n",
            "Epoch 29/100 [Valid]: 100% 2/2 [00:00<00:00, 77.47it/s]\n",
            "Epoch [29/100] | Train Loss: 0.220913 | Validation Loss: 0.235044\n",
            "Epoch 30/100 [Train]: 100% 8/8 [00:00<00:00, 45.17it/s]\n",
            "Epoch 30/100 [Valid]: 100% 2/2 [00:00<00:00, 77.24it/s]\n",
            "Epoch [30/100] | Train Loss: 0.221161 | Validation Loss: 0.210256\n",
            "Epoch 31/100 [Train]: 100% 8/8 [00:00<00:00, 45.06it/s]\n",
            "Epoch 31/100 [Valid]: 100% 2/2 [00:00<00:00, 74.88it/s]\n",
            "Epoch [31/100] | Train Loss: 0.222053 | Validation Loss: 0.216139\n",
            "Epoch 32/100 [Train]: 100% 8/8 [00:00<00:00, 45.48it/s]\n",
            "Epoch 32/100 [Valid]: 100% 2/2 [00:00<00:00, 76.69it/s]\n",
            "Epoch [32/100] | Train Loss: 0.215965 | Validation Loss: 0.232141\n",
            "Epoch 33/100 [Train]: 100% 8/8 [00:00<00:00, 44.62it/s]\n",
            "Epoch 33/100 [Valid]: 100% 2/2 [00:00<00:00, 72.32it/s]\n",
            "Epoch [33/100] | Train Loss: 0.216985 | Validation Loss: 0.219683\n",
            "Epoch 34/100 [Train]: 100% 8/8 [00:00<00:00, 44.72it/s]\n",
            "Epoch 34/100 [Valid]: 100% 2/2 [00:00<00:00, 77.11it/s]\n",
            "Epoch [34/100] | Train Loss: 0.214753 | Validation Loss: 0.240517\n",
            "Epoch 35/100 [Train]: 100% 8/8 [00:00<00:00, 44.74it/s]\n",
            "Epoch 35/100 [Valid]: 100% 2/2 [00:00<00:00, 77.88it/s]\n",
            "Epoch [35/100] | Train Loss: 0.214258 | Validation Loss: 0.255711\n",
            "Epoch 36/100 [Train]: 100% 8/8 [00:00<00:00, 44.98it/s]\n",
            "Epoch 36/100 [Valid]: 100% 2/2 [00:00<00:00, 77.03it/s]\n",
            "Epoch [36/100] | Train Loss: 0.214292 | Validation Loss: 0.229426\n",
            "Epoch 37/100 [Train]: 100% 8/8 [00:00<00:00, 45.29it/s]\n",
            "Epoch 37/100 [Valid]: 100% 2/2 [00:00<00:00, 76.88it/s]\n",
            "Epoch [37/100] | Train Loss: 0.214139 | Validation Loss: 0.219777\n",
            "Epoch 38/100 [Train]: 100% 8/8 [00:00<00:00, 45.12it/s]\n",
            "Epoch 38/100 [Valid]: 100% 2/2 [00:00<00:00, 73.52it/s]\n",
            "Epoch [38/100] | Train Loss: 0.215188 | Validation Loss: 0.219038\n",
            "Epoch 39/100 [Train]: 100% 8/8 [00:00<00:00, 44.62it/s]\n",
            "Epoch 39/100 [Valid]: 100% 2/2 [00:00<00:00, 75.67it/s]\n",
            "Epoch [39/100] | Train Loss: 0.215622 | Validation Loss: 0.222630\n",
            "Epoch 40/100 [Train]: 100% 8/8 [00:00<00:00, 44.79it/s]\n",
            "Epoch 40/100 [Valid]: 100% 2/2 [00:00<00:00, 76.30it/s]\n",
            "Epoch [40/100] | Train Loss: 0.215791 | Validation Loss: 0.228439\n",
            "Epoch 41/100 [Train]: 100% 8/8 [00:00<00:00, 44.96it/s]\n",
            "Epoch 41/100 [Valid]: 100% 2/2 [00:00<00:00, 77.81it/s]\n",
            "Epoch [41/100] | Train Loss: 0.214301 | Validation Loss: 0.220298\n",
            "Epoch 42/100 [Train]: 100% 8/8 [00:00<00:00, 44.65it/s]\n",
            "Epoch 42/100 [Valid]: 100% 2/2 [00:00<00:00, 77.08it/s]\n",
            "Epoch [42/100] | Train Loss: 0.214710 | Validation Loss: 0.226856\n",
            "Epoch 43/100 [Train]: 100% 8/8 [00:00<00:00, 45.04it/s]\n",
            "Epoch 43/100 [Valid]: 100% 2/2 [00:00<00:00, 75.31it/s]\n",
            "Epoch [43/100] | Train Loss: 0.213809 | Validation Loss: 0.218974\n",
            "Epoch 44/100 [Train]: 100% 8/8 [00:00<00:00, 44.86it/s]\n",
            "Epoch 44/100 [Valid]: 100% 2/2 [00:00<00:00, 75.28it/s]\n",
            "Epoch [44/100] | Train Loss: 0.213410 | Validation Loss: 0.222600\n",
            "Epoch 45/100 [Train]: 100% 8/8 [00:00<00:00, 45.02it/s]\n",
            "Epoch 45/100 [Valid]: 100% 2/2 [00:00<00:00, 74.70it/s]\n",
            "Epoch [45/100] | Train Loss: 0.214894 | Validation Loss: 0.222993\n",
            "Epoch 46/100 [Train]: 100% 8/8 [00:00<00:00, 44.76it/s]\n",
            "Epoch 46/100 [Valid]: 100% 2/2 [00:00<00:00, 76.92it/s]\n",
            "Epoch [46/100] | Train Loss: 0.212926 | Validation Loss: 0.232039\n",
            "Epoch 47/100 [Train]: 100% 8/8 [00:00<00:00, 44.68it/s]\n",
            "Epoch 47/100 [Valid]: 100% 2/2 [00:00<00:00, 77.35it/s]\n",
            "Epoch [47/100] | Train Loss: 0.214418 | Validation Loss: 0.224698\n",
            "Epoch 48/100 [Train]: 100% 8/8 [00:00<00:00, 44.73it/s]\n",
            "Epoch 48/100 [Valid]: 100% 2/2 [00:00<00:00, 75.97it/s]\n",
            "Epoch [48/100] | Train Loss: 0.212121 | Validation Loss: 0.235244\n",
            "Epoch 49/100 [Train]: 100% 8/8 [00:00<00:00, 44.87it/s]\n",
            "Epoch 49/100 [Valid]: 100% 2/2 [00:00<00:00, 76.10it/s]\n",
            "Epoch [49/100] | Train Loss: 0.212930 | Validation Loss: 0.229430\n",
            "Epoch 50/100 [Train]: 100% 8/8 [00:00<00:00, 45.03it/s]\n",
            "Epoch 50/100 [Valid]: 100% 2/2 [00:00<00:00, 76.01it/s]\n",
            "Epoch [50/100] | Train Loss: 0.212552 | Validation Loss: 0.229565\n",
            "Epoch 51/100 [Train]: 100% 8/8 [00:00<00:00, 44.87it/s]\n",
            "Epoch 51/100 [Valid]: 100% 2/2 [00:00<00:00, 75.23it/s]\n",
            "Epoch [51/100] | Train Loss: 0.212083 | Validation Loss: 0.231578\n",
            "Epoch 52/100 [Train]: 100% 8/8 [00:00<00:00, 44.63it/s]\n",
            "Epoch 52/100 [Valid]: 100% 2/2 [00:00<00:00, 75.64it/s]\n",
            "Epoch [52/100] | Train Loss: 0.211049 | Validation Loss: 0.233878\n",
            "Epoch 53/100 [Train]: 100% 8/8 [00:00<00:00, 44.76it/s]\n",
            "Epoch 53/100 [Valid]: 100% 2/2 [00:00<00:00, 74.99it/s]\n",
            "Epoch [53/100] | Train Loss: 0.211853 | Validation Loss: 0.239273\n",
            "Epoch 54/100 [Train]: 100% 8/8 [00:00<00:00, 44.62it/s]\n",
            "Epoch 54/100 [Valid]: 100% 2/2 [00:00<00:00, 77.86it/s]\n",
            "Epoch [54/100] | Train Loss: 0.213455 | Validation Loss: 0.225270\n",
            "Epoch 55/100 [Train]: 100% 8/8 [00:00<00:00, 44.76it/s]\n",
            "Epoch 55/100 [Valid]: 100% 2/2 [00:00<00:00, 76.87it/s]\n",
            "Epoch [55/100] | Train Loss: 0.213294 | Validation Loss: 0.234343\n",
            "Epoch 56/100 [Train]: 100% 8/8 [00:00<00:00, 45.37it/s]\n",
            "Epoch 56/100 [Valid]: 100% 2/2 [00:00<00:00, 74.11it/s]\n",
            "Epoch [56/100] | Train Loss: 0.211814 | Validation Loss: 0.263045\n",
            "Epoch 57/100 [Train]: 100% 8/8 [00:00<00:00, 44.98it/s]\n",
            "Epoch 57/100 [Valid]: 100% 2/2 [00:00<00:00, 75.31it/s]\n",
            "Epoch [57/100] | Train Loss: 0.213176 | Validation Loss: 0.230628\n",
            "Epoch 58/100 [Train]: 100% 8/8 [00:00<00:00, 44.37it/s]\n",
            "Epoch 58/100 [Valid]: 100% 2/2 [00:00<00:00, 75.14it/s]\n",
            "Epoch [58/100] | Train Loss: 0.214945 | Validation Loss: 0.233658\n",
            "Epoch 59/100 [Train]: 100% 8/8 [00:00<00:00, 44.66it/s]\n",
            "Epoch 59/100 [Valid]: 100% 2/2 [00:00<00:00, 77.05it/s]\n",
            "Epoch [59/100] | Train Loss: 0.213610 | Validation Loss: 0.236761\n",
            "Epoch 60/100 [Train]: 100% 8/8 [00:00<00:00, 44.61it/s]\n",
            "Epoch 60/100 [Valid]: 100% 2/2 [00:00<00:00, 77.69it/s]\n",
            "Epoch [60/100] | Train Loss: 0.211888 | Validation Loss: 0.230726\n",
            "Epoch 61/100 [Train]: 100% 8/8 [00:00<00:00, 45.05it/s]\n",
            "Epoch 61/100 [Valid]: 100% 2/2 [00:00<00:00, 76.82it/s]\n",
            "Epoch [61/100] | Train Loss: 0.212329 | Validation Loss: 0.241836\n",
            "Epoch 62/100 [Train]: 100% 8/8 [00:00<00:00, 45.00it/s]\n",
            "Epoch 62/100 [Valid]: 100% 2/2 [00:00<00:00, 75.97it/s]\n",
            "Epoch [62/100] | Train Loss: 0.211271 | Validation Loss: 0.234122\n",
            "Epoch 63/100 [Train]: 100% 8/8 [00:00<00:00, 44.80it/s]\n",
            "Epoch 63/100 [Valid]: 100% 2/2 [00:00<00:00, 73.91it/s]\n",
            "Epoch [63/100] | Train Loss: 0.212609 | Validation Loss: 0.227365\n",
            "Epoch 64/100 [Train]: 100% 8/8 [00:00<00:00, 44.69it/s]\n",
            "Epoch 64/100 [Valid]: 100% 2/2 [00:00<00:00, 77.05it/s]\n",
            "Epoch [64/100] | Train Loss: 0.213271 | Validation Loss: 0.213365\n",
            "Epoch 65/100 [Train]: 100% 8/8 [00:00<00:00, 44.84it/s]\n",
            "Epoch 65/100 [Valid]: 100% 2/2 [00:00<00:00, 77.42it/s]\n",
            "Epoch [65/100] | Train Loss: 0.213487 | Validation Loss: 0.241679\n",
            "Epoch 66/100 [Train]: 100% 8/8 [00:00<00:00, 44.78it/s]\n",
            "Epoch 66/100 [Valid]: 100% 2/2 [00:00<00:00, 77.24it/s]\n",
            "Epoch [66/100] | Train Loss: 0.216119 | Validation Loss: 0.243933\n",
            "Epoch 67/100 [Train]: 100% 8/8 [00:00<00:00, 44.71it/s]\n",
            "Epoch 67/100 [Valid]: 100% 2/2 [00:00<00:00, 77.33it/s]\n",
            "Epoch [67/100] | Train Loss: 0.210824 | Validation Loss: 0.240512\n",
            "Epoch 68/100 [Train]: 100% 8/8 [00:00<00:00, 44.42it/s]\n",
            "Epoch 68/100 [Valid]: 100% 2/2 [00:00<00:00, 73.56it/s]\n",
            "Epoch [68/100] | Train Loss: 0.210022 | Validation Loss: 0.226622\n",
            "Epoch 69/100 [Train]: 100% 8/8 [00:00<00:00, 42.65it/s]\n",
            "Epoch 69/100 [Valid]: 100% 2/2 [00:00<00:00, 75.20it/s]\n",
            "Epoch [69/100] | Train Loss: 0.209315 | Validation Loss: 0.236859\n",
            "Epoch 70/100 [Train]: 100% 8/8 [00:00<00:00, 44.39it/s]\n",
            "Epoch 70/100 [Valid]: 100% 2/2 [00:00<00:00, 75.17it/s]\n",
            "Epoch [70/100] | Train Loss: 0.206869 | Validation Loss: 0.236258\n",
            "Epoch 71/100 [Train]: 100% 8/8 [00:00<00:00, 44.45it/s]\n",
            "Epoch 71/100 [Valid]: 100% 2/2 [00:00<00:00, 74.17it/s]\n",
            "Epoch [71/100] | Train Loss: 0.207477 | Validation Loss: 0.251184\n",
            "Epoch 72/100 [Train]: 100% 8/8 [00:00<00:00, 44.22it/s]\n",
            "Epoch 72/100 [Valid]: 100% 2/2 [00:00<00:00, 75.67it/s]\n",
            "Epoch [72/100] | Train Loss: 0.207166 | Validation Loss: 0.239930\n",
            "Epoch 73/100 [Train]: 100% 8/8 [00:00<00:00, 44.26it/s]\n",
            "Epoch 73/100 [Valid]: 100% 2/2 [00:00<00:00, 75.77it/s]\n",
            "Epoch [73/100] | Train Loss: 0.205926 | Validation Loss: 0.217623\n",
            "Epoch 74/100 [Train]: 100% 8/8 [00:00<00:00, 43.03it/s]\n",
            "Epoch 74/100 [Valid]: 100% 2/2 [00:00<00:00, 69.47it/s]\n",
            "Epoch [74/100] | Train Loss: 0.228609 | Validation Loss: 0.253269\n",
            "Epoch 75/100 [Train]: 100% 8/8 [00:00<00:00, 41.39it/s]\n",
            "Epoch 75/100 [Valid]: 100% 2/2 [00:00<00:00, 74.94it/s]\n",
            "Epoch [75/100] | Train Loss: 0.219607 | Validation Loss: 0.225985\n",
            "Epoch 76/100 [Train]: 100% 8/8 [00:00<00:00, 44.06it/s]\n",
            "Epoch 76/100 [Valid]: 100% 2/2 [00:00<00:00, 75.18it/s]\n",
            "Epoch [76/100] | Train Loss: 0.218178 | Validation Loss: 0.226900\n",
            "Epoch 77/100 [Train]: 100% 8/8 [00:00<00:00, 44.58it/s]\n",
            "Epoch 77/100 [Valid]: 100% 2/2 [00:00<00:00, 76.87it/s]\n",
            "Epoch [77/100] | Train Loss: 0.233181 | Validation Loss: 0.230113\n",
            "Epoch 78/100 [Train]: 100% 8/8 [00:00<00:00, 45.14it/s]\n",
            "Epoch 78/100 [Valid]: 100% 2/2 [00:00<00:00, 76.83it/s]\n",
            "Epoch [78/100] | Train Loss: 0.216604 | Validation Loss: 0.323127\n",
            "Epoch 79/100 [Train]: 100% 8/8 [00:00<00:00, 44.28it/s]\n",
            "Epoch 79/100 [Valid]: 100% 2/2 [00:00<00:00, 74.97it/s]\n",
            "Epoch [79/100] | Train Loss: 0.228528 | Validation Loss: 0.217988\n",
            "Epoch 80/100 [Train]: 100% 8/8 [00:00<00:00, 44.30it/s]\n",
            "Epoch 80/100 [Valid]: 100% 2/2 [00:00<00:00, 77.00it/s]\n",
            "Epoch [80/100] | Train Loss: 0.214913 | Validation Loss: 0.262651\n",
            "Epoch 81/100 [Train]: 100% 8/8 [00:00<00:00, 44.74it/s]\n",
            "Epoch 81/100 [Valid]: 100% 2/2 [00:00<00:00, 74.70it/s]\n",
            "Epoch [81/100] | Train Loss: 0.213414 | Validation Loss: 0.224678\n",
            "Epoch 82/100 [Train]: 100% 8/8 [00:00<00:00, 44.71it/s]\n",
            "Epoch 82/100 [Valid]: 100% 2/2 [00:00<00:00, 76.32it/s]\n",
            "Epoch [82/100] | Train Loss: 0.212202 | Validation Loss: 0.232969\n",
            "Epoch 83/100 [Train]: 100% 8/8 [00:00<00:00, 44.77it/s]\n",
            "Epoch 83/100 [Valid]: 100% 2/2 [00:00<00:00, 74.18it/s]\n",
            "Epoch [83/100] | Train Loss: 0.210628 | Validation Loss: 0.220114\n",
            "Epoch 84/100 [Train]: 100% 8/8 [00:00<00:00, 44.26it/s]\n",
            "Epoch 84/100 [Valid]: 100% 2/2 [00:00<00:00, 75.02it/s]\n",
            "Epoch [84/100] | Train Loss: 0.210443 | Validation Loss: 0.233909\n",
            "Epoch 85/100 [Train]: 100% 8/8 [00:00<00:00, 44.55it/s]\n",
            "Epoch 85/100 [Valid]: 100% 2/2 [00:00<00:00, 77.19it/s]\n",
            "Epoch [85/100] | Train Loss: 0.211819 | Validation Loss: 0.243264\n",
            "Epoch 86/100 [Train]: 100% 8/8 [00:00<00:00, 44.43it/s]\n",
            "Epoch 86/100 [Valid]: 100% 2/2 [00:00<00:00, 77.52it/s]\n",
            "Epoch [86/100] | Train Loss: 0.211215 | Validation Loss: 0.234750\n",
            "Epoch 87/100 [Train]: 100% 8/8 [00:00<00:00, 45.11it/s]\n",
            "Epoch 87/100 [Valid]: 100% 2/2 [00:00<00:00, 76.08it/s]\n",
            "Epoch [87/100] | Train Loss: 0.208740 | Validation Loss: 0.247374\n",
            "Epoch 88/100 [Train]: 100% 8/8 [00:00<00:00, 44.63it/s]\n",
            "Epoch 88/100 [Valid]: 100% 2/2 [00:00<00:00, 74.99it/s]\n",
            "Epoch [88/100] | Train Loss: 0.211167 | Validation Loss: 0.221960\n",
            "Epoch 89/100 [Train]: 100% 8/8 [00:00<00:00, 44.58it/s]\n",
            "Epoch 89/100 [Valid]: 100% 2/2 [00:00<00:00, 74.27it/s]\n",
            "Epoch [89/100] | Train Loss: 0.207327 | Validation Loss: 0.231190\n",
            "Epoch 90/100 [Train]: 100% 8/8 [00:00<00:00, 44.48it/s]\n",
            "Epoch 90/100 [Valid]: 100% 2/2 [00:00<00:00, 75.58it/s]\n",
            "Epoch [90/100] | Train Loss: 0.205376 | Validation Loss: 0.217691\n",
            "Epoch 91/100 [Train]: 100% 8/8 [00:00<00:00, 43.89it/s]\n",
            "Epoch 91/100 [Valid]: 100% 2/2 [00:00<00:00, 77.22it/s]\n",
            "Epoch [91/100] | Train Loss: 0.201279 | Validation Loss: 0.230039\n",
            "Epoch 92/100 [Train]: 100% 8/8 [00:00<00:00, 44.62it/s]\n",
            "Epoch 92/100 [Valid]: 100% 2/2 [00:00<00:00, 76.69it/s]\n",
            "Epoch [92/100] | Train Loss: 0.204121 | Validation Loss: 0.268127\n",
            "Epoch 93/100 [Train]: 100% 8/8 [00:00<00:00, 44.95it/s]\n",
            "Epoch 93/100 [Valid]: 100% 2/2 [00:00<00:00, 75.61it/s]\n",
            "Epoch [93/100] | Train Loss: 0.205370 | Validation Loss: 0.254358\n",
            "Epoch 94/100 [Train]: 100% 8/8 [00:00<00:00, 44.30it/s]\n",
            "Epoch 94/100 [Valid]: 100% 2/2 [00:00<00:00, 75.49it/s]\n",
            "Epoch [94/100] | Train Loss: 0.194043 | Validation Loss: 0.242224\n",
            "Epoch 95/100 [Train]: 100% 8/8 [00:00<00:00, 44.32it/s]\n",
            "Epoch 95/100 [Valid]: 100% 2/2 [00:00<00:00, 76.32it/s]\n",
            "Epoch [95/100] | Train Loss: 0.203013 | Validation Loss: 0.227434\n",
            "Epoch 96/100 [Train]: 100% 8/8 [00:00<00:00, 44.47it/s]\n",
            "Epoch 96/100 [Valid]: 100% 2/2 [00:00<00:00, 77.46it/s]\n",
            "Epoch [96/100] | Train Loss: 0.206366 | Validation Loss: 0.256224\n",
            "Epoch 97/100 [Train]: 100% 8/8 [00:00<00:00, 44.67it/s]\n",
            "Epoch 97/100 [Valid]: 100% 2/2 [00:00<00:00, 76.27it/s]\n",
            "Epoch [97/100] | Train Loss: 0.217222 | Validation Loss: 0.207599\n",
            "Epoch 98/100 [Train]: 100% 8/8 [00:00<00:00, 44.42it/s]\n",
            "Epoch 98/100 [Valid]: 100% 2/2 [00:00<00:00, 75.56it/s]\n",
            "Epoch [98/100] | Train Loss: 0.211411 | Validation Loss: 0.213800\n",
            "Epoch 99/100 [Train]: 100% 8/8 [00:00<00:00, 44.15it/s]\n",
            "Epoch 99/100 [Valid]: 100% 2/2 [00:00<00:00, 76.64it/s]\n",
            "Epoch [99/100] | Train Loss: 0.202513 | Validation Loss: 0.239283\n",
            "Epoch 100/100 [Train]: 100% 8/8 [00:00<00:00, 44.51it/s]\n",
            "Epoch 100/100 [Valid]: 100% 2/2 [00:00<00:00, 77.01it/s]\n",
            "Epoch [100/100] | Train Loss: 0.192473 | Validation Loss: 0.244370\n",
            "\n",
            "--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\n",
            "‚úÖ Model cu·ªëi c√πng ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i epoch c√≥ Validation Loss = 0.205983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "# evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES, LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS,\n",
        "    ATTENTION_NUM_HEADS, TRAINED_MODEL_PATH\n",
        ")\n",
        "from model import FinalModel\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "def run_evaluation():\n",
        "    \"\"\"H√†m ch√≠nh ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh ƒëa nhi·ªám tr√™n t·∫≠p test.\"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu\n",
        "    _, test_loader, target_scaler, volatility_scaler = get_data_loaders()\n",
        "    if not test_loader:\n",
        "        print(\"D·ª´ng ch∆∞∆°ng tr√¨nh v√¨ kh√¥ng t·∫£i ƒë∆∞·ª£c d·ªØ li·ªáu.\")\n",
        "        return\n",
        "\n",
        "    # 2. T·∫£i l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "    print(f\"ƒêang t·∫£i m√¥ h√¨nh t·ª´: {TRAINED_MODEL_PATH}\")\n",
        "    model = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=LSTM_HIDDEN_UNITS, lstm_num_layers=LSTM_NUM_LAYERS,\n",
        "        num_heads=ATTENTION_NUM_HEADS\n",
        "    )\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n",
        "        model.to(device)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model t·∫°i '{TRAINED_MODEL_PATH}'. Vui l√≤ng ch·∫°y train.py tr∆∞·ªõc.\")\n",
        "        return\n",
        "\n",
        "    # 3. ƒê√°nh gi√° tr√™n t·∫≠p Test\n",
        "    model.eval()\n",
        "    all_price_preds, all_trend_preds, all_vol_preds = [], [], []\n",
        "    all_price_lbls, all_trend_lbls, all_vol_lbls = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, (price_lbl, trend_lbl, vol_lbl) in test_loader:\n",
        "            features = features.to(device)\n",
        "\n",
        "            price_pred, trend_pred, vol_pred = model(features)\n",
        "\n",
        "            # L∆∞u l·∫°i k·∫øt qu·∫£ d·ª± ƒëo√°n v√† nh√£n th·ª±c t·∫ø\n",
        "            all_price_preds.extend(price_pred.cpu().numpy())\n",
        "            all_trend_preds.extend(trend_pred.cpu().numpy())\n",
        "\n",
        "            all_price_lbls.extend(price_lbl.numpy())\n",
        "            all_trend_lbls.extend(trend_lbl.numpy())\n",
        "\n",
        "    # 4. X·ª≠ l√Ω v√† Gi·∫£i chu·∫©n h√≥a\n",
        "    # --- X·ª≠ l√Ω cho D·ª± ƒëo√°n Gi√° ---\n",
        "    price_preds = np.array(all_price_preds).reshape(-1, 1)\n",
        "    price_actuals = np.array(all_price_lbls).reshape(-1, 1)\n",
        "    original_price_preds = target_scaler.inverse_transform(price_preds)\n",
        "    original_price_actuals = target_scaler.inverse_transform(price_actuals)\n",
        "\n",
        "    # --- X·ª≠ l√Ω cho D·ª± ƒëo√°n Xu h∆∞·ªõng ---\n",
        "    # Chuy·ªÉn ƒë·ªïi output c·ªßa model (logits) th√†nh x√°c su·∫•t r·ªìi th√†nh nh√£n (0 ho·∫∑c 1)\n",
        "    trend_probs = torch.sigmoid(torch.tensor(all_trend_preds)).numpy()\n",
        "    trend_preds_labels = (trend_probs > 0.5).astype(int)\n",
        "    trend_actuals = np.array(all_trend_lbls)\n",
        "\n",
        "    # 5. T√≠nh to√°n c√°c ch·ªâ s·ªë v√† in k·∫øt qu·∫£\n",
        "    print(\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model Cu·ªëi c√πng) ---\")\n",
        "\n",
        "    # --- Nhi·ªám v·ª• 1: D·ª± ƒëo√°n Gi√° ---\n",
        "    mae = np.mean(np.abs(original_price_preds - original_price_actuals))\n",
        "    print(f\"üéØ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # --- Nhi·ªám v·ª• 2: D·ª± ƒëo√°n Xu h∆∞·ªõng ---\n",
        "    accuracy = accuracy_score(trend_actuals, trend_preds_labels)\n",
        "    print(f\"üéØ [Xu h∆∞·ªõng] ƒê·ªô ch√≠nh x√°c (Accuracy): {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # 6. V·∫Ω bi·ªÉu ƒë·ªì\n",
        "    # --- Bi·ªÉu ƒë·ªì 1: So s√°nh Gi√° ---\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_price_actuals, label='Gi√° tr·ªã Th·ª±c t·∫ø', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_price_preds, label='Gi√° tr·ªã D·ª± ƒëo√°n', color='red', linestyle='--')\n",
        "    plt.title('So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n (Model Cu·ªëi c√πng)')\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('final_prediction_vs_actual.png')\n",
        "    print(\"\\n‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh gi√° v√†o file 'final_prediction_vs_actual.png'\")\n",
        "\n",
        "    # --- Bi·ªÉu ƒë·ªì 2: Ma tr·∫≠n nh·∫ßm l·∫´n cho Xu h∆∞·ªõng ---\n",
        "    cm = confusion_matrix(trend_actuals, trend_preds_labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Gi·∫£m', 'TƒÉng'])\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "    ax.set_title('Ma tr·∫≠n Nh·∫ßm l·∫´n - D·ª± ƒëo√°n Xu h∆∞·ªõng')\n",
        "    plt.savefig('final_confusion_matrix.png')\n",
        "    print(\"‚úÖ ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n v√†o file 'final_confusion_matrix.png'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfytOWoegJj0",
        "outputId": "ee085cc7-4c22-4f4a-f95d-8300c2a1f05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eL6y7NUgK6x",
        "outputId": "452fddc9-34e6-4383-9f37-eed38b13dbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "ƒêang t·∫£i m√¥ h√¨nh t·ª´: final_optimized_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° tr√™n T·∫≠p Test (Model Cu·ªëi c√πng) ---\n",
            "üéØ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 20.7005 (ƒëi·ªÉm VN-Index)\n",
            "üéØ [Xu h∆∞·ªõng] ƒê·ªô ch√≠nh x√°c (Accuracy): 61.29%\n",
            "\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh gi√° v√†o file 'final_prediction_vs_actual.png'\n",
            "‚úÖ ƒê√£ l∆∞u ma tr·∫≠n nh·∫ßm l·∫´n v√†o file 'final_confusion_matrix.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ESEMBLE***"
      ],
      "metadata": {
        "id": "ODeSuOt3hM0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from config import BATCH_SIZE, LOOKBACK_WINDOW, NUM_BASE_FEATURES, NUM_SCALES\n",
        "\n",
        "# --- KI·∫æN TR√öC 1: MS-LSTM + ATTENTION ---\n",
        "class MSLSTMAttention(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(MSLSTMAttention, self).__init__()\n",
        "        self.num_scales = num_scales\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(input_size=input_feature_size, hidden_size=lstm_hidden_units,\n",
        "                    num_layers=lstm_num_layers, batch_first=True, dropout=0.2 if lstm_num_layers > 1 else 0)\n",
        "            for _ in range(num_scales)\n",
        "        ])\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=lstm_hidden_units * num_scales,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(in_features=lstm_hidden_units * num_scales, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_scales):\n",
        "            branch_input = x[:, :, i*self.input_feature_size : (i+1)*self.input_feature_size]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "        last_time_step_output = attention_output[:, -1, :]\n",
        "        final_prediction = self.fc(last_time_step_output)\n",
        "        return final_prediction.squeeze()\n",
        "\n",
        "# --- KI·∫æN TR√öC 2: FINAL MODEL (HIERARCHICAL + MULTI-TASK) ---\n",
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, input_feature_size, num_scales, lstm_hidden_units, lstm_num_layers, num_heads):\n",
        "        super(FinalModel, self).__init__()\n",
        "        self.input_feature_size = input_feature_size\n",
        "        self.num_scales = num_scales\n",
        "\n",
        "        self.lstm_branches = nn.ModuleList([\n",
        "            nn.LSTM(input_size=self.input_feature_size, hidden_size=lstm_hidden_units,\n",
        "                    num_layers=lstm_num_layers, batch_first=True, dropout=0.2 if lstm_num_layers > 1 else 0)\n",
        "            for _ in range(self.num_scales)\n",
        "        ])\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=lstm_hidden_units * self.num_scales,\n",
        "                                             num_heads=num_heads, batch_first=True)\n",
        "\n",
        "        shared_feature_dim = lstm_hidden_units * self.num_scales\n",
        "        self.intermediate_layer = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim, shared_feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.price_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "        self.trend_head = nn.Sequential(\n",
        "            nn.Linear(shared_feature_dim // 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        self.volatility_head = nn.Linear(shared_feature_dim // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_scales):\n",
        "            branch_input = x[:, :, i*self.input_feature_size : (i+1)*self.input_feature_size]\n",
        "            output, _ = self.lstm_branches[i](branch_input)\n",
        "            branch_outputs.append(output)\n",
        "\n",
        "        concatenated_output = torch.cat(branch_outputs, dim=2)\n",
        "        attention_output, _ = self.attention(concatenated_output, concatenated_output, concatenated_output)\n",
        "        shared_features = self.intermediate_layer(attention_output[:, -1, :])\n",
        "\n",
        "        price_prediction = self.price_head(shared_features)\n",
        "        trend_prediction = self.trend_head(shared_features)\n",
        "        volatility_prediction = self.volatility_head(shared_features)\n",
        "\n",
        "        return price_prediction.squeeze(), trend_prediction.squeeze(), volatility_prediction.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf5BZL1chQAx",
        "outputId": "cbca5ac3-f36e-4d24-b419-b4a8c71bcc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model.py"
      ],
      "metadata": {
        "id": "LF6HS4kbhSFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ensemble_evaluate.py\n",
        "# ensemble_evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import c·∫£ hai l·ªõp ki·∫øn tr√∫c\n",
        "from model import MSLSTMAttention, FinalModel\n",
        "from dataset import get_data_loaders\n",
        "from config import NUM_BASE_FEATURES, NUM_SCALES\n",
        "\n",
        "def run_ensemble_evaluation():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu\n",
        "    # Ch√∫ng ta c·∫ßn target_scaler ƒë·ªÉ gi·∫£i chu·∫©n h√≥a\n",
        "    _, test_loader, target_scaler, _ = get_data_loaders()\n",
        "    if not test_loader: return\n",
        "\n",
        "    # 2. ƒê·ªãnh nghƒ©a v√† t·∫£i c√°c model\n",
        "    # (Gi·ªØ nguy√™n ph·∫ßn t·∫£i model)\n",
        "    model_A_params = {'lstm_hidden_units': 128, 'lstm_num_layers': 2, 'attention_num_heads': 4}\n",
        "    model_A_path = 'mslstm_attention_tuned.pth'\n",
        "    model_A = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=model_A_params['lstm_hidden_units'],\n",
        "        lstm_num_layers=model_A_params['lstm_num_layers'],\n",
        "        num_heads=model_A_params['attention_num_heads']\n",
        "    )\n",
        "\n",
        "    model_B_params = {'lstm_hidden_units': 192, 'lstm_num_layers': 1, 'attention_num_heads': 8}\n",
        "    model_B_path = 'final_optimized_model.pth'\n",
        "    model_B = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=model_B_params['lstm_hidden_units'],\n",
        "        lstm_num_layers=model_B_params['lstm_num_layers'],\n",
        "        num_heads=model_B_params['attention_num_heads']\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"ƒêang t·∫£i Model A t·ª´: {model_A_path}\")\n",
        "        model_A.load_state_dict(torch.load(model_A_path, map_location=device))\n",
        "        model_A.to(device)\n",
        "        model_A.eval()\n",
        "\n",
        "        print(f\"ƒêang t·∫£i Model B t·ª´: {model_B_path}\")\n",
        "        model_B.load_state_dict(torch.load(model_B_path, map_location=device))\n",
        "        model_B.to(device)\n",
        "        model_B.eval()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model. H√£y ƒë·∫£m b·∫£o c√°c file sau t·ªìn t·∫°i: {e.filename}\")\n",
        "        return\n",
        "\n",
        "    # 3. L·∫•y d·ª± ƒëo√°n v√† th·ª±c hi·ªán ensemble\n",
        "    original_preds_ensemble = []\n",
        "    original_actuals_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, (price_lbl, _, _) in test_loader:\n",
        "            features = features.to(device)\n",
        "\n",
        "            # --- LOGIC S·ª¨A L·ªñI ---\n",
        "            # a. L·∫•y d·ª± ƒëo√°n ƒë√£ chu·∫©n h√≥a t·ª´ m·ªói model\n",
        "            scaled_pred_A = model_A(features)\n",
        "            scaled_pred_B_price, _, _ = model_B(features)\n",
        "\n",
        "            # b. Gi·∫£i chu·∫©n h√≥a t·ª´ng d·ª± ƒëo√°n m·ªôt\n",
        "            original_pred_A = target_scaler.inverse_transform(scaled_pred_A.cpu().numpy().reshape(-1, 1))\n",
        "            original_pred_B = target_scaler.inverse_transform(scaled_pred_B_price.cpu().numpy().reshape(-1, 1))\n",
        "\n",
        "            # c. L·∫•y trung b√¨nh c√°c d·ª± ƒëo√°n ƒë√£ ƒë∆∞·ª£c gi·∫£i chu·∫©n h√≥a\n",
        "            avg_pred_original = (original_pred_A + original_pred_B) / 2.0\n",
        "\n",
        "            # d. Gi·∫£i chu·∫©n h√≥a gi√° tr·ªã th·ª±c ƒë·ªÉ so s√°nh\n",
        "            original_actuals = target_scaler.inverse_transform(price_lbl.numpy().reshape(-1, 1))\n",
        "\n",
        "            original_preds_ensemble.extend(avg_pred_original.flatten())\n",
        "            original_actuals_list.extend(original_actuals.flatten())\n",
        "\n",
        "    # 4. T√≠nh to√°n sai s·ªë\n",
        "    mae = np.mean(np.abs(np.array(original_preds_ensemble) - np.array(original_actuals_list)))\n",
        "    print(\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° c·ªßa 'H·ªôi ƒë·ªìng Chuy√™n gia' (Ensemble) - ƒê√É S·ª¨A L·ªñI ---\")\n",
        "    print(f\"üèÜ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # 5. V·∫Ω bi·ªÉu ƒë·ªì\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_actuals_list, label='Gi√° tr·ªã Th·ª±c t·∫ø', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_preds_ensemble, label='D·ª± ƒëo√°n Ensemble', color='green', linestyle='--')\n",
        "    plt.title(\"So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n Ensemble (ƒê√£ s·ª≠a l·ªói)\")\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('final_ensemble_prediction_corrected.png')\n",
        "    print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh c·ªßa Ensemble v√†o file 'final_ensemble_prediction_corrected.png'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_ensemble_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE0wuKB3hXsa",
        "outputId": "bfb13b3d-bf97-465e-c669-52825da6cf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ensemble_evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ensemble_evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WirRtaehZd0",
        "outputId": "26d3b513-8f84-483f-a73e-fefdfcf3505d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "ƒêang t·∫£i Model A t·ª´: mslstm_attention_tuned.pth\n",
            "ƒêang t·∫£i Model B t·ª´: final_optimized_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° c·ªßa 'H·ªôi ƒë·ªìng Chuy√™n gia' (Ensemble) - ƒê√É S·ª¨A L·ªñI ---\n",
            "üèÜ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 33.4388 (ƒëi·ªÉm VN-Index)\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh c·ªßa Ensemble v√†o file 'final_ensemble_prediction_corrected.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_simple_model.py\n",
        "# train_simple_model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Import c√°c config ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u t·ª´ b∆∞·ªõc tr∆∞·ªõc\n",
        "from config import (\n",
        "    NUM_BASE_FEATURES, NUM_SCALES,\n",
        "    # D√πng c√°c tham s·ªë m·∫∑c ƒë·ªãnh nh∆∞ng hi·ªáu qu·∫£ cho model ƒë∆°n gi·∫£n h∆°n\n",
        "    LSTM_HIDDEN_UNITS, LSTM_NUM_LAYERS, ATTENTION_NUM_HEADS,\n",
        "    LEARNING_RATE, NUM_EPOCHS\n",
        ")\n",
        "# Import ki·∫øn tr√∫c model ƒë∆°n gi·∫£n\n",
        "from model import MSLSTMAttention\n",
        "# QUAN TR·ªåNG: Import h√†m get_data_loaders m·ªõi nh·∫•t ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng b·ªô\n",
        "from dataset import get_data_loaders\n",
        "\n",
        "# ƒê·∫∑t t√™n file ri√™ng cho model n√†y ƒë·ªÉ tr√°nh ghi ƒë√®\n",
        "SIMPLE_MODEL_PATH = 'mslstm_attention_synced.pth'\n",
        "\n",
        "def run_simple_training():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    # 1. T·∫£i d·ªØ li·ªáu b·∫±ng data loader m·ªõi nh·∫•t\n",
        "    # N√≥ s·∫Ω tr·∫£ v·ªÅ 3 lo·∫°i label, nh∆∞ng ch√∫ng ta ch·ªâ d√πng price_label\n",
        "    train_loader, test_loader, _, _ = get_data_loaders()\n",
        "    if not train_loader: return\n",
        "\n",
        "    # 2. Kh·ªüi t·∫°o Model A\n",
        "    model = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES,\n",
        "        num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=128, # D√πng m·ªôt gi√° tr·ªã h·ª£p l√Ω\n",
        "        lstm_num_layers=2,\n",
        "        num_heads=4\n",
        "    ).to(device)\n",
        "\n",
        "    # Model n√†y ch·ªâ c√≥ 1 nhi·ªám v·ª• n√™n ch·ªâ c·∫ßn 1 h√†m loss\n",
        "    loss_function = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"\\n--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán l·∫°i Model A (ƒê·ªìng b·ªô) ---\")\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        # Ch·ªâ l·∫•y price_lbl t·ª´ b·ªô d·ªØ li·ªáu\n",
        "        for features, (price_lbl, _, _) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "            features, price_lbl = features.to(device), price_lbl.to(device)\n",
        "\n",
        "            outputs = model(features)\n",
        "            loss = loss_function(outputs, price_lbl)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for features, (price_lbl, _, _) in test_loader:\n",
        "                features, price_lbl = features.to(device), price_lbl.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = loss_function(outputs, price_lbl)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:02d}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), SIMPLE_MODEL_PATH)\n",
        "            print(f\"   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o '{SIMPLE_MODEL_PATH}'\")\n",
        "\n",
        "    print(f\"\\n--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_simple_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyWV3P3Ui61s",
        "outputId": "140d4825-e4ff-4c8c-c09f-85f8b5ec4f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_simple_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_simple_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAgJKrbYi_LE",
        "outputId": "57e410a9-ada4-4aa5-d4c6-089cc73eb4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "\n",
            "--- B·∫Øt ƒë·∫ßu Hu·∫•n luy·ªán l·∫°i Model A (ƒê·ªìng b·ªô) ---\n",
            "Epoch 1/100 [Train]: 100% 8/8 [00:00<00:00, 22.98it/s]\n",
            "Epoch [01/100] | Train Loss: 0.129956 | Val Loss: 0.006859\n",
            "   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o 'mslstm_attention_synced.pth'\n",
            "Epoch 2/100 [Train]: 100% 8/8 [00:00<00:00, 74.54it/s]\n",
            "Epoch [02/100] | Train Loss: 0.052934 | Val Loss: 0.036107\n",
            "Epoch 3/100 [Train]: 100% 8/8 [00:00<00:00, 90.49it/s]\n",
            "Epoch [03/100] | Train Loss: 0.048240 | Val Loss: 0.004623\n",
            "   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o 'mslstm_attention_synced.pth'\n",
            "Epoch 4/100 [Train]: 100% 8/8 [00:00<00:00, 97.95it/s]\n",
            "Epoch [04/100] | Train Loss: 0.042537 | Val Loss: 0.009207\n",
            "Epoch 5/100 [Train]: 100% 8/8 [00:00<00:00, 105.60it/s]\n",
            "Epoch [05/100] | Train Loss: 0.042286 | Val Loss: 0.008672\n",
            "Epoch 6/100 [Train]: 100% 8/8 [00:00<00:00, 106.50it/s]\n",
            "Epoch [06/100] | Train Loss: 0.044165 | Val Loss: 0.006925\n",
            "Epoch 7/100 [Train]: 100% 8/8 [00:00<00:00, 105.92it/s]\n",
            "Epoch [07/100] | Train Loss: 0.040683 | Val Loss: 0.006056\n",
            "Epoch 8/100 [Train]: 100% 8/8 [00:00<00:00, 100.66it/s]\n",
            "Epoch [08/100] | Train Loss: 0.041816 | Val Loss: 0.009013\n",
            "Epoch 9/100 [Train]: 100% 8/8 [00:00<00:00, 104.45it/s]\n",
            "Epoch [09/100] | Train Loss: 0.041760 | Val Loss: 0.005343\n",
            "Epoch 10/100 [Train]: 100% 8/8 [00:00<00:00, 104.93it/s]\n",
            "Epoch [10/100] | Train Loss: 0.040896 | Val Loss: 0.008591\n",
            "Epoch 11/100 [Train]: 100% 8/8 [00:00<00:00, 102.09it/s]\n",
            "Epoch [11/100] | Train Loss: 0.041962 | Val Loss: 0.004617\n",
            "   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o 'mslstm_attention_synced.pth'\n",
            "Epoch 12/100 [Train]: 100% 8/8 [00:00<00:00, 101.44it/s]\n",
            "Epoch [12/100] | Train Loss: 0.044101 | Val Loss: 0.004553\n",
            "   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o 'mslstm_attention_synced.pth'\n",
            "Epoch 13/100 [Train]: 100% 8/8 [00:00<00:00, 101.78it/s]\n",
            "Epoch [13/100] | Train Loss: 0.041556 | Val Loss: 0.011471\n",
            "Epoch 14/100 [Train]: 100% 8/8 [00:00<00:00, 100.46it/s]\n",
            "Epoch [14/100] | Train Loss: 0.041239 | Val Loss: 0.004853\n",
            "Epoch 15/100 [Train]: 100% 8/8 [00:00<00:00, 88.26it/s]\n",
            "Epoch [15/100] | Train Loss: 0.042577 | Val Loss: 0.006235\n",
            "Epoch 16/100 [Train]: 100% 8/8 [00:00<00:00, 99.78it/s]\n",
            "Epoch [16/100] | Train Loss: 0.041460 | Val Loss: 0.008877\n",
            "Epoch 17/100 [Train]: 100% 8/8 [00:00<00:00, 90.57it/s]\n",
            "Epoch [17/100] | Train Loss: 0.043362 | Val Loss: 0.010623\n",
            "Epoch 18/100 [Train]: 100% 8/8 [00:00<00:00, 86.46it/s]\n",
            "Epoch [18/100] | Train Loss: 0.045490 | Val Loss: 0.005402\n",
            "Epoch 19/100 [Train]: 100% 8/8 [00:00<00:00, 104.56it/s]\n",
            "Epoch [19/100] | Train Loss: 0.044429 | Val Loss: 0.009395\n",
            "Epoch 20/100 [Train]: 100% 8/8 [00:00<00:00, 102.48it/s]\n",
            "Epoch [20/100] | Train Loss: 0.042276 | Val Loss: 0.014765\n",
            "Epoch 21/100 [Train]: 100% 8/8 [00:00<00:00, 101.98it/s]\n",
            "Epoch [21/100] | Train Loss: 0.044779 | Val Loss: 0.005580\n",
            "Epoch 22/100 [Train]: 100% 8/8 [00:00<00:00, 88.67it/s]\n",
            "Epoch [22/100] | Train Loss: 0.043342 | Val Loss: 0.014929\n",
            "Epoch 23/100 [Train]: 100% 8/8 [00:00<00:00, 103.55it/s]\n",
            "Epoch [23/100] | Train Loss: 0.044079 | Val Loss: 0.009179\n",
            "Epoch 24/100 [Train]: 100% 8/8 [00:00<00:00, 105.34it/s]\n",
            "Epoch [24/100] | Train Loss: 0.042888 | Val Loss: 0.005198\n",
            "Epoch 25/100 [Train]: 100% 8/8 [00:00<00:00, 101.65it/s]\n",
            "Epoch [25/100] | Train Loss: 0.041613 | Val Loss: 0.011649\n",
            "Epoch 26/100 [Train]: 100% 8/8 [00:00<00:00, 86.77it/s]\n",
            "Epoch [26/100] | Train Loss: 0.042241 | Val Loss: 0.004830\n",
            "Epoch 27/100 [Train]: 100% 8/8 [00:00<00:00, 105.68it/s]\n",
            "Epoch [27/100] | Train Loss: 0.041283 | Val Loss: 0.013817\n",
            "Epoch 28/100 [Train]: 100% 8/8 [00:00<00:00, 104.47it/s]\n",
            "Epoch [28/100] | Train Loss: 0.041811 | Val Loss: 0.004576\n",
            "Epoch 29/100 [Train]: 100% 8/8 [00:00<00:00, 100.89it/s]\n",
            "Epoch [29/100] | Train Loss: 0.041891 | Val Loss: 0.006784\n",
            "Epoch 30/100 [Train]: 100% 8/8 [00:00<00:00, 105.28it/s]\n",
            "Epoch [30/100] | Train Loss: 0.041714 | Val Loss: 0.004666\n",
            "Epoch 31/100 [Train]: 100% 8/8 [00:00<00:00, 100.67it/s]\n",
            "Epoch [31/100] | Train Loss: 0.043268 | Val Loss: 0.009304\n",
            "Epoch 32/100 [Train]: 100% 8/8 [00:00<00:00, 84.39it/s]\n",
            "Epoch [32/100] | Train Loss: 0.041067 | Val Loss: 0.008683\n",
            "Epoch 33/100 [Train]: 100% 8/8 [00:00<00:00, 102.62it/s]\n",
            "Epoch [33/100] | Train Loss: 0.040816 | Val Loss: 0.005276\n",
            "Epoch 34/100 [Train]: 100% 8/8 [00:00<00:00, 101.30it/s]\n",
            "Epoch [34/100] | Train Loss: 0.041232 | Val Loss: 0.006540\n",
            "Epoch 35/100 [Train]: 100% 8/8 [00:00<00:00, 103.41it/s]\n",
            "Epoch [35/100] | Train Loss: 0.041454 | Val Loss: 0.004705\n",
            "Epoch 36/100 [Train]: 100% 8/8 [00:00<00:00, 104.55it/s]\n",
            "Epoch [36/100] | Train Loss: 0.041899 | Val Loss: 0.006537\n",
            "Epoch 37/100 [Train]: 100% 8/8 [00:00<00:00, 103.83it/s]\n",
            "Epoch [37/100] | Train Loss: 0.040943 | Val Loss: 0.004597\n",
            "Epoch 38/100 [Train]: 100% 8/8 [00:00<00:00, 91.27it/s]\n",
            "Epoch [38/100] | Train Loss: 0.042518 | Val Loss: 0.007330\n",
            "Epoch 39/100 [Train]: 100% 8/8 [00:00<00:00, 106.17it/s]\n",
            "Epoch [39/100] | Train Loss: 0.041009 | Val Loss: 0.014150\n",
            "Epoch 40/100 [Train]: 100% 8/8 [00:00<00:00, 99.57it/s]\n",
            "Epoch [40/100] | Train Loss: 0.042783 | Val Loss: 0.005225\n",
            "Epoch 41/100 [Train]: 100% 8/8 [00:00<00:00, 105.49it/s]\n",
            "Epoch [41/100] | Train Loss: 0.041531 | Val Loss: 0.004675\n",
            "Epoch 42/100 [Train]: 100% 8/8 [00:00<00:00, 100.70it/s]\n",
            "Epoch [42/100] | Train Loss: 0.044651 | Val Loss: 0.018330\n",
            "Epoch 43/100 [Train]: 100% 8/8 [00:00<00:00, 103.64it/s]\n",
            "Epoch [43/100] | Train Loss: 0.042186 | Val Loss: 0.005729\n",
            "Epoch 44/100 [Train]: 100% 8/8 [00:00<00:00, 102.05it/s]\n",
            "Epoch [44/100] | Train Loss: 0.042678 | Val Loss: 0.004545\n",
            "   -> Val loss c·∫£i thi·ªán. ƒê√£ l∆∞u model v√†o 'mslstm_attention_synced.pth'\n",
            "Epoch 45/100 [Train]: 100% 8/8 [00:00<00:00, 99.53it/s]\n",
            "Epoch [45/100] | Train Loss: 0.042538 | Val Loss: 0.008141\n",
            "Epoch 46/100 [Train]: 100% 8/8 [00:00<00:00, 105.53it/s]\n",
            "Epoch [46/100] | Train Loss: 0.041472 | Val Loss: 0.006746\n",
            "Epoch 47/100 [Train]: 100% 8/8 [00:00<00:00, 104.50it/s]\n",
            "Epoch [47/100] | Train Loss: 0.040572 | Val Loss: 0.007370\n",
            "Epoch 48/100 [Train]: 100% 8/8 [00:00<00:00, 100.46it/s]\n",
            "Epoch [48/100] | Train Loss: 0.044384 | Val Loss: 0.012712\n",
            "Epoch 49/100 [Train]: 100% 8/8 [00:00<00:00, 95.66it/s]\n",
            "Epoch [49/100] | Train Loss: 0.052182 | Val Loss: 0.004856\n",
            "Epoch 50/100 [Train]: 100% 8/8 [00:00<00:00, 99.52it/s]\n",
            "Epoch [50/100] | Train Loss: 0.047933 | Val Loss: 0.029635\n",
            "Epoch 51/100 [Train]: 100% 8/8 [00:00<00:00, 84.88it/s]\n",
            "Epoch [51/100] | Train Loss: 0.047439 | Val Loss: 0.008233\n",
            "Epoch 52/100 [Train]: 100% 8/8 [00:00<00:00, 101.42it/s]\n",
            "Epoch [52/100] | Train Loss: 0.044690 | Val Loss: 0.004896\n",
            "Epoch 53/100 [Train]: 100% 8/8 [00:00<00:00, 102.85it/s]\n",
            "Epoch [53/100] | Train Loss: 0.044930 | Val Loss: 0.007932\n",
            "Epoch 54/100 [Train]: 100% 8/8 [00:00<00:00, 86.43it/s]\n",
            "Epoch [54/100] | Train Loss: 0.042678 | Val Loss: 0.010740\n",
            "Epoch 55/100 [Train]: 100% 8/8 [00:00<00:00, 103.36it/s]\n",
            "Epoch [55/100] | Train Loss: 0.043151 | Val Loss: 0.005221\n",
            "Epoch 56/100 [Train]: 100% 8/8 [00:00<00:00, 105.13it/s]\n",
            "Epoch [56/100] | Train Loss: 0.041478 | Val Loss: 0.006942\n",
            "Epoch 57/100 [Train]: 100% 8/8 [00:00<00:00, 103.42it/s]\n",
            "Epoch [57/100] | Train Loss: 0.041232 | Val Loss: 0.011280\n",
            "Epoch 58/100 [Train]: 100% 8/8 [00:00<00:00, 104.63it/s]\n",
            "Epoch [58/100] | Train Loss: 0.042401 | Val Loss: 0.004545\n",
            "Epoch 59/100 [Train]: 100% 8/8 [00:00<00:00, 100.70it/s]\n",
            "Epoch [59/100] | Train Loss: 0.044133 | Val Loss: 0.004817\n",
            "Epoch 60/100 [Train]: 100% 8/8 [00:00<00:00, 100.97it/s]\n",
            "Epoch [60/100] | Train Loss: 0.043976 | Val Loss: 0.025427\n",
            "Epoch 61/100 [Train]: 100% 8/8 [00:00<00:00, 91.87it/s]\n",
            "Epoch [61/100] | Train Loss: 0.047862 | Val Loss: 0.004698\n",
            "Epoch 62/100 [Train]: 100% 8/8 [00:00<00:00, 107.65it/s]\n",
            "Epoch [62/100] | Train Loss: 0.045207 | Val Loss: 0.008652\n",
            "Epoch 63/100 [Train]: 100% 8/8 [00:00<00:00, 106.55it/s]\n",
            "Epoch [63/100] | Train Loss: 0.043332 | Val Loss: 0.008487\n",
            "Epoch 64/100 [Train]: 100% 8/8 [00:00<00:00, 101.70it/s]\n",
            "Epoch [64/100] | Train Loss: 0.041215 | Val Loss: 0.006567\n",
            "Epoch 65/100 [Train]: 100% 8/8 [00:00<00:00, 100.79it/s]\n",
            "Epoch [65/100] | Train Loss: 0.041097 | Val Loss: 0.004871\n",
            "Epoch 66/100 [Train]: 100% 8/8 [00:00<00:00, 100.85it/s]\n",
            "Epoch [66/100] | Train Loss: 0.041034 | Val Loss: 0.027935\n",
            "Epoch 67/100 [Train]: 100% 8/8 [00:00<00:00, 103.03it/s]\n",
            "Epoch [67/100] | Train Loss: 0.041965 | Val Loss: 0.008085\n",
            "Epoch 68/100 [Train]: 100% 8/8 [00:00<00:00, 101.18it/s]\n",
            "Epoch [68/100] | Train Loss: 0.040808 | Val Loss: 0.005103\n",
            "Epoch 69/100 [Train]: 100% 8/8 [00:00<00:00, 105.08it/s]\n",
            "Epoch [69/100] | Train Loss: 0.044529 | Val Loss: 0.005194\n",
            "Epoch 70/100 [Train]: 100% 8/8 [00:00<00:00, 102.78it/s]\n",
            "Epoch [70/100] | Train Loss: 0.040643 | Val Loss: 0.007237\n",
            "Epoch 71/100 [Train]: 100% 8/8 [00:00<00:00, 103.20it/s]\n",
            "Epoch [71/100] | Train Loss: 0.042133 | Val Loss: 0.004964\n",
            "Epoch 72/100 [Train]: 100% 8/8 [00:00<00:00, 94.30it/s]\n",
            "Epoch [72/100] | Train Loss: 0.041856 | Val Loss: 0.016589\n",
            "Epoch 73/100 [Train]: 100% 8/8 [00:00<00:00, 92.24it/s]\n",
            "Epoch [73/100] | Train Loss: 0.043958 | Val Loss: 0.005469\n",
            "Epoch 74/100 [Train]: 100% 8/8 [00:00<00:00, 83.67it/s]\n",
            "Epoch [74/100] | Train Loss: 0.040163 | Val Loss: 0.009204\n",
            "Epoch 75/100 [Train]: 100% 8/8 [00:00<00:00, 104.52it/s]\n",
            "Epoch [75/100] | Train Loss: 0.041735 | Val Loss: 0.007463\n",
            "Epoch 76/100 [Train]: 100% 8/8 [00:00<00:00, 101.99it/s]\n",
            "Epoch [76/100] | Train Loss: 0.044018 | Val Loss: 0.008478\n",
            "Epoch 77/100 [Train]: 100% 8/8 [00:00<00:00, 100.21it/s]\n",
            "Epoch [77/100] | Train Loss: 0.043827 | Val Loss: 0.004660\n",
            "Epoch 78/100 [Train]: 100% 8/8 [00:00<00:00, 90.34it/s]\n",
            "Epoch [78/100] | Train Loss: 0.042808 | Val Loss: 0.014313\n",
            "Epoch 79/100 [Train]: 100% 8/8 [00:00<00:00, 100.73it/s]\n",
            "Epoch [79/100] | Train Loss: 0.041628 | Val Loss: 0.004548\n",
            "Epoch 80/100 [Train]: 100% 8/8 [00:00<00:00, 102.49it/s]\n",
            "Epoch [80/100] | Train Loss: 0.043119 | Val Loss: 0.016646\n",
            "Epoch 81/100 [Train]: 100% 8/8 [00:00<00:00, 94.21it/s]\n",
            "Epoch [81/100] | Train Loss: 0.042224 | Val Loss: 0.004645\n",
            "Epoch 82/100 [Train]: 100% 8/8 [00:00<00:00, 103.65it/s]\n",
            "Epoch [82/100] | Train Loss: 0.043446 | Val Loss: 0.014580\n",
            "Epoch 83/100 [Train]: 100% 8/8 [00:00<00:00, 106.05it/s]\n",
            "Epoch [83/100] | Train Loss: 0.041768 | Val Loss: 0.004617\n",
            "Epoch 84/100 [Train]: 100% 8/8 [00:00<00:00, 89.63it/s]\n",
            "Epoch [84/100] | Train Loss: 0.040823 | Val Loss: 0.008835\n",
            "Epoch 85/100 [Train]: 100% 8/8 [00:00<00:00, 98.71it/s]\n",
            "Epoch [85/100] | Train Loss: 0.041773 | Val Loss: 0.004982\n",
            "Epoch 86/100 [Train]: 100% 8/8 [00:00<00:00, 105.74it/s]\n",
            "Epoch [86/100] | Train Loss: 0.039257 | Val Loss: 0.005403\n",
            "Epoch 87/100 [Train]: 100% 8/8 [00:00<00:00, 103.27it/s]\n",
            "Epoch [87/100] | Train Loss: 0.040236 | Val Loss: 0.007075\n",
            "Epoch 88/100 [Train]: 100% 8/8 [00:00<00:00, 84.28it/s]\n",
            "Epoch [88/100] | Train Loss: 0.042086 | Val Loss: 0.012946\n",
            "Epoch 89/100 [Train]: 100% 8/8 [00:00<00:00, 105.54it/s]\n",
            "Epoch [89/100] | Train Loss: 0.042238 | Val Loss: 0.005420\n",
            "Epoch 90/100 [Train]: 100% 8/8 [00:00<00:00, 106.97it/s]\n",
            "Epoch [90/100] | Train Loss: 0.040445 | Val Loss: 0.006858\n",
            "Epoch 91/100 [Train]: 100% 8/8 [00:00<00:00, 105.48it/s]\n",
            "Epoch [91/100] | Train Loss: 0.039477 | Val Loss: 0.006244\n",
            "Epoch 92/100 [Train]: 100% 8/8 [00:00<00:00, 99.11it/s]\n",
            "Epoch [92/100] | Train Loss: 0.038958 | Val Loss: 0.006498\n",
            "Epoch 93/100 [Train]: 100% 8/8 [00:00<00:00, 100.96it/s]\n",
            "Epoch [93/100] | Train Loss: 0.106577 | Val Loss: 0.007222\n",
            "Epoch 94/100 [Train]: 100% 8/8 [00:00<00:00, 103.37it/s]\n",
            "Epoch [94/100] | Train Loss: 0.062247 | Val Loss: 0.026890\n",
            "Epoch 95/100 [Train]: 100% 8/8 [00:00<00:00, 63.41it/s]\n",
            "Epoch [95/100] | Train Loss: 0.047924 | Val Loss: 0.007219\n",
            "Epoch 96/100 [Train]: 100% 8/8 [00:00<00:00, 63.51it/s]\n",
            "Epoch [96/100] | Train Loss: 0.040523 | Val Loss: 0.005784\n",
            "Epoch 97/100 [Train]: 100% 8/8 [00:00<00:00, 67.88it/s]\n",
            "Epoch [97/100] | Train Loss: 0.040992 | Val Loss: 0.004974\n",
            "Epoch 98/100 [Train]: 100% 8/8 [00:00<00:00, 64.22it/s]\n",
            "Epoch [98/100] | Train Loss: 0.042934 | Val Loss: 0.004547\n",
            "Epoch 99/100 [Train]: 100% 8/8 [00:00<00:00, 70.80it/s]\n",
            "Epoch [99/100] | Train Loss: 0.041591 | Val Loss: 0.004582\n",
            "Epoch 100/100 [Train]: 100% 8/8 [00:00<00:00, 72.31it/s]\n",
            "Epoch [100/100] | Train Loss: 0.041268 | Val Loss: 0.004608\n",
            "\n",
            "--- Hu·∫•n luy·ªán Ho√†n t·∫•t ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ensemble_evaluate.py\n",
        "# ensemble_evaluate.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import MSLSTMAttention, FinalModel\n",
        "from dataset import get_data_loaders\n",
        "from config import NUM_BASE_FEATURES, NUM_SCALES\n",
        "\n",
        "# C√°c ƒë∆∞·ªùng d·∫´n file model m·ªõi\n",
        "SYNCED_MODEL_A_PATH = 'mslstm_attention_synced.pth'\n",
        "OPTIMIZED_MODEL_B_PATH = 'final_optimized_model.pth'\n",
        "\n",
        "def run_ensemble_evaluation():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device.upper()}\")\n",
        "\n",
        "    _, test_loader, target_scaler, _ = get_data_loaders()\n",
        "    if not test_loader: return\n",
        "\n",
        "    # Model A (ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªìng b·ªô)\n",
        "    model_A = MSLSTMAttention(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=128, lstm_num_layers=2, num_heads=4\n",
        "    )\n",
        "\n",
        "    # Model B (t·ªëi ∆∞u)\n",
        "    model_B_params = {'lstm_hidden_units': 192, 'lstm_num_layers': 1, 'attention_num_heads': 8}\n",
        "    model_B = FinalModel(\n",
        "        input_feature_size=NUM_BASE_FEATURES, num_scales=NUM_SCALES,\n",
        "        lstm_hidden_units=model_B_params['lstm_hidden_units'],\n",
        "        lstm_num_layers=model_B_params['lstm_num_layers'],\n",
        "        num_heads=model_B_params['attention_num_heads']\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        print(f\"ƒêang t·∫£i Model A (ƒë·ªìng b·ªô) t·ª´: {SYNCED_MODEL_A_PATH}\")\n",
        "        model_A.load_state_dict(torch.load(SYNCED_MODEL_A_PATH, map_location=device))\n",
        "        model_A.to(device)\n",
        "        model_A.eval()\n",
        "\n",
        "        print(f\"ƒêang t·∫£i Model B (t·ªëi ∆∞u) t·ª´: {OPTIMIZED_MODEL_B_PATH}\")\n",
        "        model_B.load_state_dict(torch.load(OPTIMIZED_MODEL_B_PATH, map_location=device))\n",
        "        model_B.to(device)\n",
        "        model_B.eval()\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file model. H√£y ƒë·∫£m b·∫£o c√°c file sau t·ªìn t·∫°i: {e.filename}\")\n",
        "        return\n",
        "\n",
        "    # L·∫•y d·ª± ƒëo√°n v√† th·ª±c hi·ªán ensemble (logic gi·∫£i chu·∫©n h√≥a tr∆∞·ªõc ƒë√£ ƒë√∫ng)\n",
        "    original_preds_ensemble = []\n",
        "    original_actuals_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, (price_lbl, _, _) in test_loader:\n",
        "            features = features.to(device)\n",
        "\n",
        "            scaled_pred_A = model_A(features)\n",
        "            scaled_pred_B_price, _, _ = model_B(features)\n",
        "\n",
        "            original_pred_A = target_scaler.inverse_transform(scaled_pred_A.cpu().numpy().reshape(-1, 1))\n",
        "            original_pred_B = target_scaler.inverse_transform(scaled_pred_B_price.cpu().numpy().reshape(-1, 1))\n",
        "\n",
        "            avg_pred_original = (original_pred_A + original_pred_B) / 2.0\n",
        "\n",
        "            original_actuals = target_scaler.inverse_transform(price_lbl.numpy().reshape(-1, 1))\n",
        "\n",
        "            original_preds_ensemble.extend(avg_pred_original.flatten())\n",
        "            original_actuals_list.extend(original_actuals.flatten())\n",
        "\n",
        "    mae = np.mean(np.abs(np.array(original_preds_ensemble) - np.array(original_actuals_list)))\n",
        "    print(\"\\n--- K·∫øt qu·∫£ ƒê√°nh gi√° c·ªßa Ensemble (ƒê√£ ƒë·ªìng b·ªô d·ªØ li·ªáu) ---\")\n",
        "    print(f\"üèÜ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): {mae:.4f} (ƒëi·ªÉm VN-Index)\")\n",
        "\n",
        "    # V·∫Ω bi·ªÉu ƒë·ªì\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(original_actuals_list, label='Gi√° tr·ªã Th·ª±c t·∫ø', color='blue', marker='.', linestyle='-')\n",
        "    plt.plot(original_preds_ensemble, label='D·ª± ƒëo√°n Ensemble', color='green', linestyle='--')\n",
        "    plt.title(\"So s√°nh Gi√° tr·ªã Th·ª±c t·∫ø v√† D·ª± ƒëo√°n Ensemble (ƒê·ªìng b·ªô)\")\n",
        "    plt.xlabel('Ng√†y (trong t·∫≠p Test)')\n",
        "    plt.ylabel('Gi√° ƒë√≥ng c·ª≠a VN-Index')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('final_ensemble_prediction_synced.png')\n",
        "    print(\"‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh c·ªßa Ensemble v√†o file 'final_ensemble_prediction_synced.png'\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_ensemble_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyTZHq4jjGN8",
        "outputId": "151d96d2-7ba3-4ef9-e4d5-b1e91766ede9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ensemble_evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ensemble_evaluate.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0OCGmpqjJ3q",
        "outputId": "5999f17c-2757-4370-b919-f4fb1f5404f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng thi·∫øt b·ªã: CUDA\n",
            "‚úÖ DataLoader ƒëa nhi·ªám ƒë√£ s·∫µn s√†ng.\n",
            "ƒêang t·∫£i Model A (ƒë·ªìng b·ªô) t·ª´: mslstm_attention_synced.pth\n",
            "ƒêang t·∫£i Model B (t·ªëi ∆∞u) t·ª´: final_optimized_model.pth\n",
            "\n",
            "--- K·∫øt qu·∫£ ƒê√°nh gi√° c·ªßa Ensemble (ƒê√£ ƒë·ªìng b·ªô d·ªØ li·ªáu) ---\n",
            "üèÜ [Gi√°] Sai s·ªë Trung b√¨nh Tuy·ªát ƒë·ªëi (MAE): 19.0011 (ƒëi·ªÉm VN-Index)\n",
            "‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì so s√°nh c·ªßa Ensemble v√†o file 'final_ensemble_prediction_synced.png'\n"
          ]
        }
      ]
    }
  ]
}